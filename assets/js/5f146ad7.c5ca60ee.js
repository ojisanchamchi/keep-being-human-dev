"use strict";(self.webpackChunkkeep_being_human_dev=self.webpackChunkkeep_being_human_dev||[]).push([[5929],{65404:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>o});var t=r(36672);const s={},i=t.createContext(s);function a(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(i.Provider,{value:n},e.children)}},65972:(e,n,r)=>{r.d(n,{A:()=>t});const t=r.p+"assets/images/redshift-dense-compute-node-38eaab7b46c5529bc0b5ac6ed30f61bf.png"},78410:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>m,frontMatter:()=>a,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"diagrams/aws-nodes/aws-analytics/redshift-dense-compute-node","title":"Amazon Redshift Dense Compute Node","description":"Overview","source":"@site/docs/diagrams/aws-nodes/aws-analytics/27-redshift-dense-compute-node.md","sourceDirName":"diagrams/aws-nodes/aws-analytics","slug":"/diagrams/aws-nodes/aws-analytics/redshift-dense-compute-node","permalink":"/keep-being-human-dev/docs/diagrams/aws-nodes/aws-analytics/redshift-dense-compute-node","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/diagrams/aws-nodes/aws-analytics/27-redshift-dense-compute-node.md","tags":[],"version":"current","sidebarPosition":27,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Amazon QuickSight","permalink":"/keep-being-human-dev/docs/diagrams/aws-nodes/aws-analytics/quicksight"},"next":{"title":"Amazon Redshift Dense Storage Node","permalink":"/keep-being-human-dev/docs/diagrams/aws-nodes/aws-analytics/redshift-dense-storage-node"}}');var s=r(23420),i=r(65404);const a={},o="Amazon Redshift Dense Compute Node",c={},l=[{value:"Overview",id:"overview",level:2},{value:"Main Functions",id:"main-functions",level:2},{value:"High-Performance Computing",id:"high-performance-computing",level:3},{value:"Workload Optimization",id:"workload-optimization",level:3},{value:"Scalability Features",id:"scalability-features",level:3},{value:"Use Cases",id:"use-cases",level:2},{value:"High-Performance Analytics Platform",id:"high-performance-analytics-platform",level:3},{value:"Interactive Analytics Workbench",id:"interactive-analytics-workbench",level:3},{value:"Architecture Diagram",id:"architecture-diagram",level:2},{value:"Node Type Specifications",id:"node-type-specifications",level:2},{value:"DC2 Instance Types",id:"dc2-instance-types",level:3},{value:"Performance Characteristics",id:"performance-characteristics",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"Cost Management",id:"cost-management",level:3},{value:"Workload Management",id:"workload-management",level:3},{value:"When to Choose DC Nodes",id:"when-to-choose-dc-nodes",level:2},{value:"Ideal Scenarios",id:"ideal-scenarios",level:3},{value:"Migration Considerations",id:"migration-considerations",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"amazon-redshift-dense-compute-node",children:"Amazon Redshift Dense Compute Node"})}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"Amazon Redshift Dense Compute (DC) nodes are optimized for performance-intensive workloads with fast CPUs, large amounts of RAM, and solid-state drives (SSDs). DC nodes are ideal for workloads that require high performance and don't need massive amounts of storage. They provide the best price-performance for data warehouses that prioritize query speed over storage capacity."}),"\n",(0,s.jsx)(n.h2,{id:"main-functions",children:"Main Functions"}),"\n",(0,s.jsx)(n.h3,{id:"high-performance-computing",children:"High-Performance Computing"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Fast SSD Storage"}),": Local SSD storage for maximum I/O performance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"High CPU Performance"}),": Optimized processors for complex analytical queries"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Large Memory"}),": Substantial RAM for in-memory processing and caching"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Low Latency"}),": Minimal query response times for interactive analytics"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"workload-optimization",children:"Workload Optimization"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"OLAP Workloads"}),": Optimized for online analytical processing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Complex Queries"}),": Handles sophisticated analytical queries efficiently"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Concurrent Users"}),": Supports multiple concurrent analytical sessions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-time Analytics"}),": Fast response times for real-time dashboards"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"scalability-features",children:"Scalability Features"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Elastic Resize"}),": Scale cluster size without downtime"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Concurrency Scaling"}),": Automatic scaling for concurrent queries"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Performance Monitoring"}),": Built-in performance metrics and optimization"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Workload Management"}),": Advanced query queue management"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"use-cases",children:"Use Cases"}),"\n",(0,s.jsx)(n.h3,{id:"high-performance-analytics-platform",children:"High-Performance Analytics Platform"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import boto3\nimport psycopg2\nfrom datetime import datetime, timedelta\nimport time\n\nclass RedshiftDCClusterManager:\n    def __init__(self):\n        self.redshift = boto3.client('redshift')\n        self.cloudwatch = boto3.client('cloudwatch')\n    \n    def create_dc_cluster(self, cluster_identifier, node_type='dc2.large', num_nodes=2):\n        \"\"\"Create Dense Compute Redshift cluster\"\"\"\n        try:\n            response = self.redshift.create_cluster(\n                ClusterIdentifier=cluster_identifier,\n                NodeType=node_type,\n                MasterUsername='admin',\n                MasterUserPassword='SecurePassword123!',\n                DBName='analytics',\n                NumberOfNodes=num_nodes,\n                ClusterType='multi-node' if num_nodes > 1 else 'single-node',\n                VpcSecurityGroupIds=['sg-12345678'],\n                ClusterSubnetGroupName='redshift-subnet-group',\n                PubliclyAccessible=False,\n                Encrypted=True,\n                KmsKeyId='arn:aws:kms:us-east-1:123456789012:key/12345678-1234-1234-1234-123456789012',\n                EnhancedVpcRouting=True,\n                ClusterParameterGroupName='high-performance-params',\n                Tags=[\n                    {\n                        'Key': 'Environment',\n                        'Value': 'Production'\n                    },\n                    {\n                        'Key': 'NodeType',\n                        'Value': 'DenseCompute'\n                    }\n                ]\n            )\n            print(f\"DC cluster creation initiated: {cluster_identifier}\")\n            return response\n        except Exception as e:\n            print(f\"Error creating DC cluster: {e}\")\n    \n    def optimize_for_performance(self, cluster_identifier):\n        \"\"\"Configure cluster for optimal performance\"\"\"\n        try:\n            # Create custom parameter group for performance\n            param_group_response = self.redshift.create_cluster_parameter_group(\n                ParameterGroupName='dc-performance-optimized',\n                ParameterGroupFamily='redshift-1.0',\n                Description='Optimized parameters for DC nodes'\n            )\n            \n            # Set performance parameters\n            performance_params = [\n                {\n                    'ParameterName': 'wlm_json_configuration',\n                    'ParameterValue': '''[\n                        {\n                            \"query_group\": \"dashboard\",\n                            \"query_group_wild_card\": 0,\n                            \"query_concurrency\": 5,\n                            \"memory_percent_to_use\": 30,\n                            \"max_execution_time\": 300000\n                        },\n                        {\n                            \"query_group\": \"etl\",\n                            \"query_group_wild_card\": 0,\n                            \"query_concurrency\": 2,\n                            \"memory_percent_to_use\": 60,\n                            \"max_execution_time\": 1800000\n                        },\n                        {\n                            \"query_concurrency\": 3,\n                            \"memory_percent_to_use\": 10\n                        }\n                    ]'''\n                },\n                {\n                    'ParameterName': 'enable_result_cache_for_session',\n                    'ParameterValue': 'true'\n                },\n                {\n                    'ParameterName': 'max_cursor_result_set_size',\n                    'ParameterValue': '1000'\n                }\n            ]\n            \n            for param in performance_params:\n                self.redshift.modify_cluster_parameter_group(\n                    ParameterGroupName='dc-performance-optimized',\n                    Parameters=[param]\n                )\n            \n            # Apply parameter group to cluster\n            self.redshift.modify_cluster(\n                ClusterIdentifier=cluster_identifier,\n                ClusterParameterGroupName='dc-performance-optimized',\n                ApplyImmediately=True\n            )\n            \n            print(f\"Performance optimization applied to {cluster_identifier}\")\n            \n        except Exception as e:\n            print(f\"Error optimizing cluster performance: {e}\")\n    \n    def monitor_dc_performance(self, cluster_identifier):\n        \"\"\"Monitor DC cluster performance metrics\"\"\"\n        try:\n            end_time = datetime.utcnow()\n            start_time = end_time - timedelta(hours=1)\n            \n            # Get CPU utilization\n            cpu_response = self.cloudwatch.get_metric_statistics(\n                Namespace='AWS/Redshift',\n                MetricName='CPUUtilization',\n                Dimensions=[\n                    {\n                        'Name': 'ClusterIdentifier',\n                        'Value': cluster_identifier\n                    }\n                ],\n                StartTime=start_time,\n                EndTime=end_time,\n                Period=300,\n                Statistics=['Average', 'Maximum']\n            )\n            \n            # Get query performance metrics\n            query_duration_response = self.cloudwatch.get_metric_statistics(\n                Namespace='AWS/Redshift',\n                MetricName='QueryDuration',\n                Dimensions=[\n                    {\n                        'Name': 'ClusterIdentifier',\n                        'Value': cluster_identifier\n                    }\n                ],\n                StartTime=start_time,\n                EndTime=end_time,\n                Period=300,\n                Statistics=['Average', 'Maximum']\n            )\n            \n            # Get concurrent connections\n            connections_response = self.cloudwatch.get_metric_statistics(\n                Namespace='AWS/Redshift',\n                MetricName='DatabaseConnections',\n                Dimensions=[\n                    {\n                        'Name': 'ClusterIdentifier',\n                        'Value': cluster_identifier\n                    }\n                ],\n                StartTime=start_time,\n                EndTime=end_time,\n                Period=300,\n                Statistics=['Average', 'Maximum']\n            )\n            \n            performance_metrics = {\n                'cpu_utilization': cpu_response['Datapoints'],\n                'query_duration': query_duration_response['Datapoints'],\n                'database_connections': connections_response['Datapoints']\n            }\n            \n            return performance_metrics\n            \n        except Exception as e:\n            print(f\"Error monitoring DC performance: {e}\")\n    \n    def implement_concurrency_scaling(self, cluster_identifier):\n        \"\"\"Enable and configure concurrency scaling\"\"\"\n        try:\n            # Enable concurrency scaling\n            response = self.redshift.modify_cluster(\n                ClusterIdentifier=cluster_identifier,\n                ConcurrencyScalingMode='auto'\n            )\n            \n            # Create usage limit for concurrency scaling\n            usage_limit_response = self.redshift.create_usage_limit(\n                ClusterIdentifier=cluster_identifier,\n                FeatureType='concurrency-scaling',\n                LimitType='time',\n                Amount=60,  # 60 minutes per day\n                Period='daily',\n                BreachAction='log'\n            )\n            \n            print(f\"Concurrency scaling enabled for {cluster_identifier}\")\n            return response\n            \n        except Exception as e:\n            print(f\"Error implementing concurrency scaling: {e}\")\n\n# Usage example\ndc_manager = RedshiftDCClusterManager()\n\n# Create DC cluster\ndc_manager.create_dc_cluster(\n    cluster_identifier='analytics-dc-cluster',\n    node_type='dc2.8xlarge',\n    num_nodes=4\n)\n\n# Optimize for performance\ndc_manager.optimize_for_performance('analytics-dc-cluster')\n\n# Enable concurrency scaling\ndc_manager.implement_concurrency_scaling('analytics-dc-cluster')\n\n# Monitor performance\nperformance_data = dc_manager.monitor_dc_performance('analytics-dc-cluster')\nprint(\"Performance metrics:\", performance_data)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"interactive-analytics-workbench",children:"Interactive Analytics Workbench"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import psycopg2\nimport pandas as pd\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\nclass DCInteractiveAnalytics:\n    def __init__(self, cluster_endpoint, database, username, password):\n        self.connection = psycopg2.connect(\n            host=cluster_endpoint,\n            database=database,\n            user=username,\n            password=password,\n            port=5439\n        )\n        self.connection.set_session(autocommit=True)\n    \n    def create_optimized_tables(self):\n        """Create tables optimized for DC nodes"""\n        cursor = self.connection.cursor()\n        \n        # Create fact table with optimal distribution\n        cursor.execute("""\n            CREATE TABLE IF NOT EXISTS sales_fact_dc (\n                sale_id BIGINT IDENTITY(1,1),\n                customer_id INTEGER,\n                product_id INTEGER,\n                sale_date DATE,\n                quantity INTEGER,\n                unit_price DECIMAL(10,2),\n                total_amount DECIMAL(12,2),\n                region_id INTEGER\n            )\n            DISTSTYLE KEY\n            DISTKEY(customer_id)\n            SORTKEY(sale_date, customer_id)\n            ENCODE AUTO;\n        """)\n        \n        # Create dimension table for fast joins\n        cursor.execute("""\n            CREATE TABLE IF NOT EXISTS customer_dim_dc (\n                customer_id INTEGER PRIMARY KEY,\n                customer_name VARCHAR(255),\n                email VARCHAR(255),\n                segment VARCHAR(50),\n                registration_date DATE\n            )\n            DISTSTYLE ALL\n            SORTKEY(customer_id)\n            ENCODE AUTO;\n        """)\n        \n        print("Optimized tables created for DC nodes")\n    \n    def execute_interactive_query(self, query, params=None):\n        """Execute query optimized for interactive performance"""\n        start_time = datetime.now()\n        \n        try:\n            df = pd.read_sql_query(query, self.connection, params=params)\n            end_time = datetime.now()\n            execution_time = (end_time - start_time).total_seconds()\n            \n            print(f"Query executed in {execution_time:.2f} seconds")\n            print(f"Returned {len(df)} rows")\n            \n            return df, execution_time\n            \n        except Exception as e:\n            print(f"Error executing query: {e}")\n            return None, None\n    \n    def real_time_dashboard_queries(self):\n        """Execute real-time dashboard queries"""\n        queries = {\n            \'daily_sales\': """\n                SELECT \n                    sale_date,\n                    COUNT(*) as transaction_count,\n                    SUM(total_amount) as daily_revenue,\n                    AVG(total_amount) as avg_order_value\n                FROM sales_fact_dc\n                WHERE sale_date >= CURRENT_DATE - 30\n                GROUP BY 1\n                ORDER BY 1;\n            """,\n            \n            \'top_customers\': """\n                SELECT \n                    c.customer_name,\n                    COUNT(s.sale_id) as purchase_count,\n                    SUM(s.total_amount) as total_spent,\n                    MAX(s.sale_date) as last_purchase\n                FROM sales_fact_dc s\n                JOIN customer_dim_dc c ON s.customer_id = c.customer_id\n                WHERE s.sale_date >= CURRENT_DATE - 90\n                GROUP BY 1\n                ORDER BY 3 DESC\n                LIMIT 20;\n            """,\n            \n            \'hourly_trends\': """\n                SELECT \n                    EXTRACT(hour FROM GETDATE()) as current_hour,\n                    COUNT(*) as current_hour_sales,\n                    SUM(total_amount) as current_hour_revenue\n                FROM sales_fact_dc\n                WHERE sale_date = CURRENT_DATE\n                    AND EXTRACT(hour FROM created_at) = EXTRACT(hour FROM GETDATE())\n                GROUP BY 1;\n            """\n        }\n        \n        results = {}\n        total_time = 0\n        \n        for query_name, sql in queries.items():\n            df, exec_time = self.execute_interactive_query(sql)\n            if df is not None:\n                results[query_name] = df\n                total_time += exec_time\n        \n        print(f"All dashboard queries completed in {total_time:.2f} seconds")\n        return results\n    \n    def performance_analysis(self):\n        """Analyze query performance on DC nodes"""\n        cursor = self.connection.cursor()\n        \n        # Get recent query performance\n        cursor.execute("""\n            SELECT \n                query,\n                starttime,\n                endtime,\n                DATEDIFF(milliseconds, starttime, endtime) as duration_ms,\n                rows,\n                bytes,\n                cpu_time,\n                blocks_read,\n                blocks_skipped\n            FROM stl_query\n            WHERE starttime >= GETDATE() - INTERVAL \'1 hour\'\n                AND userid > 1\n            ORDER BY duration_ms DESC\n            LIMIT 20;\n        """)\n        \n        performance_data = cursor.fetchall()\n        \n        # Analyze table statistics\n        cursor.execute("""\n            SELECT \n                schemaname,\n                tablename,\n                size,\n                tbl_rows,\n                skew_sortkey1,\n                skew_rows,\n                estimated_visible_rows\n            FROM svv_table_info\n            WHERE schemaname NOT IN (\'information_schema\', \'pg_catalog\')\n            ORDER BY size DESC;\n        """)\n        \n        table_stats = cursor.fetchall()\n        \n        return {\n            \'query_performance\': performance_data,\n            \'table_statistics\': table_stats\n        }\n\n# Usage example\ndc_analytics = DCInteractiveAnalytics(\n    cluster_endpoint=\'analytics-dc-cluster.abc123.us-east-1.redshift.amazonaws.com\',\n    database=\'analytics\',\n    username=\'admin\',\n    password=\'SecurePassword123!\'\n)\n\n# Setup optimized tables\ndc_analytics.create_optimized_tables()\n\n# Execute real-time dashboard queries\ndashboard_results = dc_analytics.real_time_dashboard_queries()\n\n# Analyze performance\nperformance_analysis = dc_analytics.performance_analysis()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"architecture-diagram",children:"Architecture Diagram"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Redshift Dense Compute Node Architecture",src:r(65972).A+"",width:"2563",height:"2234"})}),"\n",(0,s.jsx)(n.h2,{id:"node-type-specifications",children:"Node Type Specifications"}),"\n",(0,s.jsx)(n.h3,{id:"dc2-instance-types",children:"DC2 Instance Types"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"dc2.large"}),": 2 vCPUs, 15 GB RAM, 160 GB SSD"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"dc2.8xlarge"}),": 32 vCPUs, 244 GB RAM, 2,560 GB SSD"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"performance-characteristics",children:"Performance Characteristics"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Storage"}),": Local SSD storage for maximum I/O performance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Network"}),": Enhanced networking for low latency"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Memory"}),": High memory-to-vCPU ratio for analytical workloads"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"CPU"}),": Optimized processors for complex calculations"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,s.jsx)(n.h3,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Use appropriate distribution keys for even data distribution"}),"\n",(0,s.jsx)(n.li,{children:"Implement effective sort keys for query performance"}),"\n",(0,s.jsx)(n.li,{children:"Optimize table design for analytical workloads"}),"\n",(0,s.jsx)(n.li,{children:"Use result caching for frequently accessed data"}),"\n",(0,s.jsx)(n.li,{children:"Monitor and tune workload management queues"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"cost-management",children:"Cost Management"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Right-size clusters based on performance requirements"}),"\n",(0,s.jsx)(n.li,{children:"Use Reserved Instances for predictable workloads"}),"\n",(0,s.jsx)(n.li,{children:"Implement pause/resume for development environments"}),"\n",(0,s.jsx)(n.li,{children:"Monitor usage patterns and optimize accordingly"}),"\n",(0,s.jsx)(n.li,{children:"Consider migration to RA3 nodes for growing datasets"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"workload-management",children:"Workload Management"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Configure WLM queues for different user groups"}),"\n",(0,s.jsx)(n.li,{children:"Set appropriate memory allocation per queue"}),"\n",(0,s.jsx)(n.li,{children:"Implement query monitoring rules"}),"\n",(0,s.jsx)(n.li,{children:"Use concurrency scaling for peak loads"}),"\n",(0,s.jsx)(n.li,{children:"Monitor queue wait times and adjust as needed"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"when-to-choose-dc-nodes",children:"When to Choose DC Nodes"}),"\n",(0,s.jsx)(n.h3,{id:"ideal-scenarios",children:"Ideal Scenarios"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Performance-critical analytical workloads"}),"\n",(0,s.jsx)(n.li,{children:"Interactive dashboards and real-time analytics"}),"\n",(0,s.jsx)(n.li,{children:"Workloads with high query concurrency"}),"\n",(0,s.jsx)(n.li,{children:"Applications requiring sub-second response times"}),"\n",(0,s.jsx)(n.li,{children:"Datasets that fit within local SSD storage limits"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"migration-considerations",children:"Migration Considerations"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Evaluate storage requirements vs. performance needs"}),"\n",(0,s.jsx)(n.li,{children:"Consider RA3 nodes for larger datasets"}),"\n",(0,s.jsx)(n.li,{children:"Plan for data growth and future scalability"}),"\n",(0,s.jsx)(n.li,{children:"Assess cost implications of different node types"}),"\n",(0,s.jsx)(n.li,{children:"Test performance with representative workloads"}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);