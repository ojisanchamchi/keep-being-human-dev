"use strict";(self.webpackChunkkeep_being_human_dev=self.webpackChunkkeep_being_human_dev||[]).push([[25126],{65404:(e,n,a)=>{a.d(n,{R:()=>s,x:()=>c});var d=a(36672);const t={},i=d.createContext(t);function s(e){const n=d.useContext(i);return d.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),d.createElement(i.Provider,{value:n},e.children)}},82379:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>r,contentTitle:()=>c,default:()=>m,frontMatter:()=>s,metadata:()=>d,toc:()=>o});const d=JSON.parse('{"id":"gems/ruby-openai/advanced/batch_embeddings_async","title":"batch_embeddings_async","description":"\ud83d\udcc8 Batch Embedding with Sidekiq and Caching","source":"@site/docs/gems/ruby-openai/advanced/batch_embeddings_async.md","sourceDirName":"gems/ruby-openai/advanced","slug":"/gems/ruby-openai/advanced/batch_embeddings_async","permalink":"/keep-being-human-dev/docs/gems/ruby-openai/advanced/batch_embeddings_async","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/gems/ruby-openai/advanced/batch_embeddings_async.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"shared_examples_parameterized_usage","permalink":"/keep-being-human-dev/docs/gems/rspec/shared_examples/middle/shared_examples_parameterized_usage"},"next":{"title":"function_calling_advanced","permalink":"/keep-being-human-dev/docs/gems/ruby-openai/advanced/function_calling_advanced"}}');var t=a(23420),i=a(65404);const s={},c=void 0,r={},o=[{value:"\ud83d\udcc8 Batch Embedding with Sidekiq and Caching",id:"-batch-embedding-with-sidekiq-and-caching",level:2}];function u(e){const n={code:"code",h2:"h2",p:"p",pre:"pre",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h2,{id:"-batch-embedding-with-sidekiq-and-caching",children:"\ud83d\udcc8 Batch Embedding with Sidekiq and Caching"}),"\n",(0,t.jsx)(n.p,{children:"For large\u2010scale similarity search, process texts in batches asynchronously using Sidekiq and cache embeddings in Redis or a vector store. Chunk your corpus, enqueue jobs, and dedupe embeddings to minimize API calls and speed up lookups."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-ruby",children:'# app/workers/embedding_worker.rb\nclass EmbeddingWorker\n  include Sidekiq::Worker\n  sidekiq_options retry: 3\n\n  def perform(doc_id, text)\n    cache_key = "embedding:#{doc_id}"\n    return if Redis.current.exists(cache_key)\n\n    client = OpenAI::Client.new(access_token: ENV["OPENAI_API_KEY"])\n    resp   = client.embeddings.parameters(model: "text-embedding-ada-002", input: text)\n    vector = resp.dig("data", 0, "embedding")\n\n    # Store in Redis or push to Pinecone\n    Redis.current.set(cache_key, vector.to_json)\n  end\nend\n\n# Enqueue in batch\ndocuments.each_slice(50) do |batch|\n  batch.each { |doc| EmbeddingWorker.perform_async(doc.id, doc.content) }\nend\n'})}),"\n",(0,t.jsx)(n.p,{children:"By batching and caching, you avoid redundant embeddings, respect rate limits, and seamlessly integrate with vector stores for fast nearest\u2011neighbor searches."})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(u,{...e})}):u(e)}}}]);