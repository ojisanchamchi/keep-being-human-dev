"use strict";(self.webpackChunkkeep_being_human_dev=self.webpackChunkkeep_being_human_dev||[]).push([[97556],{59885:(e,n,r)=>{r.d(n,{A:()=>a});const a=r.p+"assets/images/kinesis-data-firehose-b90740e630678c30b373861168de557a.png"},65404:(e,n,r)=>{r.d(n,{R:()=>t,x:()=>o});var a=r(36672);const s={},i=a.createContext(s);function t(e){const n=a.useContext(i);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),a.createElement(i.Provider,{value:n},e.children)}},73866:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>m,frontMatter:()=>t,metadata:()=>a,toc:()=>l});const a=JSON.parse('{"id":"diagrams/aws-nodes/aws-analytics/kinesis-data-firehose","title":"Amazon Kinesis Data Firehose","description":"T\u1ed5ng quan","source":"@site/docs/diagrams/aws-nodes/aws-analytics/20-kinesis-data-firehose.md","sourceDirName":"diagrams/aws-nodes/aws-analytics","slug":"/diagrams/aws-nodes/aws-analytics/kinesis-data-firehose","permalink":"/keep-being-human-dev/docs/diagrams/aws-nodes/aws-analytics/kinesis-data-firehose","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/diagrams/aws-nodes/aws-analytics/20-kinesis-data-firehose.md","tags":[],"version":"current","sidebarPosition":20,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Amazon Kinesis Data Analytics","permalink":"/keep-being-human-dev/docs/diagrams/aws-nodes/aws-analytics/kinesis-data-analytics"},"next":{"title":"Amazon Kinesis Data Streams","permalink":"/keep-being-human-dev/docs/diagrams/aws-nodes/aws-analytics/kinesis-data-streams"}}');var s=r(23420),i=r(65404);const t={},o="Amazon Kinesis Data Firehose",d={},l=[{value:"T\u1ed5ng quan",id:"t\u1ed5ng-quan",level:2},{value:"Ch\u1ee9c n\u0103ng ch\xednh",id:"ch\u1ee9c-n\u0103ng-ch\xednh",level:2},{value:"1. Managed Data Delivery",id:"1-managed-data-delivery",level:3},{value:"2. Data Transformation",id:"2-data-transformation",level:3},{value:"3. Multiple Destinations",id:"3-multiple-destinations",level:3},{value:"4. Monitoring v\xe0 Management",id:"4-monitoring-v\xe0-management",level:3},{value:"Use Cases ph\u1ed5 bi\u1ebfn",id:"use-cases-ph\u1ed5-bi\u1ebfn",level:2},{value:"Diagram Architecture",id:"diagram-architecture",level:2},{value:"Code \u0111\u1ec3 t\u1ea1o diagram:",id:"code-\u0111\u1ec3-t\u1ea1o-diagram",level:3},{value:"Delivery Stream Configuration",id:"delivery-stream-configuration",level:2},{value:"1. S3 Delivery Stream",id:"1-s3-delivery-stream",level:3},{value:"2. Redshift Delivery Stream",id:"2-redshift-delivery-stream",level:3},{value:"Data Transformation",id:"data-transformation",level:2},{value:"1. Lambda Transformation Function",id:"1-lambda-transformation-function",level:3},{value:"2. Advanced Transformation Pipeline",id:"2-advanced-transformation-pipeline",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"1. Buffering Configuration",id:"1-buffering-configuration",level:3},{value:"Error Handling v\xe0 Monitoring",id:"error-handling-v\xe0-monitoring",level:2},{value:"1. Comprehensive Error Handling",id:"1-comprehensive-error-handling",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"T\xedch h\u1ee3p v\u1edbi c\xe1c d\u1ecbch v\u1ee5 AWS kh\xe1c",id:"t\xedch-h\u1ee3p-v\u1edbi-c\xe1c-d\u1ecbch-v\u1ee5-aws-kh\xe1c",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"amazon-kinesis-data-firehose",children:"Amazon Kinesis Data Firehose"})}),"\n",(0,s.jsx)(n.h2,{id:"t\u1ed5ng-quan",children:"T\u1ed5ng quan"}),"\n",(0,s.jsx)(n.p,{children:"Amazon Kinesis Data Firehose l\xe0 node \u0111\u1ea1i di\u1ec7n cho d\u1ecbch v\u1ee5 delivery streaming data \u0111\u01b0\u1ee3c qu\u1ea3n l\xfd ho\xe0n to\xe0n c\u1ee7a AWS. Firehose t\u1ef1 \u0111\u1ed9ng capture, transform, v\xe0 load streaming data v\xe0o c\xe1c data stores nh\u01b0 Amazon S3, Amazon Redshift, Amazon OpenSearch Service, v\xe0 third-party services. \u0110\xe2y l\xe0 c\xe1ch \u0111\u01a1n gi\u1ea3n nh\u1ea5t \u0111\u1ec3 load streaming data v\xe0o data lakes v\xe0 analytics services."}),"\n",(0,s.jsx)(n.h2,{id:"ch\u1ee9c-n\u0103ng-ch\xednh",children:"Ch\u1ee9c n\u0103ng ch\xednh"}),"\n",(0,s.jsx)(n.h3,{id:"1-managed-data-delivery",children:"1. Managed Data Delivery"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Serverless"}),": Kh\xf4ng c\u1ea7n qu\u1ea3n l\xfd infrastructure"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Auto Scaling"}),": T\u1ef1 \u0111\u1ed9ng scale theo throughput"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reliable Delivery"}),": Guaranteed delivery v\u1edbi retry logic"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Buffering"}),": Intelligent buffering based on size v\xe0 time"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"2-data-transformation",children:"2. Data Transformation"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Built-in Transformations"}),": Format conversion (JSON to Parquet/ORC)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"AWS Lambda Integration"}),": Custom data transformation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Data Compression"}),": GZIP, Snappy, ZIP compression"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Error Record Handling"}),": Separate error records processing"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"3-multiple-destinations",children:"3. Multiple Destinations"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Amazon S3"}),": Data lake storage"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Amazon Redshift"}),": Data warehouse loading"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Amazon OpenSearch"}),": Search v\xe0 analytics"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Third-party"}),": Splunk, Datadog, New Relic, MongoDB"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"4-monitoring-v\xe0-management",children:"4. Monitoring v\xe0 Management"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"CloudWatch Integration"}),": Comprehensive metrics"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Error Logging"}),": Detailed error tracking"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Data Lineage"}),": Track data flow"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cost Optimization"}),": Pay only for data processed"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"use-cases-ph\u1ed5-bi\u1ebfn",children:"Use Cases ph\u1ed5 bi\u1ebfn"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Data Lake Ingestion"}),": Load streaming data v\xe0o S3 data lakes"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Log Aggregation"}),": Centralize logs t\u1eeb multiple sources"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-time ETL"}),": Transform v\xe0 load data for analytics"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Backup v\xe0 Archival"}),": Backup streaming data"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multi-destination Delivery"}),": Deliver data \u0111\u1ebfn multiple targets"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"diagram-architecture",children:"Diagram Architecture"}),"\n",(0,s.jsx)(n.p,{children:"Ki\u1ebfn tr\xfac Amazon Kinesis Data Firehose v\u1edbi data delivery pipeline:"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Amazon Kinesis Data Firehose Architecture",src:r(59885).A+"",width:"2343",height:"1496"})}),"\n",(0,s.jsx)(n.h3,{id:"code-\u0111\u1ec3-t\u1ea1o-diagram",children:"Code \u0111\u1ec3 t\u1ea1o diagram:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.analytics import KinesisDataFirehose, KinesisDataStreams, Athena\nfrom diagrams.aws.storage import S3\nfrom diagrams.aws.compute import Lambda\nfrom diagrams.aws.database import Redshift, Dynamodb\nfrom diagrams.aws.integration import SQS, SNS\nfrom diagrams.aws.management import Cloudwatch\nfrom diagrams.aws.security import IAM\nfrom diagrams.onprem.client import Users\nfrom diagrams.onprem.analytics import Splunk\n\nwith Diagram("Amazon Kinesis Data Firehose Architecture", show=False, direction="TB"):\n    \n    users = Users("Applications")\n    \n    with Cluster("Data Sources"):\n        web_logs = Lambda("Web Server Logs")\n        app_events = Lambda("Application Events")\n        iot_sensors = Lambda("IoT Sensors")\n        clickstream = Lambda("Clickstream Data")\n        kinesis_streams = KinesisDataStreams("Kinesis Data Streams")\n    \n    with Cluster("Firehose Delivery Streams"):\n        firehose_s3 = KinesisDataFirehose("S3 Delivery Stream")\n        firehose_redshift = KinesisDataFirehose("Redshift Delivery Stream")\n        firehose_opensearch = KinesisDataFirehose("OpenSearch Delivery Stream")\n        firehose_splunk = KinesisDataFirehose("Splunk Delivery Stream")\n    \n    with Cluster("Data Transformation"):\n        lambda_transform = Lambda("Data Transformation")\n        format_converter = Lambda("Format Converter")\n        data_enrichment = Lambda("Data Enrichment")\n    \n    with Cluster("Destinations"):\n        s3_data_lake = S3("S3 Data Lake")\n        redshift_dw = Redshift("Redshift DW")\n        opensearch = Lambda("OpenSearch Service")\n        splunk_cloud = Splunk("Splunk Cloud")\n    \n    with Cluster("Error Handling"):\n        error_bucket = S3("Error Records")\n        processing_errors = S3("Processing Errors")\n        dlq = SQS("Dead Letter Queue")\n    \n    with Cluster("Analytics & Monitoring"):\n        athena = Athena("Athena Queries")\n        cloudwatch = Cloudwatch("CloudWatch")\n        notifications = SNS("Notifications")\n        security = IAM("IAM Roles")\n    \n    # Data ingestion flow\n    web_logs >> Edge(label="Direct PUT") >> firehose_s3\n    app_events >> Edge(label="SDK/API") >> firehose_redshift\n    iot_sensors >> Edge(label="IoT Core") >> firehose_opensearch\n    clickstream >> Edge(label="Analytics") >> firehose_splunk\n    kinesis_streams >> Edge(label="Stream Consumer") >> firehose_s3\n    \n    # Data transformation\n    firehose_s3 >> lambda_transform\n    firehose_redshift >> format_converter\n    firehose_opensearch >> data_enrichment\n    \n    lambda_transform >> firehose_s3\n    format_converter >> firehose_redshift\n    data_enrichment >> firehose_opensearch\n    \n    # Data delivery\n    firehose_s3 >> Edge(label="Parquet/ORC") >> s3_data_lake\n    firehose_redshift >> Edge(label="COPY Command") >> redshift_dw\n    firehose_opensearch >> Edge(label="Bulk API") >> opensearch\n    firehose_splunk >> Edge(label="HEC Protocol") >> splunk_cloud\n    \n    # Error handling\n    firehose_s3 >> Edge(label="Failed Records") >> error_bucket\n    lambda_transform >> Edge(label="Processing Errors") >> processing_errors\n    firehose_redshift >> Edge(label="Failed Loads") >> dlq\n    \n    # Analytics integration\n    s3_data_lake >> athena\n    users >> Edge(label="Query Data") >> athena\n    \n    # Monitoring\n    firehose_s3 >> Edge(label="Metrics") >> cloudwatch\n    cloudwatch >> notifications\n    security >> Edge(label="Access Control") >> firehose_s3\n'})}),"\n",(0,s.jsx)(n.h2,{id:"delivery-stream-configuration",children:"Delivery Stream Configuration"}),"\n",(0,s.jsx)(n.h3,{id:"1-s3-delivery-stream",children:"1. S3 Delivery Stream"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import boto3\nimport json\n\nfirehose = boto3.client('firehose')\n\n# S3 delivery stream with transformation\ns3_delivery_config = {\n    'DeliveryStreamName': 'web-logs-to-s3',\n    'DeliveryStreamType': 'DirectPut',\n    'ExtendedS3DestinationConfiguration': {\n        'RoleARN': 'arn:aws:iam::account:role/firehose-delivery-role',\n        'BucketARN': 'arn:aws:s3:::data-lake-bucket',\n        'Prefix': 'web-logs/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/',\n        'ErrorOutputPrefix': 'errors/web-logs/',\n        'BufferingHints': {\n            'SizeInMBs': 128,\n            'IntervalInSeconds': 60\n        },\n        'CompressionFormat': 'GZIP',\n        'DataFormatConversionConfiguration': {\n            'Enabled': True,\n            'OutputFormatConfiguration': {\n                'Serializer': {\n                    'ParquetSerDe': {}\n                }\n            },\n            'SchemaConfiguration': {\n                'DatabaseName': 'web_logs_db',\n                'TableName': 'access_logs',\n                'RoleARN': 'arn:aws:iam::account:role/glue-catalog-role'\n            }\n        },\n        'ProcessingConfiguration': {\n            'Enabled': True,\n            'Processors': [\n                {\n                    'Type': 'Lambda',\n                    'Parameters': [\n                        {\n                            'ParameterName': 'LambdaArn',\n                            'ParameterValue': 'arn:aws:lambda:region:account:function:log-transformer'\n                        }\n                    ]\n                }\n            ]\n        },\n        'CloudWatchLoggingOptions': {\n            'Enabled': True,\n            'LogGroupName': '/aws/kinesisfirehose/web-logs-to-s3'\n        }\n    }\n}\n\nresponse = firehose.create_delivery_stream(**s3_delivery_config)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-redshift-delivery-stream",children:"2. Redshift Delivery Stream"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Redshift delivery stream configuration\nredshift_delivery_config = {\n    'DeliveryStreamName': 'events-to-redshift',\n    'DeliveryStreamType': 'DirectPut',\n    'RedshiftDestinationConfiguration': {\n        'RoleARN': 'arn:aws:iam::account:role/firehose-redshift-role',\n        'ClusterJDBCURL': 'jdbc:redshift://redshift-cluster.region.redshift.amazonaws.com:5439/analytics',\n        'CopyCommand': {\n            'DataTableName': 'user_events',\n            'DataTableColumns': 'event_id,user_id,event_type,timestamp,properties',\n            'CopyOptions': \"JSON 'auto' GZIP TRUNCATECOLUMNS\"\n        },\n        'Username': 'firehose_user',\n        'Password': 'secure_password',\n        'S3Configuration': {\n            'RoleARN': 'arn:aws:iam::account:role/firehose-s3-role',\n            'BucketARN': 'arn:aws:s3:::redshift-staging-bucket',\n            'Prefix': 'staging/events/',\n            'BufferingHints': {\n                'SizeInMBs': 5,\n                'IntervalInSeconds': 300\n            },\n            'CompressionFormat': 'GZIP'\n        },\n        'ProcessingConfiguration': {\n            'Enabled': True,\n            'Processors': [\n                {\n                    'Type': 'Lambda',\n                    'Parameters': [\n                        {\n                            'ParameterName': 'LambdaArn',\n                            'ParameterValue': 'arn:aws:lambda:region:account:function:redshift-transformer'\n                        }\n                    ]\n                }\n            ]\n        },\n        'RetryDuration': 3600,\n        'CloudWatchLoggingOptions': {\n            'Enabled': True,\n            'LogGroupName': '/aws/kinesisfirehose/events-to-redshift'\n        }\n    }\n}\n\nresponse = firehose.create_delivery_stream(**redshift_delivery_config)\n"})}),"\n",(0,s.jsx)(n.h2,{id:"data-transformation",children:"Data Transformation"}),"\n",(0,s.jsx)(n.h3,{id:"1-lambda-transformation-function",children:"1. Lambda Transformation Function"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import json\nimport base64\nimport gzip\nfrom datetime import datetime\n\ndef lambda_handler(event, context):\n    \"\"\"\n    Transform Kinesis Data Firehose records\n    \"\"\"\n    \n    output = []\n    \n    for record in event['records']:\n        # Decode the data\n        compressed_payload = base64.b64decode(record['data'])\n        uncompressed_payload = gzip.decompress(compressed_payload)\n        data = json.loads(uncompressed_payload)\n        \n        # Transform the data\n        transformed_data = transform_log_record(data)\n        \n        # Encode the transformed data\n        output_record = {\n            'recordId': record['recordId'],\n            'result': 'Ok',\n            'data': base64.b64encode(\n                json.dumps(transformed_data).encode('utf-8') + b'\\n'\n            ).decode('utf-8')\n        }\n        \n        output.append(output_record)\n    \n    return {'records': output}\n\ndef transform_log_record(data):\n    \"\"\"Transform individual log record\"\"\"\n    \n    transformed = {\n        'timestamp': datetime.utcnow().isoformat(),\n        'source_ip': data.get('ip', ''),\n        'user_agent': data.get('user_agent', ''),\n        'request_method': data.get('method', ''),\n        'request_uri': data.get('uri', ''),\n        'response_code': int(data.get('status', 0)),\n        'response_size': int(data.get('size', 0)),\n        'processing_time': float(data.get('response_time', 0.0))\n    }\n    \n    # Add derived fields\n    transformed['is_error'] = transformed['response_code'] >= 400\n    transformed['is_bot'] = is_bot_request(transformed['user_agent'])\n    transformed['country'] = get_country_from_ip(transformed['source_ip'])\n    \n    # Data quality checks\n    if not validate_record(transformed):\n        raise ValueError(\"Invalid record format\")\n    \n    return transformed\n\ndef is_bot_request(user_agent):\n    \"\"\"Detect bot requests\"\"\"\n    bot_indicators = ['bot', 'crawler', 'spider', 'scraper']\n    return any(indicator in user_agent.lower() for indicator in bot_indicators)\n\ndef get_country_from_ip(ip_address):\n    \"\"\"Get country from IP address (simplified)\"\"\"\n    # In production, use a proper GeoIP service\n    if ip_address.startswith('192.168.'):\n        return 'US'  # Private IP\n    return 'UNKNOWN'\n\ndef validate_record(record):\n    \"\"\"Validate transformed record\"\"\"\n    required_fields = ['timestamp', 'source_ip', 'request_method']\n    return all(field in record and record[field] for field in required_fields)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-advanced-transformation-pipeline",children:"2. Advanced Transformation Pipeline"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class FirehoseTransformationPipeline:\n    def __init__(self):\n        self.transformers = [\n            self.parse_log_format,\n            self.enrich_with_geolocation,\n            self.detect_anomalies,\n            self.add_metadata\n        ]\n    \n    def process_records(self, records):\n        \"\"\"Process batch of records through transformation pipeline\"\"\"\n        \n        processed_records = []\n        \n        for record in records:\n            try:\n                # Decode record\n                data = self.decode_record(record)\n                \n                # Apply transformation pipeline\n                for transformer in self.transformers:\n                    data = transformer(data)\n                \n                # Encode transformed record\n                processed_record = self.encode_record(record['recordId'], data)\n                processed_records.append(processed_record)\n                \n            except Exception as e:\n                # Handle transformation errors\n                error_record = {\n                    'recordId': record['recordId'],\n                    'result': 'ProcessingFailed',\n                    'data': record['data']  # Return original data\n                }\n                processed_records.append(error_record)\n                \n                # Log error for monitoring\n                print(f\"Transformation error: {str(e)}\")\n        \n        return processed_records\n    \n    def parse_log_format(self, data):\n        \"\"\"Parse common log formats\"\"\"\n        \n        if 'message' in data:\n            # Parse Apache/Nginx log format\n            log_pattern = r'(\\S+) \\S+ \\S+ \\[([\\w:/]+\\s[+\\-]\\d{4})\\] \"(\\S+) (\\S+) (\\S+)\" (\\d{3}) (\\d+)'\n            match = re.match(log_pattern, data['message'])\n            \n            if match:\n                data.update({\n                    'ip': match.group(1),\n                    'timestamp': match.group(2),\n                    'method': match.group(3),\n                    'uri': match.group(4),\n                    'protocol': match.group(5),\n                    'status': int(match.group(6)),\n                    'size': int(match.group(7))\n                })\n        \n        return data\n    \n    def enrich_with_geolocation(self, data):\n        \"\"\"Enrich with geolocation data\"\"\"\n        \n        if 'ip' in data:\n            # In production, use MaxMind GeoIP or similar service\n            geo_data = self.lookup_geolocation(data['ip'])\n            data.update({\n                'country': geo_data.get('country', 'UNKNOWN'),\n                'city': geo_data.get('city', 'UNKNOWN'),\n                'latitude': geo_data.get('latitude'),\n                'longitude': geo_data.get('longitude')\n            })\n        \n        return data\n    \n    def detect_anomalies(self, data):\n        \"\"\"Detect anomalous patterns\"\"\"\n        \n        anomaly_score = 0.0\n        \n        # Check for suspicious patterns\n        if data.get('status', 0) >= 400:\n            anomaly_score += 0.3\n        \n        if data.get('size', 0) > 10000000:  # Large response\n            anomaly_score += 0.2\n        \n        if self.is_suspicious_uri(data.get('uri', '')):\n            anomaly_score += 0.5\n        \n        data['anomaly_score'] = anomaly_score\n        data['is_anomalous'] = anomaly_score > 0.7\n        \n        return data\n    \n    def add_metadata(self, data):\n        \"\"\"Add processing metadata\"\"\"\n        \n        data['processed_at'] = datetime.utcnow().isoformat()\n        data['processor_version'] = '1.0.0'\n        data['partition_key'] = self.generate_partition_key(data)\n        \n        return data\n"})}),"\n",(0,s.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,s.jsx)(n.h3,{id:"1-buffering-configuration",children:"1. Buffering Configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class FirehoseOptimizer:\n    def __init__(self):\n        self.firehose = boto3.client('firehose')\n    \n    def optimize_buffering(self, delivery_stream_name, throughput_mbps):\n        \"\"\"Optimize buffering based on throughput\"\"\"\n        \n        if throughput_mbps < 1:\n            # Low throughput - optimize for latency\n            buffer_config = {\n                'SizeInMBs': 1,\n                'IntervalInSeconds': 60\n            }\n        elif throughput_mbps < 10:\n            # Medium throughput - balanced approach\n            buffer_config = {\n                'SizeInMBs': 5,\n                'IntervalInSeconds': 300\n            }\n        else:\n            # High throughput - optimize for efficiency\n            buffer_config = {\n                'SizeInMBs': 128,\n                'IntervalInSeconds': 900\n            }\n        \n        return buffer_config\n    \n    def configure_compression(self, data_type):\n        \"\"\"Choose optimal compression based on data type\"\"\"\n        \n        compression_map = {\n            'logs': 'GZIP',        # Good compression ratio\n            'json': 'GZIP',        # Good for structured data\n            'csv': 'GZIP',         # Good for tabular data\n            'binary': 'UNCOMPRESSED',  # Already compressed\n            'images': 'UNCOMPRESSED'   # Already compressed\n        }\n        \n        return compression_map.get(data_type, 'GZIP')\n    \n    def estimate_costs(self, records_per_second, avg_record_size_kb, hours_per_month):\n        \"\"\"Estimate Firehose costs\"\"\"\n        \n        # Calculate monthly data volume\n        monthly_records = records_per_second * 3600 * hours_per_month\n        monthly_gb = (monthly_records * avg_record_size_kb) / (1024 * 1024)\n        \n        # Firehose pricing (example rates)\n        cost_per_gb = 0.029  # $0.029 per GB\n        \n        estimated_cost = monthly_gb * cost_per_gb\n        \n        return {\n            'monthly_records': monthly_records,\n            'monthly_gb': monthly_gb,\n            'estimated_cost_usd': estimated_cost,\n            'cost_per_million_records': (estimated_cost / monthly_records) * 1000000\n        }\n"})}),"\n",(0,s.jsx)(n.h2,{id:"error-handling-v\xe0-monitoring",children:"Error Handling v\xe0 Monitoring"}),"\n",(0,s.jsx)(n.h3,{id:"1-comprehensive-error-handling",children:"1. Comprehensive Error Handling"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class FirehoseErrorHandler:\n    def __init__(self):\n        self.cloudwatch = boto3.client(\'cloudwatch\')\n        self.sns = boto3.client(\'sns\')\n    \n    def handle_delivery_errors(self, delivery_stream_name):\n        """Monitor and handle delivery errors"""\n        \n        # Get error metrics\n        error_metrics = self.get_error_metrics(delivery_stream_name)\n        \n        if error_metrics[\'error_rate\'] > 0.05:  # 5% error rate threshold\n            self.send_alert(\n                f"High error rate detected for {delivery_stream_name}",\n                error_metrics\n            )\n        \n        # Check for specific error patterns\n        self.analyze_error_patterns(delivery_stream_name)\n    \n    def get_error_metrics(self, delivery_stream_name):\n        """Get delivery error metrics"""\n        \n        end_time = datetime.utcnow()\n        start_time = end_time - timedelta(hours=1)\n        \n        # Get delivery success/failure metrics\n        success_response = self.cloudwatch.get_metric_statistics(\n            Namespace=\'AWS/KinesisFirehose\',\n            MetricName=\'DeliveryToS3.Success\',\n            Dimensions=[\n                {\n                    \'Name\': \'DeliveryStreamName\',\n                    \'Value\': delivery_stream_name\n                }\n            ],\n            StartTime=start_time,\n            EndTime=end_time,\n            Period=300,\n            Statistics=[\'Sum\']\n        )\n        \n        failure_response = self.cloudwatch.get_metric_statistics(\n            Namespace=\'AWS/KinesisFirehose\',\n            MetricName=\'DeliveryToS3.DataFreshness\',\n            Dimensions=[\n                {\n                    \'Name\': \'DeliveryStreamName\',\n                    \'Value\': delivery_stream_name\n                }\n            ],\n            StartTime=start_time,\n            EndTime=end_time,\n            Period=300,\n            Statistics=[\'Maximum\']\n        )\n        \n        total_success = sum([dp[\'Sum\'] for dp in success_response[\'Datapoints\']])\n        max_freshness = max([dp[\'Maximum\'] for dp in failure_response[\'Datapoints\']] or [0])\n        \n        return {\n            \'total_deliveries\': total_success,\n            \'max_data_freshness\': max_freshness,\n            \'error_rate\': 0 if total_success == 0 else (1 - total_success / (total_success + 1))\n        }\n    \n    def create_monitoring_dashboard(self, delivery_stream_names):\n        """Create CloudWatch dashboard for monitoring"""\n        \n        dashboard_body = {\n            "widgets": []\n        }\n        \n        for i, stream_name in enumerate(delivery_stream_names):\n            # Delivery metrics widget\n            widget = {\n                "type": "metric",\n                "properties": {\n                    "metrics": [\n                        ["AWS/KinesisFirehose", "DeliveryToS3.Records", "DeliveryStreamName", stream_name],\n                        [".", "DeliveryToS3.Bytes", ".", "."],\n                        [".", "DeliveryToS3.Success", ".", "."]\n                    ],\n                    "period": 300,\n                    "stat": "Sum",\n                    "region": "us-west-2",\n                    "title": f"Delivery Metrics - {stream_name}"\n                }\n            }\n            dashboard_body["widgets"].append(widget)\n        \n        self.cloudwatch.put_dashboard(\n            DashboardName=\'KinesisFirehose-Monitoring\',\n            DashboardBody=json.dumps(dashboard_body)\n        )\n'})}),"\n",(0,s.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Buffer Sizing"}),": Optimize buffer size v\xe0 interval cho throughput"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Compression"}),": S\u1eed d\u1ee5ng appropriate compression formats"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Partitioning"}),": Design effective S3 partitioning strategy"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Error Handling"}),": Implement comprehensive error handling"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Monitoring"}),": Set up detailed monitoring v\xe0 alerting"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cost Optimization"}),": Monitor v\xe0 optimize costs regularly"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Security"}),": Use IAM roles v\xe0 encrypt data in transit"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Testing"}),": Test v\u1edbi realistic data volumes"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"t\xedch-h\u1ee3p-v\u1edbi-c\xe1c-d\u1ecbch-v\u1ee5-aws-kh\xe1c",children:"T\xedch h\u1ee3p v\u1edbi c\xe1c d\u1ecbch v\u1ee5 AWS kh\xe1c"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Amazon Kinesis Data Streams"}),": Source for streaming data"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"AWS Lambda"}),": Data transformation functions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Amazon S3"}),": Primary destination for data lakes"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Amazon Redshift"}),": Data warehouse loading"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Amazon OpenSearch Service"}),": Search v\xe0 analytics"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"AWS Glue"}),": Data catalog integration"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Amazon Athena"}),": Query delivered data"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Amazon CloudWatch"}),": Monitoring v\xe0 alerting"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"AWS IAM"}),": Security v\xe0 access control"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Amazon VPC"}),": Network isolation"]}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}}}]);