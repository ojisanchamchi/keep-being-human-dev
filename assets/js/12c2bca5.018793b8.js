"use strict";(self.webpackChunkkeep_being_human_dev=self.webpackChunkkeep_being_human_dev||[]).push([[27038],{44851:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>p,frontMatter:()=>i,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"aws-nodes/aws-analytics/emr-hdfs-cluster","title":"EMR HDFS Cluster","description":"T\u1ed5ng quan","source":"@site/docs/aws-nodes/aws-analytics/14-emr-hdfs-cluster.md","sourceDirName":"aws-nodes/aws-analytics","slug":"/aws-nodes/aws-analytics/emr-hdfs-cluster","permalink":"/keep-being-human-dev/docs/aws-nodes/aws-analytics/emr-hdfs-cluster","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/aws-nodes/aws-analytics/14-emr-hdfs-cluster.md","tags":[],"version":"current","sidebarPosition":14,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"EMR Engine","permalink":"/keep-being-human-dev/docs/aws-nodes/aws-analytics/emr-engine"},"next":{"title":"Amazon EMR","permalink":"/keep-being-human-dev/docs/aws-nodes/aws-analytics/emr"}}');var s=a(23420),r=a(65404);const i={},o="EMR HDFS Cluster",c={},l=[{value:"T\u1ed5ng quan",id:"t\u1ed5ng-quan",level:2},{value:"Ch\u1ee9c n\u0103ng ch\xednh",id:"ch\u1ee9c-n\u0103ng-ch\xednh",level:2},{value:"1. HDFS Storage System",id:"1-hdfs-storage-system",level:3},{value:"2. Cluster Architecture",id:"2-cluster-architecture",level:3},{value:"3. Performance Features",id:"3-performance-features",level:3},{value:"4. Integration Capabilities",id:"4-integration-capabilities",level:3},{value:"Use Cases ph\u1ed5 bi\u1ebfn",id:"use-cases-ph\u1ed5-bi\u1ebfn",level:2},{value:"Diagram Architecture",id:"diagram-architecture",level:2},{value:"Code \u0111\u1ec3 t\u1ea1o diagram:",id:"code-\u0111\u1ec3-t\u1ea1o-diagram",level:3},{value:"HDFS Architecture",id:"hdfs-architecture",level:2},{value:"1. NameNode Configuration",id:"1-namenode-configuration",level:3},{value:"2. DataNode Configuration",id:"2-datanode-configuration",level:3},{value:"3. Block Management",id:"3-block-management",level:3},{value:"Storage Optimization",id:"storage-optimization",level:2},{value:"1. Block Size Tuning",id:"1-block-size-tuning",level:3},{value:"2. Compression Strategy",id:"2-compression-strategy",level:3},{value:"3. Data Placement Strategy",id:"3-data-placement-strategy",level:3},{value:"Performance Tuning",id:"performance-tuning",level:2},{value:"1. Memory Configuration",id:"1-memory-configuration",level:3},{value:"2. I/O Optimization",id:"2-io-optimization",level:3},{value:"3. Network Optimization",id:"3-network-optimization",level:3},{value:"Data Management",id:"data-management",level:2},{value:"1. Data Lifecycle Management",id:"1-data-lifecycle-management",level:3},{value:"2. Data Quality Monitoring",id:"2-data-quality-monitoring",level:3},{value:"Backup v\xe0 Recovery",id:"backup-v\xe0-recovery",level:2},{value:"1. HDFS Backup Strategy",id:"1-hdfs-backup-strategy",level:3},{value:"2. Disaster Recovery",id:"2-disaster-recovery",level:3},{value:"Monitoring v\xe0 Alerting",id:"monitoring-v\xe0-alerting",level:2},{value:"1. HDFS Health Monitoring",id:"1-hdfs-health-monitoring",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"T\xedch h\u1ee3p v\u1edbi c\xe1c d\u1ecbch v\u1ee5 AWS kh\xe1c",id:"t\xedch-h\u1ee3p-v\u1edbi-c\xe1c-d\u1ecbch-v\u1ee5-aws-kh\xe1c",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"emr-hdfs-cluster",children:"EMR HDFS Cluster"})}),"\n",(0,s.jsx)(n.h2,{id:"t\u1ed5ng-quan",children:"T\u1ed5ng quan"}),"\n",(0,s.jsx)(n.p,{children:"EMR HDFS Cluster l\xe0 node \u0111\u1ea1i di\u1ec7n cho Amazon EMR cluster v\u1edbi Hadoop Distributed File System (HDFS) l\xe0m primary storage layer. HDFS cung c\u1ea5p high-throughput access \u0111\u1ebfn application data v\xe0 fault-tolerant storage cho big data applications. Node n\xe0y bi\u1ec3u th\u1ecb ki\u1ebfn tr\xfac EMR cluster t\u1ed1i \u01b0u cho workloads \u0111\xf2i h\u1ecfi high-performance local storage v\xe0 data locality."}),"\n",(0,s.jsx)(n.h2,{id:"ch\u1ee9c-n\u0103ng-ch\xednh",children:"Ch\u1ee9c n\u0103ng ch\xednh"}),"\n",(0,s.jsx)(n.h3,{id:"1-hdfs-storage-system",children:"1. HDFS Storage System"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Distributed Storage"}),": Ph\xe2n t\xe1n d\u1eef li\u1ec7u tr\xean multiple nodes"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Fault Tolerance"}),": Automatic replication v\xe0 recovery"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"High Throughput"}),": Optimized cho sequential read/write"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Data Locality"}),": Co-locate computation v\u1edbi data"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"2-cluster-architecture",children:"2. Cluster Architecture"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"NameNode"}),": Metadata management v\xe0 namespace"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"DataNodes"}),": Actual data storage v\xe0 retrieval"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Secondary NameNode"}),": Checkpoint v\xe0 backup metadata"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Block Management"}),": Efficient data block distribution"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"3-performance-features",children:"3. Performance Features"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Block-based Storage"}),": Large block sizes cho efficiency"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Replication Strategy"}),": Configurable replication factor"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Rack Awareness"}),": Optimize placement across racks"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Compression"}),": Built-in compression support"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"4-integration-capabilities",children:"4. Integration Capabilities"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Hadoop Ecosystem"}),": Native integration v\u1edbi Hadoop tools"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Spark Integration"}),": Direct HDFS access t\u1eeb Spark"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Hive/HBase"}),": Warehouse v\xe0 NoSQL database support"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"MapReduce"}),": Optimized cho MapReduce workloads"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"use-cases-ph\u1ed5-bi\u1ebfn",children:"Use Cases ph\u1ed5 bi\u1ebfn"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Data Warehousing"}),": Large-scale data warehouse storage"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Log Processing"}),": Centralized log storage v\xe0 processing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Batch Analytics"}),": High-throughput batch processing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Data Archival"}),": Long-term data retention"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ETL Workloads"}),": Extract, transform, load operations"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"diagram-architecture",children:"Diagram Architecture"}),"\n",(0,s.jsx)(n.p,{children:"Ki\u1ebfn tr\xfac EMR HDFS Cluster v\u1edbi distributed storage:"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"EMR HDFS Cluster Architecture",src:a(91737).A+"",width:"2715",height:"1843"})}),"\n",(0,s.jsx)(n.h3,{id:"code-\u0111\u1ec3-t\u1ea1o-diagram",children:"Code \u0111\u1ec3 t\u1ea1o diagram:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.analytics import EMRHdfsCluster, Glue\nfrom diagrams.aws.storage import S3, EBS\nfrom diagrams.aws.compute import EC2\nfrom diagrams.aws.database import RDS\nfrom diagrams.aws.network import ELB\nfrom diagrams.aws.management import Cloudwatch\nfrom diagrams.aws.security import IAM\nfrom diagrams.onprem.client import Users\nfrom diagrams.onprem.analytics import Hadoop\n\nwith Diagram("EMR HDFS Cluster Architecture", show=False, direction="TB"):\n    \n    users = Users("Data Engineers")\n    \n    with Cluster("Data Ingestion"):\n        s3_raw = S3("Raw Data")\n        rds_source = RDS("Operational DB")\n        external_data = EC2("External Sources")\n    \n    with Cluster("EMR HDFS Cluster"):\n        hdfs_cluster = EMRHdfsCluster("HDFS Cluster")\n        \n        with Cluster("Master Services"):\n            namenode = Hadoop("NameNode")\n            secondary_nn = Hadoop("Secondary NameNode")\n            resource_manager = EC2("ResourceManager")\n        \n        with Cluster("Worker Nodes"):\n            datanode1 = EC2("DataNode 1\\n+ NodeManager")\n            datanode2 = EC2("DataNode 2\\n+ NodeManager")\n            datanode3 = EC2("DataNode 3\\n+ NodeManager")\n            datanode4 = EC2("DataNode 4\\n+ NodeManager")\n        \n        with Cluster("Local Storage"):\n            ebs1 = EBS("EBS Volume 1")\n            ebs2 = EBS("EBS Volume 2")\n            ebs3 = EBS("EBS Volume 3")\n            ebs4 = EBS("EBS Volume 4")\n    \n    with Cluster("Processing Frameworks"):\n        mapreduce = Hadoop("MapReduce")\n        spark_hdfs = EC2("Spark on HDFS")\n        hive_hdfs = EC2("Hive")\n        hbase = EC2("HBase")\n    \n    with Cluster("Output & Analytics"):\n        processed_hdfs = S3("Processed Data")\n        s3_backup = S3("HDFS Backup")\n        data_catalog = Glue("Data Catalog")\n        analytics_api = ELB("Analytics API")\n    \n    with Cluster("Monitoring & Management"):\n        monitoring = Cloudwatch("Cluster Monitoring")\n        security = IAM("Security & Access")\n        hdfs_ui = EC2("HDFS Web UI")\n    \n    # Data ingestion\n    s3_raw >> Edge(label="Import") >> hdfs_cluster\n    rds_source >> Edge(label="Sqoop Import") >> hdfs_cluster\n    external_data >> Edge(label="Batch Load") >> hdfs_cluster\n    \n    # HDFS cluster structure\n    hdfs_cluster >> namenode\n    hdfs_cluster >> secondary_nn\n    hdfs_cluster >> resource_manager\n    \n    # DataNodes and storage\n    hdfs_cluster >> datanode1 >> ebs1\n    hdfs_cluster >> datanode2 >> ebs2\n    hdfs_cluster >> datanode3 >> ebs3\n    hdfs_cluster >> datanode4 >> ebs4\n    \n    # NameNode coordination\n    namenode >> Edge(label="Block Metadata") >> [datanode1, datanode2, datanode3, datanode4]\n    \n    # Processing frameworks\n    hdfs_cluster >> mapreduce\n    hdfs_cluster >> spark_hdfs\n    hdfs_cluster >> hive_hdfs\n    hdfs_cluster >> hbase\n    \n    # Output and backup\n    spark_hdfs >> processed_hdfs\n    hdfs_cluster >> Edge(label="Backup") >> s3_backup\n    hive_hdfs >> data_catalog\n    mapreduce >> analytics_api\n    \n    # User interaction\n    users >> Edge(label="Submit Jobs") >> resource_manager\n    users >> Edge(label="Query Data") >> hive_hdfs\n    users >> Edge(label="Monitor") >> hdfs_ui\n    \n    # Monitoring and security\n    hdfs_cluster >> monitoring\n    security >> hdfs_cluster\n'})}),"\n",(0,s.jsx)(n.h2,{id:"hdfs-architecture",children:"HDFS Architecture"}),"\n",(0,s.jsx)(n.h3,{id:"1-namenode-configuration",children:"1. NameNode Configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:"\x3c!-- hdfs-site.xml for NameNode --\x3e\n<configuration>\n  <property>\n    <name>dfs.nameservices</name>\n    <value>emr-cluster</value>\n  </property>\n  \n  <property>\n    <name>dfs.ha.namenodes.emr-cluster</name>\n    <value>nn1,nn2</value>\n  </property>\n  \n  <property>\n    <name>dfs.namenode.rpc-address.emr-cluster.nn1</name>\n    <value>master1:8020</value>\n  </property>\n  \n  <property>\n    <name>dfs.namenode.http-address.emr-cluster.nn1</name>\n    <value>master1:50070</value>\n  </property>\n  \n  <property>\n    <name>dfs.namenode.name.dir</name>\n    <value>/mnt/hdfs/namenode</value>\n  </property>\n  \n  <property>\n    <name>dfs.replication</name>\n    <value>3</value>\n  </property>\n  \n  <property>\n    <name>dfs.blocksize</name>\n    <value>134217728</value> \x3c!-- 128MB --\x3e\n  </property>\n</configuration>\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-datanode-configuration",children:"2. DataNode Configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:"\x3c!-- hdfs-site.xml for DataNode --\x3e\n<configuration>\n  <property>\n    <name>dfs.datanode.data.dir</name>\n    <value>/mnt/hdfs/datanode1,/mnt/hdfs/datanode2</value>\n  </property>\n  \n  <property>\n    <name>dfs.datanode.handler.count</name>\n    <value>40</value>\n  </property>\n  \n  <property>\n    <name>dfs.datanode.max.transfer.threads</name>\n    <value>8192</value>\n  </property>\n  \n  <property>\n    <name>dfs.datanode.balance.bandwidthPerSec</name>\n    <value>104857600</value> \x3c!-- 100MB/s --\x3e\n  </property>\n  \n  <property>\n    <name>dfs.datanode.failed.volumes.tolerated</name>\n    <value>1</value>\n  </property>\n</configuration>\n"})}),"\n",(0,s.jsx)(n.h3,{id:"3-block-management",children:"3. Block Management"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# HDFS block management commands\n# Check filesystem health\nhdfs fsck / -files -blocks -locations\n\n# Balance cluster\nhdfs balancer -threshold 5\n\n# Set replication factor\nhdfs dfsadmin -setDefaultReplication 3\n\n# Decommission nodes\nhdfs dfsadmin -refreshNodes\n\n# Safe mode operations\nhdfs dfsadmin -safemode enter\nhdfs dfsadmin -safemode leave\n"})}),"\n",(0,s.jsx)(n.h2,{id:"storage-optimization",children:"Storage Optimization"}),"\n",(0,s.jsx)(n.h3,{id:"1-block-size-tuning",children:"1. Block Size Tuning"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Optimal block size calculation\ndef calculate_optimal_block_size(file_size_gb, cluster_size):\n    """\n    Calculate optimal HDFS block size based on file size and cluster\n    """\n    file_size_bytes = file_size_gb * 1024 * 1024 * 1024\n    \n    # Rule of thumb: aim for 100-1000 blocks per file\n    target_blocks = min(1000, max(100, cluster_size * 2))\n    optimal_block_size = file_size_bytes / target_blocks\n    \n    # Round to nearest power of 2, minimum 64MB, maximum 1GB\n    block_size = max(64 * 1024 * 1024, \n                    min(1024 * 1024 * 1024, \n                        2 ** round(math.log2(optimal_block_size))))\n    \n    return block_size\n\n# Example usage\nfile_size = 100  # GB\ncluster_nodes = 20\noptimal_size = calculate_optimal_block_size(file_size, cluster_nodes)\nprint(f"Optimal block size: {optimal_size / (1024*1024)} MB")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"2-compression-strategy",children:"2. Compression Strategy"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:'// Hadoop compression configuration\nConfiguration conf = new Configuration();\n\n// Enable compression\nconf.setBoolean("mapreduce.map.output.compress", true);\nconf.setClass("mapreduce.map.output.compress.codec", \n              SnappyCodec.class, CompressionCodec.class);\n\nconf.setBoolean("mapreduce.output.fileoutputformat.compress", true);\nconf.setClass("mapreduce.output.fileoutputformat.compress.codec",\n              GzipCodec.class, CompressionCodec.class);\n\n// Sequence file compression\nconf.setClass("mapreduce.output.fileoutputformat.compress.type",\n              SequenceFile.CompressionType.BLOCK, \n              SequenceFile.CompressionType.class);\n\n// Parquet compression\nconf.set("parquet.compression", "SNAPPY");\n'})}),"\n",(0,s.jsx)(n.h3,{id:"3-data-placement-strategy",children:"3. Data Placement Strategy"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# Rack awareness configuration\n# /etc/hadoop/conf/rack-topology.sh\n#!/bin/bash\nwhile [ $# -gt 0 ] ; do\n  nodeArg=$1\n  exec< /etc/hadoop/conf/rack-topology.data\n  result=""\n  while read line ; do\n    ar=( $line )\n    if [ "${ar[0]}" = "$nodeArg" ] ; then\n      result="${ar[1]}"\n    fi\n  done\n  shift\n  if [ -z "$result" ] ; then\n    echo -n "/default-rack "\n  else\n    echo -n "$result "\n  fi\ndone\n\n# rack-topology.data\nip-10-0-1-100 /rack1\nip-10-0-1-101 /rack1\nip-10-0-2-100 /rack2\nip-10-0-2-101 /rack2\n'})}),"\n",(0,s.jsx)(n.h2,{id:"performance-tuning",children:"Performance Tuning"}),"\n",(0,s.jsx)(n.h3,{id:"1-memory-configuration",children:"1. Memory Configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:"\x3c!-- mapred-site.xml --\x3e\n<configuration>\n  <property>\n    <name>mapreduce.map.memory.mb</name>\n    <value>4096</value>\n  </property>\n  \n  <property>\n    <name>mapreduce.reduce.memory.mb</name>\n    <value>8192</value>\n  </property>\n  \n  <property>\n    <name>mapreduce.map.java.opts</name>\n    <value>-Xmx3276m</value>\n  </property>\n  \n  <property>\n    <name>mapreduce.reduce.java.opts</name>\n    <value>-Xmx6553m</value>\n  </property>\n  \n  <property>\n    <name>yarn.app.mapreduce.am.resource.mb</name>\n    <value>2048</value>\n  </property>\n</configuration>\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-io-optimization",children:"2. I/O Optimization"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:"\x3c!-- core-site.xml --\x3e\n<configuration>\n  <property>\n    <name>io.file.buffer.size</name>\n    <value>131072</value> \x3c!-- 128KB --\x3e\n  </property>\n  \n  <property>\n    <name>fs.local.block.size</name>\n    <value>134217728</value> \x3c!-- 128MB --\x3e\n  </property>\n  \n  <property>\n    <name>io.sort.mb</name>\n    <value>512</value>\n  </property>\n  \n  <property>\n    <name>io.sort.factor</name>\n    <value>100</value>\n  </property>\n</configuration>\n"})}),"\n",(0,s.jsx)(n.h3,{id:"3-network-optimization",children:"3. Network Optimization"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Network tuning for HDFS\necho 'net.core.rmem_max = 134217728' >> /etc/sysctl.conf\necho 'net.core.wmem_max = 134217728' >> /etc/sysctl.conf\necho 'net.ipv4.tcp_rmem = 4096 87380 134217728' >> /etc/sysctl.conf\necho 'net.ipv4.tcp_wmem = 4096 65536 134217728' >> /etc/sysctl.conf\necho 'net.core.netdev_max_backlog = 5000' >> /etc/sysctl.conf\n\nsysctl -p\n"})}),"\n",(0,s.jsx)(n.h2,{id:"data-management",children:"Data Management"}),"\n",(0,s.jsx)(n.h3,{id:"1-data-lifecycle-management",children:"1. Data Lifecycle Management"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# HDFS data lifecycle automation\nimport subprocess\nimport datetime\nfrom pathlib import Path\n\nclass HDFSLifecycleManager:\n    def __init__(self, hdfs_client):\n        self.hdfs_client = hdfs_client\n    \n    def archive_old_data(self, path, days_old=90):\n        """Archive data older than specified days"""\n        cutoff_date = datetime.datetime.now() - datetime.timedelta(days=days_old)\n        \n        # List files in path\n        files = self.hdfs_client.list(path, status=True)\n        \n        for file_info in files:\n            if file_info.modification_time < cutoff_date.timestamp():\n                # Move to archive\n                archive_path = f"/archive/{file_info.path}"\n                self.hdfs_client.rename(file_info.path, archive_path)\n                print(f"Archived: {file_info.path} -> {archive_path}")\n    \n    def compress_data(self, path, compression=\'gzip\'):\n        """Compress data in specified path"""\n        cmd = [\n            \'hadoop\', \'jar\', \n            \'/opt/hadoop/share/hadoop/tools/lib/hadoop-archive-*.jar\',\n            \'-archiveName\', f\'archive_{datetime.date.today()}.har\',\n            \'-p\', path, \'/compressed/\'\n        ]\n        subprocess.run(cmd, check=True)\n    \n    def cleanup_temp_data(self, temp_path=\'/tmp\'):\n        """Clean up temporary data"""\n        cutoff = datetime.datetime.now() - datetime.timedelta(hours=24)\n        \n        files = self.hdfs_client.list(temp_path, status=True)\n        for file_info in files:\n            if file_info.modification_time < cutoff.timestamp():\n                self.hdfs_client.delete(file_info.path, recursive=True)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"2-data-quality-monitoring",children:"2. Data Quality Monitoring"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# HDFS data quality checks\nclass HDFSDataQualityMonitor:\n    def __init__(self, spark_session):\n        self.spark = spark_session\n    \n    def check_data_freshness(self, path, max_age_hours=24):\n        """Check if data is fresh enough"""\n        df = self.spark.read.parquet(path)\n        \n        # Assuming there\'s a timestamp column\n        latest_timestamp = df.agg({"timestamp": "max"}).collect()[0][0]\n        age_hours = (datetime.datetime.now() - latest_timestamp).total_seconds() / 3600\n        \n        return {\n            \'path\': path,\n            \'latest_timestamp\': latest_timestamp,\n            \'age_hours\': age_hours,\n            \'is_fresh\': age_hours <= max_age_hours\n        }\n    \n    def check_data_completeness(self, path, expected_partitions):\n        """Check if all expected partitions exist"""\n        df = self.spark.read.parquet(path)\n        actual_partitions = df.select("date_partition").distinct().count()\n        \n        return {\n            \'path\': path,\n            \'expected_partitions\': expected_partitions,\n            \'actual_partitions\': actual_partitions,\n            \'completeness_ratio\': actual_partitions / expected_partitions\n        }\n    \n    def check_schema_consistency(self, paths):\n        """Check schema consistency across multiple paths"""\n        schemas = {}\n        for path in paths:\n            df = self.spark.read.parquet(path)\n            schemas[path] = df.schema\n        \n        # Compare schemas\n        base_schema = list(schemas.values())[0]\n        inconsistencies = []\n        \n        for path, schema in schemas.items():\n            if schema != base_schema:\n                inconsistencies.append({\n                    \'path\': path,\n                    \'schema_diff\': self.compare_schemas(base_schema, schema)\n                })\n        \n        return inconsistencies\n'})}),"\n",(0,s.jsx)(n.h2,{id:"backup-v\xe0-recovery",children:"Backup v\xe0 Recovery"}),"\n",(0,s.jsx)(n.h3,{id:"1-hdfs-backup-strategy",children:"1. HDFS Backup Strategy"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'#!/bin/bash\n# HDFS backup script\n\nBACKUP_DATE=$(date +%Y%m%d)\nBACKUP_PATH="/backup/$BACKUP_DATE"\nSOURCE_PATH="/data"\n\n# Create backup directory\nhdfs dfs -mkdir -p $BACKUP_PATH\n\n# Backup using distcp\nhadoop distcp \\\n  -update \\\n  -delete \\\n  -skipcrccheck \\\n  $SOURCE_PATH \\\n  $BACKUP_PATH\n\n# Backup to S3\nhadoop distcp \\\n  -update \\\n  -delete \\\n  hdfs://cluster$SOURCE_PATH \\\n  s3a://backup-bucket/hdfs-backup/$BACKUP_DATE/\n\n# Verify backup\nhdfs fsck $BACKUP_PATH -files -blocks\n\necho "Backup completed: $BACKUP_PATH"\n'})}),"\n",(0,s.jsx)(n.h3,{id:"2-disaster-recovery",children:"2. Disaster Recovery"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# HDFS disaster recovery automation\nclass HDFSDisasterRecovery:\n    def __init__(self, primary_cluster, backup_cluster):\n        self.primary = primary_cluster\n        self.backup = backup_cluster\n    \n    def create_recovery_plan(self, critical_paths):\n        \"\"\"Create disaster recovery plan\"\"\"\n        recovery_plan = {\n            'timestamp': datetime.datetime.now(),\n            'critical_paths': critical_paths,\n            'recovery_steps': []\n        }\n        \n        for path in critical_paths:\n            # Check data size and replication\n            size_info = self.get_path_size(path)\n            recovery_plan['recovery_steps'].append({\n                'path': path,\n                'size_gb': size_info['size_gb'],\n                'estimated_recovery_time': size_info['size_gb'] / 10,  # 10GB/min\n                'priority': self.get_path_priority(path)\n            })\n        \n        return recovery_plan\n    \n    def execute_recovery(self, recovery_plan):\n        \"\"\"Execute disaster recovery\"\"\"\n        # Sort by priority\n        steps = sorted(recovery_plan['recovery_steps'], \n                      key=lambda x: x['priority'], reverse=True)\n        \n        for step in steps:\n            print(f\"Recovering {step['path']}...\")\n            \n            # Restore from backup\n            self.restore_from_backup(step['path'])\n            \n            # Verify data integrity\n            if self.verify_data_integrity(step['path']):\n                print(f\"\u2713 Successfully recovered {step['path']}\")\n            else:\n                print(f\"\u2717 Failed to recover {step['path']}\")\n    \n    def restore_from_backup(self, path):\n        \"\"\"Restore data from backup\"\"\"\n        backup_path = f\"s3a://backup-bucket/hdfs-backup/latest{path}\"\n        \n        cmd = [\n            'hadoop', 'distcp',\n            '-overwrite',\n            backup_path,\n            f'hdfs://cluster{path}'\n        ]\n        \n        subprocess.run(cmd, check=True)\n"})}),"\n",(0,s.jsx)(n.h2,{id:"monitoring-v\xe0-alerting",children:"Monitoring v\xe0 Alerting"}),"\n",(0,s.jsx)(n.h3,{id:"1-hdfs-health-monitoring",children:"1. HDFS Health Monitoring"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# HDFS health monitoring\nimport requests\nimport json\n\nclass HDFSHealthMonitor:\n    def __init__(self, namenode_host, namenode_port=50070):\n        self.base_url = f\"http://{namenode_host}:{namenode_port}\"\n    \n    def get_cluster_info(self):\n        \"\"\"Get cluster information\"\"\"\n        response = requests.get(f\"{self.base_url}/jmx?qry=Hadoop:service=NameNode,name=FSNamesystemState\")\n        data = response.json()\n        \n        return {\n            'total_capacity': data['beans'][0]['CapacityTotal'],\n            'used_capacity': data['beans'][0]['CapacityUsed'],\n            'remaining_capacity': data['beans'][0]['CapacityRemaining'],\n            'capacity_used_percent': data['beans'][0]['PercentUsed'],\n            'live_nodes': data['beans'][0]['NumLiveDataNodes'],\n            'dead_nodes': data['beans'][0]['NumDeadDataNodes']\n        }\n    \n    def check_datanode_health(self):\n        \"\"\"Check DataNode health\"\"\"\n        response = requests.get(f\"{self.base_url}/jmx?qry=Hadoop:service=NameNode,name=FSNamesystem\")\n        data = response.json()\n        \n        under_replicated = data['beans'][0]['UnderReplicatedBlocks']\n        corrupt_blocks = data['beans'][0]['CorruptBlocks']\n        missing_blocks = data['beans'][0]['MissingBlocks']\n        \n        return {\n            'under_replicated_blocks': under_replicated,\n            'corrupt_blocks': corrupt_blocks,\n            'missing_blocks': missing_blocks,\n            'health_status': 'healthy' if (under_replicated + corrupt_blocks + missing_blocks) == 0 else 'unhealthy'\n        }\n    \n    def generate_health_report(self):\n        \"\"\"Generate comprehensive health report\"\"\"\n        cluster_info = self.get_cluster_info()\n        datanode_health = self.check_datanode_health()\n        \n        report = {\n            'timestamp': datetime.datetime.now().isoformat(),\n            'cluster_info': cluster_info,\n            'datanode_health': datanode_health,\n            'alerts': []\n        }\n        \n        # Generate alerts\n        if cluster_info['capacity_used_percent'] > 80:\n            report['alerts'].append('High disk usage: {}%'.format(cluster_info['capacity_used_percent']))\n        \n        if cluster_info['dead_nodes'] > 0:\n            report['alerts'].append('{} DataNodes are dead'.format(cluster_info['dead_nodes']))\n        \n        if datanode_health['corrupt_blocks'] > 0:\n            report['alerts'].append('{} corrupt blocks detected'.format(datanode_health['corrupt_blocks']))\n        \n        return report\n"})}),"\n",(0,s.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Capacity Planning"}),": Monitor disk usage v\xe0 plan expansion"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Replication Strategy"}),": Set appropriate replication factor"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Block Size Optimization"}),": Choose optimal block size cho workload"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Rack Awareness"}),": Configure rack topology"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Regular Maintenance"}),": Schedule regular fsck v\xe0 balancer"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Backup Strategy"}),": Implement comprehensive backup plan"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Security"}),": Enable Kerberos v\xe0 encryption"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Monitoring"}),": Set up proactive monitoring v\xe0 alerting"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"t\xedch-h\u1ee3p-v\u1edbi-c\xe1c-d\u1ecbch-v\u1ee5-aws-kh\xe1c",children:"T\xedch h\u1ee3p v\u1edbi c\xe1c d\u1ecbch v\u1ee5 AWS kh\xe1c"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Amazon S3"}),": Backup v\xe0 archival storage"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Amazon EBS"}),": High-performance local storage"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"AWS Direct Connect"}),": High-bandwidth data transfer"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Amazon CloudWatch"}),": Monitoring v\xe0 alerting"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"AWS IAM"}),": Access control v\xe0 security"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Amazon VPC"}),": Network isolation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"AWS KMS"}),": Encryption key management"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"AWS Glue"}),": Data catalog integration"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Amazon EMR"}),": Managed Hadoop ecosystem"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Amazon Kinesis"}),": Real-time data ingestion"]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},65404:(e,n,a)=>{a.d(n,{R:()=>i,x:()=>o});var t=a(36672);const s={},r=t.createContext(s);function i(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),t.createElement(r.Provider,{value:n},e.children)}},91737:(e,n,a)=>{a.d(n,{A:()=>t});const t=a.p+"assets/images/emr-hdfs-cluster-4dc53fabd4053ca7c3ff6629d6183af3.png"}}]);