"use strict";(self.webpackChunkkeep_being_human_dev=self.webpackChunkkeep_being_human_dev||[]).push([[5894],{407:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>i,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"gems/ruby-openai/middle/streaming_responses","title":"streaming_responses","description":"\ud83c\udfa5 Use Streaming for Real\u2011Time Responses","source":"@site/docs/gems/ruby-openai/middle/streaming_responses.md","sourceDirName":"gems/ruby-openai/middle","slug":"/gems/ruby-openai/middle/streaming_responses","permalink":"/keep-being-human-dev/docs/gems/ruby-openai/middle/streaming_responses","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/gems/ruby-openai/middle/streaming_responses.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"parallel_requests","permalink":"/keep-being-human-dev/docs/gems/ruby-openai/middle/parallel_requests"},"next":{"title":"batch_orchestration","permalink":"/keep-being-human-dev/docs/gems/sidekiq/advanced/batch_orchestration"}}');var r=s(23420),a=s(65404);const i={},o=void 0,l={},c=[{value:"\ud83c\udfa5 Use Streaming for Real\u2011Time Responses",id:"-use-streaming-for-realtime-responses",level:2}];function d(e){const n={code:"code",h2:"h2",p:"p",pre:"pre",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h2,{id:"-use-streaming-for-realtime-responses",children:"\ud83c\udfa5 Use Streaming for Real\u2011Time Responses"}),"\n",(0,r.jsxs)(n.p,{children:["When interacting with large language models, you can process partial outputs as they arrive by enabling streaming. This reduces latency and lets you display or process tokens in real time. Simply set ",(0,r.jsx)(n.code,{children:"stream: proc"})," and handle each chunk in the callback."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-ruby",children:'require "ruby/openai"\n\nclient = OpenAI::Client.new\n\nclient.chat.completions(\n  parameters: {\n    model: "gpt-4o-mini",\n    messages: [{ role: "user", content: "Explain streaming in Ruby." }],\n    stream: proc { |chunk| print chunk.dig("choices", 0, "delta", "content") }\n  }\n)\n'})}),"\n",(0,r.jsx)(n.p,{children:"This will print tokens as they arrive, allowing you to build a live UI or log partial data without waiting for the full response."})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},65404:(e,n,s)=>{s.d(n,{R:()=>i,x:()=>o});var t=s(36672);const r={},a=t.createContext(r);function i(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);