"use strict";(self.webpackChunkkeep_being_human_dev=self.webpackChunkkeep_being_human_dev||[]).push([[84567],{37645:(n,e,s)=>{s.d(e,{A:()=>t});const t=s.p+"assets/images/emr-4d955fcfd8f84ee5ef826714303a608f.png"},49648:(n,e,s)=>{s.r(e),s.d(e,{assets:()=>c,contentTitle:()=>o,default:()=>m,frontMatter:()=>a,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"diagrams/aws-nodes/aws-analytics/emr","title":"Amazon EMR","description":"T\u1ed5ng quan","source":"@site/docs/diagrams/aws-nodes/aws-analytics/15-emr.md","sourceDirName":"diagrams/aws-nodes/aws-analytics","slug":"/diagrams/aws-nodes/aws-analytics/emr","permalink":"/keep-being-human-dev/docs/diagrams/aws-nodes/aws-analytics/emr","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/diagrams/aws-nodes/aws-analytics/15-emr.md","tags":[],"version":"current","sidebarPosition":15,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"EMR HDFS Cluster","permalink":"/keep-being-human-dev/docs/diagrams/aws-nodes/aws-analytics/emr-hdfs-cluster"},"next":{"title":"AWS Glue Crawlers","permalink":"/keep-being-human-dev/docs/diagrams/aws-nodes/aws-analytics/glue-crawlers"}}');var i=s(23420),r=s(65404);const a={},o="Amazon EMR",c={},l=[{value:"T\u1ed5ng quan",id:"t\u1ed5ng-quan",level:2},{value:"Ch\u1ee9c n\u0103ng ch\xednh",id:"ch\u1ee9c-n\u0103ng-ch\xednh",level:2},{value:"1. Managed Big Data Platform",id:"1-managed-big-data-platform",level:3},{value:"2. Flexible Deployment Options",id:"2-flexible-deployment-options",level:3},{value:"3. Storage Integration",id:"3-storage-integration",level:3},{value:"4. Enterprise Features",id:"4-enterprise-features",level:3},{value:"Use Cases ph\u1ed5 bi\u1ebfn",id:"use-cases-ph\u1ed5-bi\u1ebfn",level:2},{value:"Diagram Architecture",id:"diagram-architecture",level:2},{value:"Code \u0111\u1ec3 t\u1ea1o diagram:",id:"code-\u0111\u1ec3-t\u1ea1o-diagram",level:3},{value:"EMR Deployment Options",id:"emr-deployment-options",level:2},{value:"1. EMR on EC2",id:"1-emr-on-ec2",level:3},{value:"2. EMR on EKS",id:"2-emr-on-eks",level:3},{value:"3. EMR Serverless",id:"3-emr-serverless",level:3},{value:"Framework Integration",id:"framework-integration",level:2},{value:"1. Apache Spark on EMR",id:"1-apache-spark-on-emr",level:3},{value:"2. Apache Flink on EMR",id:"2-apache-flink-on-emr",level:3},{value:"3. Presto on EMR",id:"3-presto-on-emr",level:3},{value:"Cost Optimization Strategies",id:"cost-optimization-strategies",level:2},{value:"1. Spot Instance Strategy",id:"1-spot-instance-strategy",level:3},{value:"2. EMR Managed Scaling",id:"2-emr-managed-scaling",level:3},{value:"3. Cost Monitoring",id:"3-cost-monitoring",level:3},{value:"Security Best Practices",id:"security-best-practices",level:2},{value:"1. Encryption Configuration",id:"1-encryption-configuration",level:3},{value:"2. IAM Roles v\xe0 Policies",id:"2-iam-roles-v\xe0-policies",level:3},{value:"Monitoring v\xe0 Troubleshooting",id:"monitoring-v\xe0-troubleshooting",level:2},{value:"1. CloudWatch Integration",id:"1-cloudwatch-integration",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"T\xedch h\u1ee3p v\u1edbi c\xe1c d\u1ecbch v\u1ee5 AWS kh\xe1c",id:"t\xedch-h\u1ee3p-v\u1edbi-c\xe1c-d\u1ecbch-v\u1ee5-aws-kh\xe1c",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"amazon-emr",children:"Amazon EMR"})}),"\n",(0,i.jsx)(e.h2,{id:"t\u1ed5ng-quan",children:"T\u1ed5ng quan"}),"\n",(0,i.jsx)(e.p,{children:"Amazon EMR (Elastic MapReduce) l\xe0 node t\u1ed5ng qu\xe1t \u0111\u1ea1i di\u1ec7n cho d\u1ecbch v\u1ee5 big data platform \u0111\u01b0\u1ee3c qu\u1ea3n l\xfd ho\xe0n to\xe0n c\u1ee7a AWS. EMR gi\xfap b\u1ea1n d\u1ec5 d\xe0ng ch\u1ea1y v\xe0 m\u1edf r\u1ed9ng c\xe1c framework big data nh\u01b0 Apache Spark, Apache Hadoop, Apache HBase, Apache Flink, Apache Hudi, v\xe0 Presto tr\xean AWS. Node n\xe0y bi\u1ec3u th\u1ecb to\xe0n b\u1ed9 EMR ecosystem v\u1edbi kh\u1ea3 n\u0103ng x\u1eed l\xfd petabyte-scale data m\u1ed9t c\xe1ch cost-effective."}),"\n",(0,i.jsx)(e.h2,{id:"ch\u1ee9c-n\u0103ng-ch\xednh",children:"Ch\u1ee9c n\u0103ng ch\xednh"}),"\n",(0,i.jsx)(e.h3,{id:"1-managed-big-data-platform",children:"1. Managed Big Data Platform"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Framework Support"}),": Spark, Hadoop, Flink, Presto, HBase, Hive"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Auto Scaling"}),": T\u1ef1 \u0111\u1ed9ng \u0111i\u1ec1u ch\u1ec9nh cluster size"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Spot Integration"}),": S\u1eed d\u1ee5ng EC2 Spot Instances"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Serverless Options"}),": EMR Serverless cho serverless analytics"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"2-flexible-deployment-options",children:"2. Flexible Deployment Options"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"EC2 Clusters"}),": Traditional cluster deployment"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"EKS Integration"}),": Run on Amazon EKS"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Outposts"}),": On-premises deployment"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Serverless"}),": Pay-per-use serverless execution"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"3-storage-integration",children:"3. Storage Integration"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Amazon S3"}),": Primary data lake storage"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"HDFS"}),": Local distributed storage"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"EBS"}),": High-performance block storage"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"EFS"}),": Shared file system"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"4-enterprise-features",children:"4. Enterprise Features"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Security"}),": Encryption, Kerberos, IAM integration"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Monitoring"}),": CloudWatch integration"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Notebooks"}),": EMR Notebooks cho interactive development"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Studio"}),": EMR Studio cho collaborative analytics"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"use-cases-ph\u1ed5-bi\u1ebfn",children:"Use Cases ph\u1ed5 bi\u1ebfn"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Data Processing"}),": Large-scale ETL operations"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Machine Learning"}),": ML model training v\xe0 inference"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Interactive Analytics"}),": Ad-hoc data exploration"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Streaming Analytics"}),": Real-time data processing"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Data Science"}),": Research v\xe0 experimentation"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"diagram-architecture",children:"Diagram Architecture"}),"\n",(0,i.jsx)(e.p,{children:"Ki\u1ebfn tr\xfac t\u1ed5ng quan Amazon EMR ecosystem:"}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.img,{alt:"Amazon EMR Architecture",src:s(37645).A+"",width:"2365",height:"1981"})}),"\n",(0,i.jsx)(e.h3,{id:"code-\u0111\u1ec3-t\u1ea1o-diagram",children:"Code \u0111\u1ec3 t\u1ea1o diagram:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.analytics import EMR, Kinesis, Glue, Athena\nfrom diagrams.aws.storage import S3, EBS\nfrom diagrams.aws.compute import EC2, EKS, Lambda\nfrom diagrams.aws.database import RDS, Dynamodb\nfrom diagrams.aws.ml import SagemakerModel, SagemakerNotebook\nfrom diagrams.aws.network import ELB, VPC\nfrom diagrams.aws.management import Cloudwatch\nfrom diagrams.aws.security import IAM\nfrom diagrams.onprem.client import Users\n\nwith Diagram("Amazon EMR Architecture", show=False, direction="TB"):\n    \n    users = Users("Data Teams")\n    \n    with Cluster("Data Sources"):\n        s3_data_lake = S3("Data Lake")\n        rds_oltp = RDS("OLTP Systems")\n        kinesis_streams = Kinesis("Streaming Data")\n        external_apis = Lambda("External APIs")\n    \n    with Cluster("EMR Ecosystem"):\n        emr_service = EMR("Amazon EMR")\n        \n        with Cluster("Deployment Options"):\n            emr_ec2 = EC2("EMR on EC2")\n            emr_eks = EKS("EMR on EKS")\n            emr_serverless = Lambda("EMR Serverless")\n        \n        with Cluster("Processing Frameworks"):\n            spark = EC2("Apache Spark")\n            hadoop = EC2("Apache Hadoop")\n            flink = EC2("Apache Flink")\n            presto = EC2("Presto")\n            hbase = EC2("Apache HBase")\n    \n    with Cluster("Development Environment"):\n        emr_studio = SagemakerNotebook("EMR Studio")\n        emr_notebooks = SagemakerNotebook("EMR Notebooks")\n        jupyter = EC2("Jupyter Hub")\n    \n    with Cluster("Storage Layer"):\n        s3_storage = S3("S3 Storage")\n        hdfs_storage = EBS("HDFS Storage")\n        efs_storage = EBS("EFS Storage")\n    \n    with Cluster("Analytics & ML"):\n        athena = Athena("Athena")\n        sagemaker = SagemakerModel("SageMaker")\n        glue_catalog = Glue("Data Catalog")\n        quicksight = EC2("QuickSight")\n    \n    with Cluster("Infrastructure"):\n        vpc = VPC("VPC")\n        load_balancer = ELB("Load Balancer")\n        monitoring = Cloudwatch("Monitoring")\n        security = IAM("Security")\n    \n    # Data ingestion\n    s3_data_lake >> Edge(label="Batch Data") >> emr_service\n    rds_oltp >> Edge(label="CDC") >> emr_service\n    kinesis_streams >> Edge(label="Streaming") >> emr_service\n    external_apis >> Edge(label="API Data") >> emr_service\n    \n    # EMR deployment options\n    emr_service >> emr_ec2\n    emr_service >> emr_eks\n    emr_service >> emr_serverless\n    \n    # Processing frameworks\n    emr_ec2 >> spark\n    emr_ec2 >> hadoop\n    emr_eks >> flink\n    emr_serverless >> spark\n    \n    # Development environment\n    users >> emr_studio\n    users >> emr_notebooks\n    emr_studio >> emr_service\n    emr_notebooks >> jupyter\n    \n    # Storage integration\n    spark >> s3_storage\n    hadoop >> hdfs_storage\n    flink >> efs_storage\n    \n    # Analytics integration\n    spark >> athena\n    emr_service >> sagemaker\n    hadoop >> glue_catalog\n    emr_service >> quicksight\n    \n    # Infrastructure\n    emr_service >> vpc\n    emr_service >> load_balancer\n    emr_service >> monitoring\n    security >> emr_service\n    \n    # User interaction\n    users >> Edge(label="Submit Jobs") >> emr_service\n    users >> Edge(label="Monitor") >> monitoring\n'})}),"\n",(0,i.jsx)(e.h2,{id:"emr-deployment-options",children:"EMR Deployment Options"}),"\n",(0,i.jsx)(e.h3,{id:"1-emr-on-ec2",children:"1. EMR on EC2"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# EMR cluster configuration\nimport boto3\n\nemr = boto3.client('emr')\n\ncluster_config = {\n    'Name': 'production-analytics-cluster',\n    'ReleaseLabel': 'emr-6.9.0',\n    'Applications': [\n        {'Name': 'Spark'},\n        {'Name': 'Hadoop'},\n        {'Name': 'Hive'},\n        {'Name': 'Presto'},\n        {'Name': 'JupyterHub'}\n    ],\n    'Instances': {\n        'InstanceGroups': [\n            {\n                'Name': 'Master',\n                'Market': 'ON_DEMAND',\n                'InstanceRole': 'MASTER',\n                'InstanceType': 'm5.xlarge',\n                'InstanceCount': 1\n            },\n            {\n                'Name': 'Core',\n                'Market': 'ON_DEMAND',\n                'InstanceRole': 'CORE',\n                'InstanceType': 'm5.xlarge',\n                'InstanceCount': 2\n            },\n            {\n                'Name': 'Task',\n                'Market': 'SPOT',\n                'InstanceRole': 'TASK',\n                'InstanceType': 'm5.xlarge',\n                'InstanceCount': 4,\n                'BidPrice': '0.10'\n            }\n        ],\n        'Ec2KeyName': 'my-key-pair',\n        'KeepJobFlowAliveWhenNoSteps': True,\n        'TerminationProtected': False\n    },\n    'ServiceRole': 'EMR_DefaultRole',\n    'JobFlowRole': 'EMR_EC2_DefaultRole',\n    'LogUri': 's3://my-emr-logs/',\n    'BootstrapActions': [\n        {\n            'Name': 'Install Additional Packages',\n            'ScriptBootstrapAction': {\n                'Path': 's3://my-bootstrap-scripts/install-packages.sh'\n            }\n        }\n    ]\n}\n\nresponse = emr.run_job_flow(**cluster_config)\ncluster_id = response['JobFlowId']\n"})}),"\n",(0,i.jsx)(e.h3,{id:"2-emr-on-eks",children:"2. EMR on EKS"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-yaml",children:'# EMR on EKS configuration\napiVersion: emrcontainers.aws.crossplane.io/v1alpha1\nkind: VirtualCluster\nmetadata:\n  name: emr-eks-cluster\nspec:\n  forProvider:\n    name: analytics-virtual-cluster\n    containerProvider:\n      - id: my-eks-cluster\n        type: EKS\n        info:\n          eksInfo:\n            - namespace: emr\n    region: us-west-2\n  providerConfigRef:\n    name: aws-provider-config\n\n---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: spark-job-emr-eks\nspec:\n  template:\n    spec:\n      containers:\n      - name: spark-submit\n        image: amazon/aws-cli\n        command: ["aws", "emr-containers", "start-job-run"]\n        args:\n        - "--virtual-cluster-id"\n        - "$(VIRTUAL_CLUSTER_ID)"\n        - "--name"\n        - "spark-analytics-job"\n        - "--execution-role-arn"\n        - "$(EXECUTION_ROLE_ARN)"\n        - "--release-label"\n        - "emr-6.9.0-latest"\n        - "--job-driver"\n        - |\n          {\n            "sparkSubmitJobDriver": {\n              "entryPoint": "s3://my-bucket/spark-job.py",\n              "sparkSubmitParameters": "--conf spark.executor.instances=10"\n            }\n          }\n'})}),"\n",(0,i.jsx)(e.h3,{id:"3-emr-serverless",children:"3. EMR Serverless"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# EMR Serverless application\nimport boto3\n\nemr_serverless = boto3.client('emr-serverless')\n\n# Create application\napp_response = emr_serverless.create_application(\n    name='analytics-serverless-app',\n    releaseLabel='emr-6.9.0',\n    type='Spark',\n    initialCapacity={\n        'Driver': {\n            'workerCount': 1,\n            'workerConfiguration': {\n                'cpu': '2 vCPU',\n                'memory': '4 GB'\n            }\n        },\n        'Executor': {\n            'workerCount': 10,\n            'workerConfiguration': {\n                'cpu': '4 vCPU',\n                'memory': '8 GB'\n            }\n        }\n    },\n    maximumCapacity={\n        'cpu': '400 vCPU',\n        'memory': '1000 GB'\n    },\n    autoStartConfiguration={\n        'enabled': True\n    },\n    autoStopConfiguration={\n        'enabled': True,\n        'idleTimeoutMinutes': 15\n    }\n)\n\napplication_id = app_response['applicationId']\n\n# Submit job\njob_response = emr_serverless.start_job_run(\n    applicationId=application_id,\n    executionRoleArn='arn:aws:iam::account:role/EMRServerlessRole',\n    jobDriver={\n        'sparkSubmit': {\n            'entryPoint': 's3://my-bucket/analytics-job.py',\n            'entryPointArguments': ['--input', 's3://data/input/', '--output', 's3://data/output/'],\n            'sparkSubmitParameters': '--conf spark.sql.adaptive.enabled=true'\n        }\n    },\n    configurationOverrides={\n        'monitoringConfiguration': {\n            's3MonitoringConfiguration': {\n                'logUri': 's3://my-logs/emr-serverless/'\n            }\n        }\n    }\n)\n"})}),"\n",(0,i.jsx)(e.h2,{id:"framework-integration",children:"Framework Integration"}),"\n",(0,i.jsx)(e.h3,{id:"1-apache-spark-on-emr",children:"1. Apache Spark on EMR"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# Optimized Spark configuration for EMR\nfrom pyspark.sql import SparkSession\nfrom pyspark.conf import SparkConf\n\n# EMR-optimized Spark configuration\nconf = SparkConf() \\\n    .setAppName("EMR-Optimized-Analytics") \\\n    .set("spark.sql.adaptive.enabled", "true") \\\n    .set("spark.sql.adaptive.coalescePartitions.enabled", "true") \\\n    .set("spark.sql.adaptive.skewJoin.enabled", "true") \\\n    .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \\\n    .set("spark.hadoop.fs.s3a.fast.upload", "true") \\\n    .set("spark.hadoop.fs.s3a.block.size", "134217728") \\\n    .set("spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version", "2") \\\n    .set("spark.speculation", "true") \\\n    .set("spark.sql.execution.arrow.pyspark.enabled", "true")\n\nspark = SparkSession.builder.config(conf=conf).getOrCreate()\n\n# Read from S3 with optimizations\ndf = spark.read \\\n    .option("multiline", "true") \\\n    .option("inferSchema", "true") \\\n    .parquet("s3://data-lake/events/")\n\n# Optimized transformations\nresult = df \\\n    .repartition(200) \\\n    .cache() \\\n    .groupBy("user_id", "event_date") \\\n    .agg(\n        count("*").alias("event_count"),\n        sum("revenue").alias("total_revenue")\n    ) \\\n    .coalesce(50)\n\n# Write with optimizations\nresult.write \\\n    .mode("overwrite") \\\n    .option("compression", "snappy") \\\n    .partitionBy("event_date") \\\n    .parquet("s3://data-lake/user-metrics/")\n'})}),"\n",(0,i.jsx)(e.h3,{id:"2-apache-flink-on-emr",children:"2. Apache Flink on EMR"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-java",children:'// Flink streaming job on EMR\npublic class EMRFlinkStreamingJob {\n    public static void main(String[] args) throws Exception {\n        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n        \n        // EMR-specific configuration\n        env.setParallelism(16);\n        env.enableCheckpointing(60000, CheckpointingMode.EXACTLY_ONCE);\n        env.getCheckpointConfig().setCheckpointStorage("s3://checkpoints/flink/");\n        \n        // Kinesis source\n        Properties kinesisProps = new Properties();\n        kinesisProps.setProperty(ConsumerConfigConstants.AWS_REGION, "us-west-2");\n        kinesisProps.setProperty(ConsumerConfigConstants.STREAM_INITIAL_POSITION, "LATEST");\n        \n        DataStream<Event> events = env\n            .addSource(new FlinkKinesisConsumer<>("analytics-stream", \n                                                 new EventDeserializer(), \n                                                 kinesisProps))\n            .assignTimestampsAndWatermarks(\n                WatermarkStrategy.<Event>forBoundedOutOfOrderness(Duration.ofSeconds(10))\n                    .withTimestampAssigner((event, timestamp) -> event.getTimestamp())\n            );\n        \n        // Stream processing\n        DataStream<UserMetrics> metrics = events\n            .keyBy(Event::getUserId)\n            .window(TumblingEventTimeWindows.of(Time.minutes(5)))\n            .aggregate(new UserMetricsAggregator());\n        \n        // S3 sink\n        StreamingFileSink<UserMetrics> s3Sink = StreamingFileSink\n            .forRowFormat(new Path("s3://analytics-output/user-metrics/"), \n                         new SimpleStringEncoder<UserMetrics>("UTF-8"))\n            .withRollingPolicy(DefaultRollingPolicy.builder()\n                .withRolloverInterval(TimeUnit.MINUTES.toMillis(15))\n                .withInactivityInterval(TimeUnit.MINUTES.toMillis(5))\n                .build())\n            .build();\n        \n        metrics.addSink(s3Sink);\n        \n        env.execute("EMR Flink Streaming Analytics");\n    }\n}\n'})}),"\n",(0,i.jsx)(e.h3,{id:"3-presto-on-emr",children:"3. Presto on EMR"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-sql",children:"-- Presto queries optimized for EMR\n-- Create external table pointing to S3\nCREATE TABLE events (\n  event_id VARCHAR,\n  user_id VARCHAR,\n  event_type VARCHAR,\n  timestamp BIGINT,\n  properties MAP(VARCHAR, VARCHAR)\n)\nWITH (\n  format = 'PARQUET',\n  external_location = 's3://data-lake/events/',\n  partitioned_by = ARRAY['date_partition']\n);\n\n-- Optimized analytical query\nWITH user_sessions AS (\n  SELECT \n    user_id,\n    date_partition,\n    COUNT(*) as event_count,\n    MIN(timestamp) as session_start,\n    MAX(timestamp) as session_end,\n    MAX(timestamp) - MIN(timestamp) as session_duration\n  FROM events\n  WHERE date_partition >= '2023-01-01'\n    AND event_type IN ('page_view', 'click', 'purchase')\n  GROUP BY user_id, date_partition\n),\nuser_metrics AS (\n  SELECT \n    user_id,\n    AVG(session_duration) as avg_session_duration,\n    SUM(event_count) as total_events,\n    COUNT(DISTINCT date_partition) as active_days\n  FROM user_sessions\n  GROUP BY user_id\n)\nSELECT \n  CASE \n    WHEN avg_session_duration > 300 THEN 'engaged'\n    WHEN avg_session_duration > 60 THEN 'moderate'\n    ELSE 'casual'\n  END as user_segment,\n  COUNT(*) as user_count,\n  AVG(total_events) as avg_events_per_user,\n  AVG(active_days) as avg_active_days\nFROM user_metrics\nGROUP BY 1\nORDER BY user_count DESC;\n"})}),"\n",(0,i.jsx)(e.h2,{id:"cost-optimization-strategies",children:"Cost Optimization Strategies"}),"\n",(0,i.jsx)(e.h3,{id:"1-spot-instance-strategy",children:"1. Spot Instance Strategy"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# Advanced Spot instance configuration\nspot_fleet_config = {\n    'Name': 'cost-optimized-emr-cluster',\n    'ReleaseLabel': 'emr-6.9.0',\n    'Instances': {\n        'InstanceFleets': [\n            {\n                'Name': 'MasterFleet',\n                'InstanceFleetType': 'MASTER',\n                'TargetOnDemandCapacity': 1,\n                'InstanceTypeConfigs': [\n                    {\n                        'InstanceType': 'm5.xlarge',\n                        'WeightedCapacity': 1\n                    }\n                ]\n            },\n            {\n                'Name': 'CoreFleet',\n                'InstanceFleetType': 'CORE',\n                'TargetOnDemandCapacity': 2,\n                'TargetSpotCapacity': 8,\n                'InstanceTypeConfigs': [\n                    {\n                        'InstanceType': 'm5.xlarge',\n                        'WeightedCapacity': 1,\n                        'BidPrice': '0.08',\n                        'EbsConfiguration': {\n                            'EbsBlockDeviceConfigs': [\n                                {\n                                    'VolumeSpecification': {\n                                        'VolumeType': 'gp3',\n                                        'SizeInGB': 100\n                                    },\n                                    'VolumesPerInstance': 1\n                                }\n                            ]\n                        }\n                    },\n                    {\n                        'InstanceType': 'm5.2xlarge',\n                        'WeightedCapacity': 2,\n                        'BidPrice': '0.16'\n                    },\n                    {\n                        'InstanceType': 'c5.2xlarge',\n                        'WeightedCapacity': 2,\n                        'BidPrice': '0.15'\n                    }\n                ]\n            }\n        ]\n    },\n    'AutoTerminationPolicy': {\n        'IdleTimeout': 3600  # 1 hour\n    }\n}\n"})}),"\n",(0,i.jsx)(e.h3,{id:"2-emr-managed-scaling",children:"2. EMR Managed Scaling"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# Configure managed scaling\nmanaged_scaling_config = {\n    'ComputeLimits': {\n        'UnitType': 'Instances',\n        'MinimumCapacityUnits': 2,\n        'MaximumCapacityUnits': 20,\n        'MaximumOnDemandCapacityUnits': 5,\n        'MaximumCoreCapacityUnits': 10\n    }\n}\n\n# Apply to cluster\nemr.put_managed_scaling_policy(\n    ClusterId=cluster_id,\n    ManagedScalingPolicy=managed_scaling_config\n)\n"})}),"\n",(0,i.jsx)(e.h3,{id:"3-cost-monitoring",children:"3. Cost Monitoring"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# EMR cost monitoring\nclass EMRCostMonitor:\n    def __init__(self, cluster_id):\n        self.cluster_id = cluster_id\n        self.emr = boto3.client('emr')\n        self.ce = boto3.client('ce')\n    \n    def get_cluster_cost(self, start_date, end_date):\n        \"\"\"Get cluster cost for date range\"\"\"\n        response = self.ce.get_cost_and_usage(\n            TimePeriod={\n                'Start': start_date,\n                'End': end_date\n            },\n            Granularity='DAILY',\n            Metrics=['BlendedCost'],\n            GroupBy=[\n                {\n                    'Type': 'DIMENSION',\n                    'Key': 'SERVICE'\n                }\n            ],\n            Filter={\n                'Dimensions': {\n                    'Key': 'SERVICE',\n                    'Values': ['Amazon Elastic MapReduce']\n                }\n            }\n        )\n        \n        return response['ResultsByTime']\n    \n    def optimize_cluster_config(self, workload_pattern):\n        \"\"\"Suggest optimizations based on workload\"\"\"\n        recommendations = []\n        \n        if workload_pattern['avg_cpu_utilization'] < 50:\n            recommendations.append(\"Consider using smaller instance types\")\n        \n        if workload_pattern['peak_hours']:\n            recommendations.append(\"Use scheduled scaling for predictable peaks\")\n        \n        if workload_pattern['batch_jobs']:\n            recommendations.append(\"Consider EMR Serverless for batch workloads\")\n        \n        return recommendations\n"})}),"\n",(0,i.jsx)(e.h2,{id:"security-best-practices",children:"Security Best Practices"}),"\n",(0,i.jsx)(e.h3,{id:"1-encryption-configuration",children:"1. Encryption Configuration"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# EMR security configuration\nsecurity_config = {\n    'Name': 'emr-security-config',\n    'EncryptionConfiguration': {\n        'AtRestEncryptionConfiguration': {\n            'S3EncryptionConfiguration': {\n                'EncryptionMode': 'SSE-S3'\n            },\n            'LocalDiskEncryptionConfiguration': {\n                'EncryptionKeyProviderType': 'AwsKms',\n                'AwsKmsKey': 'arn:aws:kms:region:account:key/key-id'\n            }\n        },\n        'InTransitEncryptionConfiguration': {\n            'TLSCertificateConfiguration': {\n                'CertificateProviderType': 'PEM',\n                'S3Object': 's3://my-certs/certificate.pem'\n            }\n        }\n    },\n    'AuthenticationConfiguration': {\n        'KerberosConfiguration': {\n            'Provider': 'ClusterDedicatedKdc',\n            'ClusterDedicatedKdcConfiguration': {\n                'TicketLifetimeInHours': 24,\n                'CrossRealmTrustConfiguration': {\n                    'Realm': 'AD.DOMAIN.COM',\n                    'Domain': 'ad.domain.com',\n                    'AdminServer': 'ad.domain.com',\n                    'KdcServer': 'ad.domain.com'\n                }\n            }\n        }\n    }\n}\n\n# Create security configuration\nemr.create_security_configuration(**security_config)\n"})}),"\n",(0,i.jsx)(e.h3,{id:"2-iam-roles-v\xe0-policies",children:"2. IAM Roles v\xe0 Policies"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-json",children:'{\n  "Version": "2012-10-17",\n  "Statement": [\n    {\n      "Effect": "Allow",\n      "Action": [\n        "s3:GetObject",\n        "s3:PutObject",\n        "s3:DeleteObject"\n      ],\n      "Resource": [\n        "arn:aws:s3:::data-lake/*",\n        "arn:aws:s3:::analytics-output/*"\n      ]\n    },\n    {\n      "Effect": "Allow",\n      "Action": [\n        "s3:ListBucket"\n      ],\n      "Resource": [\n        "arn:aws:s3:::data-lake",\n        "arn:aws:s3:::analytics-output"\n      ]\n    },\n    {\n      "Effect": "Allow",\n      "Action": [\n        "kms:Decrypt",\n        "kms:GenerateDataKey"\n      ],\n      "Resource": "arn:aws:kms:region:account:key/key-id"\n    }\n  ]\n}\n'})}),"\n",(0,i.jsx)(e.h2,{id:"monitoring-v\xe0-troubleshooting",children:"Monitoring v\xe0 Troubleshooting"}),"\n",(0,i.jsx)(e.h3,{id:"1-cloudwatch-integration",children:"1. CloudWatch Integration"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# EMR CloudWatch monitoring\nclass EMRMonitor:\n    def __init__(self, cluster_id):\n        self.cluster_id = cluster_id\n        self.cloudwatch = boto3.client('cloudwatch')\n    \n    def create_custom_metrics(self):\n        \"\"\"Create custom CloudWatch metrics\"\"\"\n        # CPU utilization alarm\n        self.cloudwatch.put_metric_alarm(\n            AlarmName=f'EMR-{self.cluster_id}-HighCPU',\n            ComparisonOperator='GreaterThanThreshold',\n            EvaluationPeriods=2,\n            MetricName='CPUUtilization',\n            Namespace='AWS/EMR',\n            Period=300,\n            Statistic='Average',\n            Threshold=80.0,\n            ActionsEnabled=True,\n            AlarmActions=[\n                'arn:aws:sns:region:account:emr-alerts'\n            ],\n            AlarmDescription='EMR cluster high CPU utilization',\n            Dimensions=[\n                {\n                    'Name': 'JobFlowId',\n                    'Value': self.cluster_id\n                }\n            ]\n        )\n        \n        # Memory utilization alarm\n        self.cloudwatch.put_metric_alarm(\n            AlarmName=f'EMR-{self.cluster_id}-HighMemory',\n            ComparisonOperator='GreaterThanThreshold',\n            EvaluationPeriods=2,\n            MetricName='MemoryPercentage',\n            Namespace='AWS/EMR',\n            Period=300,\n            Statistic='Average',\n            Threshold=85.0,\n            ActionsEnabled=True,\n            AlarmActions=[\n                'arn:aws:sns:region:account:emr-alerts'\n            ]\n        )\n    \n    def get_cluster_metrics(self, start_time, end_time):\n        \"\"\"Get cluster performance metrics\"\"\"\n        metrics = {}\n        \n        # Get CPU utilization\n        cpu_response = self.cloudwatch.get_metric_statistics(\n            Namespace='AWS/EMR',\n            MetricName='CPUUtilization',\n            Dimensions=[\n                {\n                    'Name': 'JobFlowId',\n                    'Value': self.cluster_id\n                }\n            ],\n            StartTime=start_time,\n            EndTime=end_time,\n            Period=300,\n            Statistics=['Average', 'Maximum']\n        )\n        \n        metrics['cpu'] = cpu_response['Datapoints']\n        \n        return metrics\n"})}),"\n",(0,i.jsx)(e.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Right-sizing"}),": Ch\u1ecdn instance types ph\xf9 h\u1ee3p v\u1edbi workload"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Cost Optimization"}),": S\u1eed d\u1ee5ng Spot instances v\xe0 auto-termination"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Security"}),": Enable encryption v\xe0 proper IAM roles"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Monitoring"}),": Set up comprehensive monitoring"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Data Partitioning"}),": Optimize data layout trong S3"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Framework Selection"}),": Ch\u1ecdn framework ph\xf9 h\u1ee3p cho use case"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Performance Tuning"}),": Optimize framework configurations"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Backup Strategy"}),": Implement data backup v\xe0 recovery"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"t\xedch-h\u1ee3p-v\u1edbi-c\xe1c-d\u1ecbch-v\u1ee5-aws-kh\xe1c",children:"T\xedch h\u1ee3p v\u1edbi c\xe1c d\u1ecbch v\u1ee5 AWS kh\xe1c"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Amazon S3"}),": Primary data storage"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"AWS Glue"}),": Data catalog v\xe0 ETL"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Amazon Athena"}),": Serverless queries"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Amazon Kinesis"}),": Real-time data streaming"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Amazon SageMaker"}),": Machine learning workflows"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Amazon Redshift"}),": Data warehouse integration"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"AWS Lambda"}),": Event-driven processing"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Amazon CloudWatch"}),": Monitoring v\xe0 alerting"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"AWS IAM"}),": Identity v\xe0 access management"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Amazon VPC"}),": Network security"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"AWS KMS"}),": Encryption key management"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Amazon EKS"}),": Container orchestration"]}),"\n"]})]})}function m(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}},65404:(n,e,s)=>{s.d(e,{R:()=>a,x:()=>o});var t=s(36672);const i={},r=t.createContext(i);function a(n){const e=t.useContext(r);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:a(n.components),t.createElement(r.Provider,{value:e},n.children)}}}]);