"use strict";(self.webpackChunkkeep_being_human_dev=self.webpackChunkkeep_being_human_dev||[]).push([[50855],{33285:(n,e,t)=>{t.d(e,{A:()=>r});const r=t.p+"assets/images/managed-streaming-for-kafka-c4846191af74b8a7cfd7a56229d0871f.png"},42072:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>o,default:()=>u,frontMatter:()=>i,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"aws-nodes/aws-analytics/managed-streaming-for-kafka","title":"Amazon Managed Streaming for Apache Kafka (MSK)","description":"Overview","source":"@site/docs/aws-nodes/aws-analytics/25-managed-streaming-for-kafka.md","sourceDirName":"aws-nodes/aws-analytics","slug":"/aws-nodes/aws-analytics/managed-streaming-for-kafka","permalink":"/keep-being-human-dev/docs/aws-nodes/aws-analytics/managed-streaming-for-kafka","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/aws-nodes/aws-analytics/25-managed-streaming-for-kafka.md","tags":[],"version":"current","sidebarPosition":25,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"AWS Lake Formation","permalink":"/keep-being-human-dev/docs/aws-nodes/aws-analytics/lake-formation"},"next":{"title":"Amazon QuickSight","permalink":"/keep-being-human-dev/docs/aws-nodes/aws-analytics/quicksight"}}');var s=t(23420),a=t(65404);const i={},o="Amazon Managed Streaming for Apache Kafka (MSK)",c={},l=[{value:"Overview",id:"overview",level:2},{value:"Main Functions",id:"main-functions",level:2},{value:"Cluster Management",id:"cluster-management",level:3},{value:"Security and Compliance",id:"security-and-compliance",level:3},{value:"Monitoring and Operations",id:"monitoring-and-operations",level:3},{value:"Performance and Scalability",id:"performance-and-scalability",level:3},{value:"Use Cases",id:"use-cases",level:2},{value:"Real-Time Data Pipeline",id:"real-time-data-pipeline",level:3},{value:"Stream Processing with Kafka Streams",id:"stream-processing-with-kafka-streams",level:3},{value:"MSK Connect for Data Integration",id:"msk-connect-for-data-integration",level:3},{value:"Architecture Diagram",id:"architecture-diagram",level:2},{value:"AWS Service Integrations",id:"aws-service-integrations",level:2},{value:"Analytics and Processing",id:"analytics-and-processing",level:3},{value:"Storage and Data Lakes",id:"storage-and-data-lakes",level:3},{value:"Monitoring and Security",id:"monitoring-and-security",level:3},{value:"Integration Services",id:"integration-services",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"Security",id:"security",level:3},{value:"Reliability",id:"reliability",level:3},{value:"Cost Optimization",id:"cost-optimization",level:3},{value:"Monitoring and Troubleshooting",id:"monitoring-and-troubleshooting",level:2},{value:"Key Metrics",id:"key-metrics",level:3},{value:"Common Issues",id:"common-issues",level:3},{value:"Troubleshooting Steps",id:"troubleshooting-steps",level:3}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"amazon-managed-streaming-for-apache-kafka-msk",children:"Amazon Managed Streaming for Apache Kafka (MSK)"})}),"\n",(0,s.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(e.p,{children:"Amazon Managed Streaming for Apache Kafka (Amazon MSK) is a fully managed service that makes it easy for you to build and run applications that use Apache Kafka to process streaming data. Apache Kafka is an open-source platform for building real-time streaming data pipelines and applications. With Amazon MSK, you can use native Apache Kafka APIs to populate data lakes, stream changes to and from databases, and power machine learning and analytics applications."}),"\n",(0,s.jsx)(e.h2,{id:"main-functions",children:"Main Functions"}),"\n",(0,s.jsx)(e.h3,{id:"cluster-management",children:"Cluster Management"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Fully Managed Kafka"}),": Automated provisioning, configuration, and maintenance"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Multi-AZ Deployment"}),": High availability across multiple Availability Zones"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Auto Scaling"}),": Automatic scaling of broker capacity and storage"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Version Management"}),": Automated Kafka version upgrades"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"security-and-compliance",children:"Security and Compliance"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Encryption"}),": Data encryption in transit and at rest"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Authentication"}),": IAM, SASL/SCRAM, and mTLS authentication"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Network Security"}),": VPC isolation and security group controls"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Audit Logging"}),": CloudTrail integration for API calls"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"monitoring-and-operations",children:"Monitoring and Operations"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"CloudWatch Integration"}),": Comprehensive metrics and monitoring"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Prometheus Metrics"}),": Open-source monitoring support"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Logging"}),": Broker logs to CloudWatch, S3, or Kinesis Data Firehose"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Configuration Management"}),": Centralized configuration updates"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"performance-and-scalability",children:"Performance and Scalability"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"High Throughput"}),": Optimized for high-volume data streaming"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Low Latency"}),": Sub-millisecond latency for real-time applications"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Elastic Storage"}),": Automatic storage scaling"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Custom Configurations"}),": Fine-tuned Kafka configurations"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"use-cases",children:"Use Cases"}),"\n",(0,s.jsx)(e.h3,{id:"real-time-data-pipeline",children:"Real-Time Data Pipeline"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"import boto3\nfrom kafka import KafkaProducer, KafkaConsumer\nimport json\nimport time\nfrom datetime import datetime\n\nclass MSKDataPipeline:\n    def __init__(self, bootstrap_servers, security_protocol='SSL'):\n        self.bootstrap_servers = bootstrap_servers\n        self.security_protocol = security_protocol\n        self.msk_client = boto3.client('kafka')\n    \n    def create_cluster(self, cluster_name, kafka_version='2.8.1'):\n        \"\"\"Create MSK cluster\"\"\"\n        try:\n            response = self.msk_client.create_cluster(\n                ClusterName=cluster_name,\n                KafkaVersion=kafka_version,\n                NumberOfBrokerNodes=3,\n                BrokerNodeGroupInfo={\n                    'InstanceType': 'kafka.m5.large',\n                    'ClientSubnets': [\n                        'subnet-12345678',\n                        'subnet-87654321',\n                        'subnet-11223344'\n                    ],\n                    'SecurityGroups': ['sg-12345678'],\n                    'StorageInfo': {\n                        'EBSStorageInfo': {\n                            'VolumeSize': 100\n                        }\n                    }\n                },\n                EncryptionInfo={\n                    'EncryptionAtRest': {\n                        'DataVolumeKMSKeyId': 'arn:aws:kms:us-east-1:123456789012:key/12345678-1234-1234-1234-123456789012'\n                    },\n                    'EncryptionInTransit': {\n                        'ClientBroker': 'TLS',\n                        'InCluster': True\n                    }\n                },\n                ClientAuthentication={\n                    'Sasl': {\n                        'Scram': {\n                            'Enabled': True\n                        }\n                    }\n                },\n                ConfigurationInfo={\n                    'Arn': 'arn:aws:kafka:us-east-1:123456789012:configuration/my-config/12345678-1234-1234-1234-123456789012-1',\n                    'Revision': 1\n                }\n            )\n            print(f\"MSK cluster creation initiated: {cluster_name}\")\n            return response\n        except Exception as e:\n            print(f\"Error creating cluster: {e}\")\n    \n    def create_producer(self, topic_name):\n        \"\"\"Create Kafka producer\"\"\"\n        try:\n            producer = KafkaProducer(\n                bootstrap_servers=self.bootstrap_servers,\n                security_protocol=self.security_protocol,\n                value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n                key_serializer=lambda k: k.encode('utf-8') if k else None,\n                acks='all',\n                retries=3,\n                batch_size=16384,\n                linger_ms=10,\n                buffer_memory=33554432\n            )\n            return producer\n        except Exception as e:\n            print(f\"Error creating producer: {e}\")\n    \n    def create_consumer(self, topic_name, group_id):\n        \"\"\"Create Kafka consumer\"\"\"\n        try:\n            consumer = KafkaConsumer(\n                topic_name,\n                bootstrap_servers=self.bootstrap_servers,\n                security_protocol=self.security_protocol,\n                group_id=group_id,\n                value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n                key_deserializer=lambda k: k.decode('utf-8') if k else None,\n                auto_offset_reset='earliest',\n                enable_auto_commit=True,\n                auto_commit_interval_ms=1000\n            )\n            return consumer\n        except Exception as e:\n            print(f\"Error creating consumer: {e}\")\n    \n    def produce_events(self, producer, topic_name, events):\n        \"\"\"Produce events to Kafka topic\"\"\"\n        try:\n            for event in events:\n                # Add timestamp to event\n                event['timestamp'] = datetime.now().isoformat()\n                \n                # Send event\n                future = producer.send(\n                    topic_name,\n                    key=event.get('id'),\n                    value=event\n                )\n                \n                # Wait for acknowledgment\n                record_metadata = future.get(timeout=10)\n                print(f\"Event sent to partition {record_metadata.partition} at offset {record_metadata.offset}\")\n            \n            producer.flush()\n        except Exception as e:\n            print(f\"Error producing events: {e}\")\n    \n    def consume_events(self, consumer, process_function):\n        \"\"\"Consume and process events\"\"\"\n        try:\n            for message in consumer:\n                event = {\n                    'topic': message.topic,\n                    'partition': message.partition,\n                    'offset': message.offset,\n                    'key': message.key,\n                    'value': message.value,\n                    'timestamp': message.timestamp\n                }\n                \n                # Process the event\n                process_function(event)\n                \n        except Exception as e:\n            print(f\"Error consuming events: {e}\")\n\n# Usage example\nmsk_pipeline = MSKDataPipeline(\n    bootstrap_servers=['b-1.mycluster.abc123.c2.kafka.us-east-1.amazonaws.com:9092']\n)\n\n# Create producer and consumer\nproducer = msk_pipeline.create_producer('user-events')\nconsumer = msk_pipeline.create_consumer('user-events', 'analytics-group')\n\n# Sample events\nevents = [\n    {'id': 'user-001', 'action': 'login', 'user_id': '12345'},\n    {'id': 'user-002', 'action': 'purchase', 'user_id': '67890', 'amount': 99.99},\n    {'id': 'user-003', 'action': 'logout', 'user_id': '12345'}\n]\n\n# Produce events\nmsk_pipeline.produce_events(producer, 'user-events', events)\n\n# Define event processing function\ndef process_event(event):\n    print(f\"Processing event: {event['value']['action']} for user {event['value']['user_id']}\")\n    # Add your processing logic here\n\n# Consume events (this would run in a separate process/thread)\n# msk_pipeline.consume_events(consumer, process_event)\n"})}),"\n",(0,s.jsx)(e.h3,{id:"stream-processing-with-kafka-streams",children:"Stream Processing with Kafka Streams"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"import boto3\nfrom kafka import KafkaProducer, KafkaConsumer\nimport json\nfrom collections import defaultdict\nimport time\n\nclass MSKStreamProcessor:\n    def __init__(self, bootstrap_servers):\n        self.bootstrap_servers = bootstrap_servers\n        self.state_store = defaultdict(dict)\n    \n    def create_windowed_aggregation(self, input_topic, output_topic, window_size_seconds=60):\n        \"\"\"Create windowed aggregation processor\"\"\"\n        consumer = KafkaConsumer(\n            input_topic,\n            bootstrap_servers=self.bootstrap_servers,\n            group_id='stream-processor',\n            value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n            auto_offset_reset='earliest'\n        )\n        \n        producer = KafkaProducer(\n            bootstrap_servers=self.bootstrap_servers,\n            value_serializer=lambda v: json.dumps(v).encode('utf-8')\n        )\n        \n        window_data = defaultdict(lambda: defaultdict(int))\n        last_window_time = int(time.time() // window_size_seconds)\n        \n        try:\n            for message in consumer:\n                event = message.value\n                current_window = int(time.time() // window_size_seconds)\n                \n                # Process current event\n                key = event.get('category', 'unknown')\n                window_data[current_window][key] += event.get('value', 1)\n                \n                # Check if window has closed\n                if current_window > last_window_time:\n                    # Emit results for completed windows\n                    for window_time in list(window_data.keys()):\n                        if window_time < current_window:\n                            result = {\n                                'window_start': window_time * window_size_seconds,\n                                'window_end': (window_time + 1) * window_size_seconds,\n                                'aggregations': dict(window_data[window_time])\n                            }\n                            \n                            producer.send(output_topic, value=result)\n                            del window_data[window_time]\n                    \n                    last_window_time = current_window\n                    \n        except Exception as e:\n            print(f\"Error in stream processing: {e}\")\n        finally:\n            consumer.close()\n            producer.close()\n    \n    def create_join_processor(self, left_topic, right_topic, output_topic, join_key):\n        \"\"\"Create stream-stream join processor\"\"\"\n        left_consumer = KafkaConsumer(\n            left_topic,\n            bootstrap_servers=self.bootstrap_servers,\n            group_id='join-left',\n            value_deserializer=lambda m: json.loads(m.decode('utf-8'))\n        )\n        \n        right_consumer = KafkaConsumer(\n            right_topic,\n            bootstrap_servers=self.bootstrap_servers,\n            group_id='join-right',\n            value_deserializer=lambda m: json.loads(m.decode('utf-8'))\n        )\n        \n        producer = KafkaProducer(\n            bootstrap_servers=self.bootstrap_servers,\n            value_serializer=lambda v: json.dumps(v).encode('utf-8')\n        )\n        \n        # Simple in-memory join state (in production, use external state store)\n        left_state = {}\n        right_state = {}\n        \n        def process_left_stream():\n            for message in left_consumer:\n                event = message.value\n                key = event.get(join_key)\n                if key:\n                    left_state[key] = event\n                    \n                    # Check for join match\n                    if key in right_state:\n                        joined_event = {\n                            'left': event,\n                            'right': right_state[key],\n                            'join_key': key,\n                            'timestamp': time.time()\n                        }\n                        producer.send(output_topic, value=joined_event)\n        \n        def process_right_stream():\n            for message in right_consumer:\n                event = message.value\n                key = event.get(join_key)\n                if key:\n                    right_state[key] = event\n                    \n                    # Check for join match\n                    if key in left_state:\n                        joined_event = {\n                            'left': left_state[key],\n                            'right': event,\n                            'join_key': key,\n                            'timestamp': time.time()\n                        }\n                        producer.send(output_topic, value=joined_event)\n        \n        # In production, run these in separate threads\n        import threading\n        left_thread = threading.Thread(target=process_left_stream)\n        right_thread = threading.Thread(target=process_right_stream)\n        \n        left_thread.start()\n        right_thread.start()\n        \n        return left_thread, right_thread\n\n# Usage example\nstream_processor = MSKStreamProcessor(\n    bootstrap_servers=['b-1.mycluster.abc123.c2.kafka.us-east-1.amazonaws.com:9092']\n)\n\n# Create windowed aggregation\nstream_processor.create_windowed_aggregation(\n    input_topic='raw-events',\n    output_topic='aggregated-events',\n    window_size_seconds=300  # 5-minute windows\n)\n"})}),"\n",(0,s.jsx)(e.h3,{id:"msk-connect-for-data-integration",children:"MSK Connect for Data Integration"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"import boto3\nimport json\n\nclass MSKConnectManager:\n    def __init__(self):\n        self.msk_connect = boto3.client('kafkaconnect')\n        self.msk = boto3.client('kafka')\n    \n    def create_s3_sink_connector(self, connector_name, cluster_arn, topics, s3_bucket):\n        \"\"\"Create S3 sink connector\"\"\"\n        try:\n            connector_config = {\n                'connector.class': 'io.confluent.connect.s3.S3SinkConnector',\n                'tasks.max': '2',\n                'topics': ','.join(topics),\n                's3.bucket.name': s3_bucket,\n                's3.region': 'us-east-1',\n                'flush.size': '1000',\n                'rotate.interval.ms': '60000',\n                'format.class': 'io.confluent.connect.s3.format.json.JsonFormat',\n                'partitioner.class': 'io.confluent.connect.storage.partitioner.TimeBasedPartitioner',\n                'partition.duration.ms': '3600000',  # 1 hour partitions\n                'path.format': 'year=YYYY/month=MM/day=dd/hour=HH',\n                'locale': 'en-US',\n                'timezone': 'UTC'\n            }\n            \n            response = self.msk_connect.create_connector(\n                connectorName=connector_name,\n                kafkaCluster={\n                    'apacheKafkaCluster': {\n                        'bootstrapServers': cluster_arn\n                    }\n                },\n                kafkaClusterClientAuthentication={\n                    'authenticationType': 'IAM'\n                },\n                kafkaClusterEncryptionInTransit={\n                    'encryptionType': 'TLS'\n                },\n                kafkaConnectVersion='2.7.1',\n                plugins=[\n                    {\n                        'customPlugin': {\n                            'customPluginArn': 'arn:aws:kafkaconnect:us-east-1:123456789012:custom-plugin/s3-sink-plugin/12345678-1234-1234-1234-123456789012-1',\n                            'revision': 1\n                        }\n                    }\n                ],\n                connectorConfiguration=connector_config,\n                capacity={\n                    'provisionedCapacity': {\n                        'mcuCount': 2,\n                        'workerCount': 2\n                    }\n                },\n                serviceExecutionRoleArn='arn:aws:iam::123456789012:role/MSKConnectRole'\n            )\n            \n            print(f\"S3 sink connector created: {connector_name}\")\n            return response\n        except Exception as e:\n            print(f\"Error creating S3 sink connector: {e}\")\n    \n    def create_database_source_connector(self, connector_name, cluster_arn, \n                                       database_config, topics_prefix):\n        \"\"\"Create database source connector\"\"\"\n        try:\n            connector_config = {\n                'connector.class': 'io.debezium.connector.mysql.MySqlConnector',\n                'tasks.max': '1',\n                'database.hostname': database_config['host'],\n                'database.port': str(database_config['port']),\n                'database.user': database_config['username'],\n                'database.password': database_config['password'],\n                'database.server.id': '184054',\n                'database.server.name': database_config['server_name'],\n                'database.include.list': database_config['databases'],\n                'database.history.kafka.bootstrap.servers': cluster_arn,\n                'database.history.kafka.topic': f\"{topics_prefix}.history\",\n                'include.schema.changes': 'true',\n                'transforms': 'route',\n                'transforms.route.type': 'org.apache.kafka.connect.transforms.RegexRouter',\n                'transforms.route.regex': '([^.]+)\\\\.([^.]+)\\\\.([^.]+)',\n                'transforms.route.replacement': f'{topics_prefix}.$2.$3'\n            }\n            \n            response = self.msk_connect.create_connector(\n                connectorName=connector_name,\n                kafkaCluster={\n                    'apacheKafkaCluster': {\n                        'bootstrapServers': cluster_arn\n                    }\n                },\n                kafkaClusterClientAuthentication={\n                    'authenticationType': 'IAM'\n                },\n                kafkaClusterEncryptionInTransit={\n                    'encryptionType': 'TLS'\n                },\n                kafkaConnectVersion='2.7.1',\n                plugins=[\n                    {\n                        'customPlugin': {\n                            'customPluginArn': 'arn:aws:kafkaconnect:us-east-1:123456789012:custom-plugin/debezium-mysql/12345678-1234-1234-1234-123456789012-1',\n                            'revision': 1\n                        }\n                    }\n                ],\n                connectorConfiguration=connector_config,\n                capacity={\n                    'provisionedCapacity': {\n                        'mcuCount': 1,\n                        'workerCount': 1\n                    }\n                },\n                serviceExecutionRoleArn='arn:aws:iam::123456789012:role/MSKConnectRole'\n            )\n            \n            print(f\"Database source connector created: {connector_name}\")\n            return response\n        except Exception as e:\n            print(f\"Error creating database source connector: {e}\")\n    \n    def monitor_connector(self, connector_arn):\n        \"\"\"Monitor connector status and metrics\"\"\"\n        try:\n            response = self.msk_connect.describe_connector(\n                connectorArn=connector_arn\n            )\n            \n            connector_info = {\n                'name': response['connectorName'],\n                'state': response['connectorState'],\n                'creation_time': response['creationTime'],\n                'current_version': response['currentVersion'],\n                'worker_configuration': response.get('workerConfiguration', {}),\n                'capacity': response.get('capacity', {})\n            }\n            \n            return connector_info\n        except Exception as e:\n            print(f\"Error monitoring connector: {e}\")\n\n# Usage example\nconnect_manager = MSKConnectManager()\n\n# Create S3 sink connector\nconnect_manager.create_s3_sink_connector(\n    connector_name='user-events-s3-sink',\n    cluster_arn='arn:aws:kafka:us-east-1:123456789012:cluster/my-cluster/12345678-1234-1234-1234-123456789012-1',\n    topics=['user-events', 'purchase-events'],\n    s3_bucket='my-data-lake-bucket'\n)\n\n# Create database source connector\ndatabase_config = {\n    'host': 'mydb.cluster-xyz.us-east-1.rds.amazonaws.com',\n    'port': 3306,\n    'username': 'kafka_user',\n    'password': 'secure_password',\n    'server_name': 'production-db',\n    'databases': 'ecommerce,analytics'\n}\n\nconnect_manager.create_database_source_connector(\n    connector_name='mysql-source-connector',\n    cluster_arn='arn:aws:kafka:us-east-1:123456789012:cluster/my-cluster/12345678-1234-1234-1234-123456789012-1',\n    database_config=database_config,\n    topics_prefix='mysql'\n)\n"})}),"\n",(0,s.jsx)(e.h2,{id:"architecture-diagram",children:"Architecture Diagram"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.img,{alt:"Amazon MSK Architecture",src:t(33285).A+"",width:"2361",height:"1525"})}),"\n",(0,s.jsx)(e.h2,{id:"aws-service-integrations",children:"AWS Service Integrations"}),"\n",(0,s.jsx)(e.h3,{id:"analytics-and-processing",children:"Analytics and Processing"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Amazon Kinesis Data Analytics"}),": SQL-based stream processing"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"AWS Lambda"}),": Serverless event processing"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Amazon EMR"}),": Big data processing with Spark and Hadoop"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"AWS Glue"}),": ETL and data cataloging"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"storage-and-data-lakes",children:"Storage and Data Lakes"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Amazon S3"}),": Data lake storage via MSK Connect"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Amazon Redshift"}),": Data warehouse integration"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Amazon DynamoDB"}),": NoSQL database integration"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"AWS Lake Formation"}),": Data lake governance"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"monitoring-and-security",children:"Monitoring and Security"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Amazon CloudWatch"}),": Metrics and monitoring"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"AWS CloudTrail"}),": API audit logging"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"AWS IAM"}),": Authentication and authorization"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Amazon VPC"}),": Network isolation and security"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"integration-services",children:"Integration Services"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"MSK Connect"}),": Managed Kafka Connect service"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Amazon API Gateway"}),": REST API integration"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Amazon EventBridge"}),": Event routing"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"AWS Step Functions"}),": Workflow orchestration"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,s.jsx)(e.h3,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Choose appropriate instance types for brokers"}),"\n",(0,s.jsx)(e.li,{children:"Configure proper partition counts for topics"}),"\n",(0,s.jsx)(e.li,{children:"Optimize producer and consumer configurations"}),"\n",(0,s.jsx)(e.li,{children:"Use compression for better throughput"}),"\n",(0,s.jsx)(e.li,{children:"Monitor and tune JVM settings"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"security",children:"Security"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Enable encryption in transit and at rest"}),"\n",(0,s.jsx)(e.li,{children:"Use IAM for authentication and authorization"}),"\n",(0,s.jsx)(e.li,{children:"Implement network security with VPCs"}),"\n",(0,s.jsx)(e.li,{children:"Regular security audits and compliance checks"}),"\n",(0,s.jsx)(e.li,{children:"Secure credential management"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"reliability",children:"Reliability"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Deploy across multiple Availability Zones"}),"\n",(0,s.jsx)(e.li,{children:"Configure appropriate replication factors"}),"\n",(0,s.jsx)(e.li,{children:"Implement proper error handling"}),"\n",(0,s.jsx)(e.li,{children:"Set up monitoring and alerting"}),"\n",(0,s.jsx)(e.li,{children:"Regular backup and disaster recovery testing"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"cost-optimization",children:"Cost Optimization"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Right-size broker instances"}),"\n",(0,s.jsx)(e.li,{children:"Use appropriate storage types"}),"\n",(0,s.jsx)(e.li,{children:"Monitor and optimize data retention"}),"\n",(0,s.jsx)(e.li,{children:"Implement auto-scaling where possible"}),"\n",(0,s.jsx)(e.li,{children:"Regular cost analysis and optimization"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"monitoring-and-troubleshooting",children:"Monitoring and Troubleshooting"}),"\n",(0,s.jsx)(e.h3,{id:"key-metrics",children:"Key Metrics"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Broker CPU and memory utilization"}),"\n",(0,s.jsx)(e.li,{children:"Network throughput and latency"}),"\n",(0,s.jsx)(e.li,{children:"Topic partition metrics"}),"\n",(0,s.jsx)(e.li,{children:"Consumer lag monitoring"}),"\n",(0,s.jsx)(e.li,{children:"Error rates and failed requests"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Consumer lag problems"}),"\n",(0,s.jsx)(e.li,{children:"Broker connectivity issues"}),"\n",(0,s.jsx)(e.li,{children:"Authentication failures"}),"\n",(0,s.jsx)(e.li,{children:"Performance bottlenecks"}),"\n",(0,s.jsx)(e.li,{children:"Data serialization errors"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"troubleshooting-steps",children:"Troubleshooting Steps"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Check cluster health and broker status"}),"\n",(0,s.jsx)(e.li,{children:"Verify network connectivity and security groups"}),"\n",(0,s.jsx)(e.li,{children:"Review CloudWatch metrics and logs"}),"\n",(0,s.jsx)(e.li,{children:"Validate producer/consumer configurations"}),"\n",(0,s.jsx)(e.li,{children:"Monitor partition distribution and rebalancing"}),"\n"]})]})}function u(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}},65404:(n,e,t)=>{t.d(e,{R:()=>i,x:()=>o});var r=t(36672);const s={},a=r.createContext(s);function i(n){const e=r.useContext(a);return r.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:i(n.components),r.createElement(a.Provider,{value:e},n.children)}}}]);