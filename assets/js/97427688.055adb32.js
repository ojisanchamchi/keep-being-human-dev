"use strict";(self.webpackChunkkeep_being_human_dev=self.webpackChunkkeep_being_human_dev||[]).push([[14222],{8775:(e,n,r)=>{r.d(n,{A:()=>s});const s=r.p+"assets/images/kinesis-data-streams-8f37e4a6b3c24ef9cb2f7b406967737f.png"},65404:(e,n,r)=>{r.d(n,{R:()=>i,x:()=>o});var s=r(36672);const a={},t=s.createContext(a);function i(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),s.createElement(t.Provider,{value:n},e.children)}},89041:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>m,frontMatter:()=>i,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"aws-nodes/aws-analytics/kinesis-data-streams","title":"Amazon Kinesis Data Streams","description":"T\u1ed5ng quan","source":"@site/docs/aws-nodes/aws-analytics/21-kinesis-data-streams.md","sourceDirName":"aws-nodes/aws-analytics","slug":"/aws-nodes/aws-analytics/kinesis-data-streams","permalink":"/keep-being-human-dev/docs/aws-nodes/aws-analytics/kinesis-data-streams","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/aws-nodes/aws-analytics/21-kinesis-data-streams.md","tags":[],"version":"current","sidebarPosition":21,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Amazon Kinesis Data Firehose","permalink":"/keep-being-human-dev/docs/aws-nodes/aws-analytics/kinesis-data-firehose"},"next":{"title":"Amazon Kinesis Video Streams","permalink":"/keep-being-human-dev/docs/aws-nodes/aws-analytics/kinesis-video-streams"}}');var a=r(23420),t=r(65404);const i={},o="Amazon Kinesis Data Streams",c={},d=[{value:"T\u1ed5ng quan",id:"t\u1ed5ng-quan",level:2},{value:"Ch\u1ee9c n\u0103ng ch\xednh",id:"ch\u1ee9c-n\u0103ng-ch\xednh",level:2},{value:"1. Real-time Data Ingestion",id:"1-real-time-data-ingestion",level:3},{value:"2. Scalable Architecture",id:"2-scalable-architecture",level:3},{value:"3. Data Processing Models",id:"3-data-processing-models",level:3},{value:"4. Monitoring v\xe0 Management",id:"4-monitoring-v\xe0-management",level:3},{value:"Use Cases ph\u1ed5 bi\u1ebfn",id:"use-cases-ph\u1ed5-bi\u1ebfn",level:2},{value:"Diagram Architecture",id:"diagram-architecture",level:2},{value:"Code \u0111\u1ec3 t\u1ea1o diagram:",id:"code-\u0111\u1ec3-t\u1ea1o-diagram",level:3},{value:"Stream Configuration",id:"stream-configuration",level:2},{value:"1. Stream Creation v\xe0 Management",id:"1-stream-creation-v\xe0-management",level:3},{value:"2. Data Producer Implementation",id:"2-data-producer-implementation",level:3},{value:"3. Advanced Producer v\u1edbi KPL",id:"3-advanced-producer-v\u1edbi-kpl",level:3},{value:"Stream Consumer Implementation",id:"stream-consumer-implementation",level:2},{value:"1. Lambda Consumer",id:"1-lambda-consumer",level:3},{value:"2. KCL Consumer Application",id:"2-kcl-consumer-application",level:3},{value:"Scaling v\xe0 Performance",id:"scaling-v\xe0-performance",level:2},{value:"1. Shard Management",id:"1-shard-management",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"T\xedch h\u1ee3p v\u1edbi c\xe1c d\u1ecbch v\u1ee5 AWS kh\xe1c",id:"t\xedch-h\u1ee3p-v\u1edbi-c\xe1c-d\u1ecbch-v\u1ee5-aws-kh\xe1c",level:2}];function l(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"amazon-kinesis-data-streams",children:"Amazon Kinesis Data Streams"})}),"\n",(0,a.jsx)(n.h2,{id:"t\u1ed5ng-quan",children:"T\u1ed5ng quan"}),"\n",(0,a.jsx)(n.p,{children:"Amazon Kinesis Data Streams l\xe0 node \u0111\u1ea1i di\u1ec7n cho d\u1ecbch v\u1ee5 streaming data platform real-time c\u1ee7a AWS. Data Streams cho ph\xe9p b\u1ea1n build applications c\xf3 th\u1ec3 process v\xe0 analyze streaming data trong real-time. D\u1ecbch v\u1ee5 n\xe0y c\xf3 th\u1ec3 handle terabytes c\u1ee7a data per hour t\u1eeb h\xe0ng tr\u0103m ngh\xecn sources nh\u01b0 website clickstreams, database event streams, financial transactions, social media feeds, IT logs, v\xe0 location-tracking events."}),"\n",(0,a.jsx)(n.h2,{id:"ch\u1ee9c-n\u0103ng-ch\xednh",children:"Ch\u1ee9c n\u0103ng ch\xednh"}),"\n",(0,a.jsx)(n.h3,{id:"1-real-time-data-ingestion",children:"1. Real-time Data Ingestion"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"High Throughput"}),": X\u1eed l\xfd millions of records per second"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Low Latency"}),": Sub-second processing latency"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Durable Storage"}),": Data retention t\u1eeb 24 hours \u0111\u1ebfn 365 days"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Ordered Processing"}),": Maintain order within shards"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"2-scalable-architecture",children:"2. Scalable Architecture"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Shard-based Scaling"}),": Scale b\u1eb1ng c\xe1ch th\xeam/b\u1edbt shards"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Auto Scaling"}),": T\u1ef1 \u0111\u1ed9ng adjust capacity"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Multiple Consumers"}),": Multiple applications c\xf3 th\u1ec3 consume c\xf9ng stream"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Fan-out"}),": Enhanced fan-out cho multiple consumers"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"3-data-processing-models",children:"3. Data Processing Models"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Shared Throughput"}),": Traditional consumer model"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Enhanced Fan-out"}),": Dedicated throughput per consumer"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Kinesis Client Library"}),": Simplified consumer development"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Lambda Integration"}),": Serverless processing"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"4-monitoring-v\xe0-management",children:"4. Monitoring v\xe0 Management"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"CloudWatch Integration"}),": Comprehensive metrics"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Server-side Encryption"}),": Data encryption at rest"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"VPC Endpoints"}),": Private connectivity"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Cross-region Replication"}),": Disaster recovery"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"use-cases-ph\u1ed5-bi\u1ebfn",children:"Use Cases ph\u1ed5 bi\u1ebfn"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Real-time Analytics"}),": Live dashboards v\xe0 metrics"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Log Processing"}),": Centralized log aggregation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"IoT Data Processing"}),": Sensor data streams"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Clickstream Analysis"}),": Website user behavior"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Financial Data"}),": Trading data v\xe0 fraud detection"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"diagram-architecture",children:"Diagram Architecture"}),"\n",(0,a.jsx)(n.p,{children:"Ki\u1ebfn tr\xfac Amazon Kinesis Data Streams v\u1edbi real-time processing:"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Amazon Kinesis Data Streams Architecture",src:r(8775).A+"",width:"1712",height:"2046"})}),"\n",(0,a.jsx)(n.h3,{id:"code-\u0111\u1ec3-t\u1ea1o-diagram",children:"Code \u0111\u1ec3 t\u1ea1o diagram:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.analytics import KinesisDataStreams, KinesisDataAnalytics, KinesisDataFirehose\nfrom diagrams.aws.storage import S3\nfrom diagrams.aws.compute import Lambda, EC2\nfrom diagrams.aws.database import Dynamodb, RDS\nfrom diagrams.aws.integration import SQS, SNS\nfrom diagrams.aws.management import Cloudwatch\nfrom diagrams.aws.security import IAM\nfrom diagrams.onprem.client import Users\n\nwith Diagram("Amazon Kinesis Data Streams Architecture", show=False, direction="TB"):\n    \n    users = Users("Data Producers")\n    \n    with Cluster("Data Producers"):\n        web_apps = Lambda("Web Applications")\n        mobile_apps = Lambda("Mobile Apps")\n        iot_devices = Lambda("IoT Devices")\n        log_agents = EC2("Log Agents")\n        databases = RDS("Database CDC")\n    \n    with Cluster("Kinesis Data Streams"):\n        stream = KinesisDataStreams("Data Stream")\n        \n        with Cluster("Shards"):\n            shard1 = KinesisDataStreams("Shard 1")\n            shard2 = KinesisDataStreams("Shard 2")\n            shard3 = KinesisDataStreams("Shard 3")\n            shard4 = KinesisDataStreams("Shard 4")\n    \n    with Cluster("Stream Consumers"):\n        lambda_consumer = Lambda("Lambda Consumer")\n        kda_app = KinesisDataAnalytics("KDA Application")\n        firehose_consumer = KinesisDataFirehose("Firehose Consumer")\n        custom_consumer = EC2("Custom Consumer")\n    \n    with Cluster("Processing & Storage"):\n        real_time_db = Dynamodb("Real-time Results")\n        s3_storage = S3("Data Lake")\n        analytics_db = RDS("Analytics DB")\n        search_index = Lambda("Search Index")\n    \n    with Cluster("Downstream Applications"):\n        dashboard = Lambda("Real-time Dashboard")\n        alerts = SNS("Alert System")\n        ml_pipeline = Lambda("ML Pipeline")\n        reporting = Lambda("Reporting System")\n    \n    with Cluster("Monitoring & Management"):\n        cloudwatch = Cloudwatch("CloudWatch")\n        security = IAM("IAM Roles")\n        scaling = Lambda("Auto Scaling")\n    \n    # Data ingestion\n    web_apps >> Edge(label="Events") >> stream\n    mobile_apps >> Edge(label="Analytics") >> stream\n    iot_devices >> Edge(label="Sensor Data") >> stream\n    log_agents >> Edge(label="Logs") >> stream\n    databases >> Edge(label="Change Events") >> stream\n    \n    # Shard distribution\n    stream >> shard1\n    stream >> shard2\n    stream >> shard3\n    stream >> shard4\n    \n    # Stream consumption\n    shard1 >> lambda_consumer\n    shard2 >> kda_app\n    shard3 >> firehose_consumer\n    shard4 >> custom_consumer\n    \n    # Data processing and storage\n    lambda_consumer >> real_time_db\n    kda_app >> analytics_db\n    firehose_consumer >> s3_storage\n    custom_consumer >> search_index\n    \n    # Downstream applications\n    real_time_db >> dashboard\n    analytics_db >> alerts\n    s3_storage >> ml_pipeline\n    search_index >> reporting\n    \n    # User interaction\n    users >> Edge(label="View Dashboards") >> dashboard\n    users >> Edge(label="Receive Alerts") >> alerts\n    \n    # Monitoring and management\n    stream >> Edge(label="Metrics") >> cloudwatch\n    cloudwatch >> scaling >> stream\n    security >> Edge(label="Access Control") >> stream\n'})}),"\n",(0,a.jsx)(n.h2,{id:"stream-configuration",children:"Stream Configuration"}),"\n",(0,a.jsx)(n.h3,{id:"1-stream-creation-v\xe0-management",children:"1. Stream Creation v\xe0 Management"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import boto3\nimport json\nfrom datetime import datetime\n\nkinesis = boto3.client(\'kinesis\')\n\n# Create stream with multiple shards\ndef create_kinesis_stream(stream_name, shard_count):\n    """Create Kinesis Data Stream"""\n    \n    try:\n        response = kinesis.create_stream(\n            StreamName=stream_name,\n            ShardCount=shard_count,\n            StreamModeDetails={\n                \'StreamMode\': \'PROVISIONED\'\n            }\n        )\n        \n        print(f"Stream {stream_name} created with {shard_count} shards")\n        return response\n        \n    except kinesis.exceptions.ResourceInUseException:\n        print(f"Stream {stream_name} already exists")\n        return None\n\n# On-demand mode for variable workloads\ndef create_on_demand_stream(stream_name):\n    """Create on-demand Kinesis stream"""\n    \n    response = kinesis.create_stream(\n        StreamName=stream_name,\n        StreamModeDetails={\n            \'StreamMode\': \'ON_DEMAND\'\n        }\n    )\n    \n    return response\n\n# Configure stream retention\ndef configure_retention(stream_name, retention_hours):\n    """Configure data retention period"""\n    \n    kinesis.increase_stream_retention_period(\n        StreamName=stream_name,\n        RetentionPeriodHours=retention_hours  # 24-8760 hours\n    )\n    \n    print(f"Retention set to {retention_hours} hours for {stream_name}")\n\n# Enable server-side encryption\ndef enable_encryption(stream_name, key_id):\n    """Enable server-side encryption"""\n    \n    kinesis.enable_stream_encryption(\n        StreamName=stream_name,\n        EncryptionType=\'KMS\',\n        KeyId=key_id\n    )\n    \n    print(f"Encryption enabled for {stream_name}")\n'})}),"\n",(0,a.jsx)(n.h3,{id:"2-data-producer-implementation",children:"2. Data Producer Implementation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"class KinesisProducer:\n    def __init__(self, stream_name, region='us-west-2'):\n        self.kinesis = boto3.client('kinesis', region_name=region)\n        self.stream_name = stream_name\n        \n    def put_record(self, data, partition_key):\n        \"\"\"Put single record to stream\"\"\"\n        \n        try:\n            response = self.kinesis.put_record(\n                StreamName=self.stream_name,\n                Data=json.dumps(data),\n                PartitionKey=partition_key\n            )\n            \n            return {\n                'success': True,\n                'shard_id': response['ShardId'],\n                'sequence_number': response['SequenceNumber']\n            }\n            \n        except Exception as e:\n            return {\n                'success': False,\n                'error': str(e)\n            }\n    \n    def put_records_batch(self, records):\n        \"\"\"Put multiple records in batch\"\"\"\n        \n        # Prepare records for batch put\n        kinesis_records = []\n        for record in records:\n            kinesis_records.append({\n                'Data': json.dumps(record['data']),\n                'PartitionKey': record['partition_key']\n            })\n        \n        # Batch put with retry logic\n        failed_records = []\n        batch_size = 500  # Max 500 records per batch\n        \n        for i in range(0, len(kinesis_records), batch_size):\n            batch = kinesis_records[i:i + batch_size]\n            \n            try:\n                response = self.kinesis.put_records(\n                    Records=batch,\n                    StreamName=self.stream_name\n                )\n                \n                # Handle partial failures\n                if response['FailedRecordCount'] > 0:\n                    for j, record_result in enumerate(response['Records']):\n                        if 'ErrorCode' in record_result:\n                            failed_records.append({\n                                'record': batch[j],\n                                'error': record_result['ErrorMessage']\n                            })\n                \n            except Exception as e:\n                # Add entire batch to failed records\n                failed_records.extend([{\n                    'record': record,\n                    'error': str(e)\n                } for record in batch])\n        \n        return {\n            'total_records': len(kinesis_records),\n            'failed_records': failed_records,\n            'success_rate': (len(kinesis_records) - len(failed_records)) / len(kinesis_records)\n        }\n    \n    def put_record_with_retry(self, data, partition_key, max_retries=3):\n        \"\"\"Put record with exponential backoff retry\"\"\"\n        \n        import time\n        import random\n        \n        for attempt in range(max_retries + 1):\n            result = self.put_record(data, partition_key)\n            \n            if result['success']:\n                return result\n            \n            if attempt < max_retries:\n                # Exponential backoff with jitter\n                wait_time = (2 ** attempt) + random.uniform(0, 1)\n                time.sleep(wait_time)\n        \n        return result\n"})}),"\n",(0,a.jsx)(n.h3,{id:"3-advanced-producer-v\u1edbi-kpl",children:"3. Advanced Producer v\u1edbi KPL"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# Kinesis Producer Library (KPL) configuration\nclass AdvancedKinesisProducer:\n    def __init__(self, stream_name):\n        self.stream_name = stream_name\n        self.producer = self.create_kpl_producer()\n    \n    def create_kpl_producer(self):\n        \"\"\"Create KPL producer with optimized configuration\"\"\"\n        \n        from amazon_kinesis_producer import KinesisProducer\n        \n        config = {\n            'region': 'us-west-2',\n            'aggregation_enabled': True,\n            'aggregation_max_count': 4294967295,\n            'aggregation_max_size': 51200,\n            'collection_max_count': 500,\n            'collection_max_size': 5242880,\n            'record_max_buffered_time': 100,\n            'request_timeout': 6000,\n            'record_ttl': 30000,\n            'metrics_level': 'detailed',\n            'metrics_granularity': 'shard'\n        }\n        \n        return KinesisProducer(config)\n    \n    def send_async(self, data, partition_key):\n        \"\"\"Send record asynchronously with KPL\"\"\"\n        \n        future = self.producer.add_user_record(\n            stream_name=self.stream_name,\n            partition_key=partition_key,\n            data=json.dumps(data)\n        )\n        \n        return future\n    \n    def send_batch_async(self, records):\n        \"\"\"Send batch of records asynchronously\"\"\"\n        \n        futures = []\n        \n        for record in records:\n            future = self.send_async(record['data'], record['partition_key'])\n            futures.append(future)\n        \n        return futures\n    \n    def flush_and_wait(self):\n        \"\"\"Flush all pending records and wait for completion\"\"\"\n        \n        self.producer.flush_sync()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"stream-consumer-implementation",children:"Stream Consumer Implementation"}),"\n",(0,a.jsx)(n.h3,{id:"1-lambda-consumer",children:"1. Lambda Consumer"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import json\nimport base64\nfrom datetime import datetime\n\ndef lambda_handler(event, context):\n    \"\"\"Process Kinesis records in Lambda\"\"\"\n    \n    processed_records = []\n    failed_records = []\n    \n    for record in event['Records']:\n        try:\n            # Decode Kinesis data\n            payload = base64.b64decode(record['kinesis']['data'])\n            data = json.loads(payload)\n            \n            # Process the record\n            processed_data = process_kinesis_record(data, record)\n            processed_records.append(processed_data)\n            \n        except Exception as e:\n            failed_records.append({\n                'recordId': record['kinesis']['sequenceNumber'],\n                'error': str(e)\n            })\n    \n    # Store processed records\n    if processed_records:\n        store_processed_records(processed_records)\n    \n    # Handle failed records\n    if failed_records:\n        handle_failed_records(failed_records)\n    \n    return {\n        'statusCode': 200,\n        'body': json.dumps({\n            'processed': len(processed_records),\n            'failed': len(failed_records)\n        })\n    }\n\ndef process_kinesis_record(data, kinesis_record):\n    \"\"\"Process individual Kinesis record\"\"\"\n    \n    processed = {\n        'original_data': data,\n        'kinesis_metadata': {\n            'shard_id': kinesis_record['kinesis']['shardId'],\n            'sequence_number': kinesis_record['kinesis']['sequenceNumber'],\n            'approximate_arrival_timestamp': kinesis_record['kinesis']['approximateArrivalTimestamp']\n        },\n        'processed_at': datetime.utcnow().isoformat()\n    }\n    \n    # Add business logic processing\n    if data.get('event_type') == 'user_action':\n        processed['user_segment'] = determine_user_segment(data)\n        processed['action_score'] = calculate_action_score(data)\n    \n    return processed\n\ndef determine_user_segment(data):\n    \"\"\"Determine user segment based on data\"\"\"\n    \n    user_id = data.get('user_id')\n    action_type = data.get('action_type')\n    \n    # Simple segmentation logic\n    if action_type in ['purchase', 'premium_feature']:\n        return 'high_value'\n    elif action_type in ['login', 'page_view']:\n        return 'active'\n    else:\n        return 'standard'\n\ndef calculate_action_score(data):\n    \"\"\"Calculate action importance score\"\"\"\n    \n    score_map = {\n        'purchase': 10,\n        'signup': 8,\n        'premium_feature': 7,\n        'login': 3,\n        'page_view': 1\n    }\n    \n    return score_map.get(data.get('action_type'), 0)\n"})}),"\n",(0,a.jsx)(n.h3,{id:"2-kcl-consumer-application",children:"2. KCL Consumer Application"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from amazon_kclpy import kcl\nfrom amazon_kclpy.v3 import processor\nimport json\n\nclass KinesisRecordProcessor(processor.RecordProcessorBase):\n    """\n    Kinesis Client Library record processor\n    """\n    \n    def __init__(self):\n        self.shard_id = None\n        self.checkpoint_freq_seconds = 60\n        self.last_checkpoint_time = 0\n    \n    def initialize(self, initialize_input):\n        """Initialize the processor"""\n        \n        self.shard_id = initialize_input.shard_id\n        print(f"Initialized processor for shard: {self.shard_id}")\n    \n    def process_records(self, process_records_input):\n        """Process batch of records"""\n        \n        try:\n            records = process_records_input.records\n            \n            for record in records:\n                # Process individual record\n                self.process_single_record(record)\n            \n            # Checkpoint periodically\n            current_time = time.time()\n            if current_time - self.last_checkpoint_time > self.checkpoint_freq_seconds:\n                self.checkpoint(process_records_input.checkpointer)\n                self.last_checkpoint_time = current_time\n                \n        except Exception as e:\n            print(f"Error processing records: {str(e)}")\n    \n    def process_single_record(self, record):\n        """Process individual record"""\n        \n        try:\n            # Decode record data\n            data = json.loads(record.binary_data.decode(\'utf-8\'))\n            \n            # Business logic processing\n            if data.get(\'event_type\') == \'order\':\n                self.process_order_event(data)\n            elif data.get(\'event_type\') == \'user_activity\':\n                self.process_user_activity(data)\n            \n            print(f"Processed record: {record.sequence_number}")\n            \n        except Exception as e:\n            print(f"Error processing record {record.sequence_number}: {str(e)}")\n    \n    def process_order_event(self, data):\n        """Process order-related events"""\n        \n        # Update real-time metrics\n        self.update_order_metrics(data)\n        \n        # Check for fraud patterns\n        if self.detect_fraud_pattern(data):\n            self.send_fraud_alert(data)\n        \n        # Update inventory\n        self.update_inventory(data)\n    \n    def process_user_activity(self, data):\n        """Process user activity events"""\n        \n        # Update user profile\n        self.update_user_profile(data)\n        \n        # Personalization engine\n        self.update_recommendations(data)\n        \n        # Analytics tracking\n        self.track_user_journey(data)\n    \n    def checkpoint(self, checkpointer):\n        """Checkpoint progress"""\n        \n        try:\n            checkpointer.checkpoint()\n            print(f"Checkpointed shard: {self.shard_id}")\n        except Exception as e:\n            print(f"Error checkpointing: {str(e)}")\n    \n    def shutdown(self, shutdown_input):\n        """Shutdown processor"""\n        \n        try:\n            if shutdown_input.reason == \'TERMINATE\':\n                # Final checkpoint\n                shutdown_input.checkpointer.checkpoint()\n            \n            print(f"Shutdown processor for shard: {self.shard_id}")\n            \n        except Exception as e:\n            print(f"Error during shutdown: {str(e)}")\n\n# KCL application configuration\nif __name__ == \'__main__\':\n    kcl_process = kcl.KCLProcess(KinesisRecordProcessor())\n    kcl_process.run()\n'})}),"\n",(0,a.jsx)(n.h2,{id:"scaling-v\xe0-performance",children:"Scaling v\xe0 Performance"}),"\n",(0,a.jsx)(n.h3,{id:"1-shard-management",children:"1. Shard Management"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"class KinesisShardManager:\n    def __init__(self, stream_name):\n        self.kinesis = boto3.client('kinesis')\n        self.cloudwatch = boto3.client('cloudwatch')\n        self.stream_name = stream_name\n    \n    def get_stream_metrics(self):\n        \"\"\"Get stream performance metrics\"\"\"\n        \n        from datetime import datetime, timedelta\n        \n        end_time = datetime.utcnow()\n        start_time = end_time - timedelta(hours=1)\n        \n        metrics = {}\n        \n        # Get incoming records metric\n        response = self.cloudwatch.get_metric_statistics(\n            Namespace='AWS/Kinesis',\n            MetricName='IncomingRecords',\n            Dimensions=[\n                {\n                    'Name': 'StreamName',\n                    'Value': self.stream_name\n                }\n            ],\n            StartTime=start_time,\n            EndTime=end_time,\n            Period=300,\n            Statistics=['Sum', 'Average']\n        )\n        \n        metrics['incoming_records'] = response['Datapoints']\n        \n        # Get incoming bytes metric\n        response = self.cloudwatch.get_metric_statistics(\n            Namespace='AWS/Kinesis',\n            MetricName='IncomingBytes',\n            Dimensions=[\n                {\n                    'Name': 'StreamName',\n                    'Value': self.stream_name\n                }\n            ],\n            StartTime=start_time,\n            EndTime=end_time,\n            Period=300,\n            Statistics=['Sum', 'Average']\n        )\n        \n        metrics['incoming_bytes'] = response['Datapoints']\n        \n        return metrics\n    \n    def should_scale_out(self):\n        \"\"\"Determine if stream should scale out\"\"\"\n        \n        metrics = self.get_stream_metrics()\n        \n        # Check if any shard is approaching limits\n        stream_description = self.kinesis.describe_stream(StreamName=self.stream_name)\n        shard_count = len(stream_description['StreamDescription']['Shards'])\n        \n        # Calculate average throughput per shard\n        if metrics['incoming_records']:\n            avg_records_per_minute = sum([dp['Sum'] for dp in metrics['incoming_records']]) / len(metrics['incoming_records'])\n            records_per_shard_per_minute = avg_records_per_minute / shard_count\n            \n            # Scale out if approaching 1000 records/second per shard\n            if records_per_shard_per_minute > 50000:  # 1000 * 60 * 0.8 (80% threshold)\n                return True\n        \n        if metrics['incoming_bytes']:\n            avg_bytes_per_minute = sum([dp['Sum'] for dp in metrics['incoming_bytes']]) / len(metrics['incoming_bytes'])\n            bytes_per_shard_per_minute = avg_bytes_per_minute / shard_count\n            \n            # Scale out if approaching 1MB/second per shard\n            if bytes_per_shard_per_minute > 48000000:  # 1MB * 60 * 0.8 (80% threshold)\n                return True\n        \n        return False\n    \n    def scale_stream(self, target_shard_count):\n        \"\"\"Scale stream to target shard count\"\"\"\n        \n        current_description = self.kinesis.describe_stream(StreamName=self.stream_name)\n        current_shard_count = len(current_description['StreamDescription']['Shards'])\n        \n        if target_shard_count > current_shard_count:\n            # Scale out\n            self.kinesis.update_shard_count(\n                StreamName=self.stream_name,\n                TargetShardCount=target_shard_count,\n                ScalingType='UNIFORM_SCALING'\n            )\n            print(f\"Scaling out from {current_shard_count} to {target_shard_count} shards\")\n            \n        elif target_shard_count < current_shard_count:\n            # Scale in\n            self.kinesis.update_shard_count(\n                StreamName=self.stream_name,\n                TargetShardCount=target_shard_count,\n                ScalingType='UNIFORM_SCALING'\n            )\n            print(f\"Scaling in from {current_shard_count} to {target_shard_count} shards\")\n    \n    def auto_scale_based_on_metrics(self):\n        \"\"\"Automatically scale based on metrics\"\"\"\n        \n        if self.should_scale_out():\n            current_description = self.kinesis.describe_stream(StreamName=self.stream_name)\n            current_shard_count = len(current_description['StreamDescription']['Shards'])\n            \n            # Scale out by 50%\n            new_shard_count = int(current_shard_count * 1.5)\n            self.scale_stream(new_shard_count)\n"})}),"\n",(0,a.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Partition Key Design"}),": Choose partition keys \u0111\u1ec3 distribute data evenly"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Shard Management"}),": Monitor v\xe0 scale shards appropriately"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Consumer Design"}),": Implement efficient consumer applications"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Error Handling"}),": Handle failures v\xe0 implement retry logic"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Monitoring"}),": Set up comprehensive monitoring"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Security"}),": Use IAM roles v\xe0 encrypt sensitive data"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Cost Optimization"}),": Use on-demand mode cho variable workloads"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Data Retention"}),": Set appropriate retention periods"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"t\xedch-h\u1ee3p-v\u1edbi-c\xe1c-d\u1ecbch-v\u1ee5-aws-kh\xe1c",children:"T\xedch h\u1ee3p v\u1edbi c\xe1c d\u1ecbch v\u1ee5 AWS kh\xe1c"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"AWS Lambda"}),": Serverless stream processing"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Amazon Kinesis Data Analytics"}),": Real-time analytics"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Amazon Kinesis Data Firehose"}),": Data delivery"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Amazon DynamoDB"}),": Real-time data storage"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Amazon S3"}),": Data archival v\xe0 analytics"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Amazon CloudWatch"}),": Monitoring v\xe0 alerting"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"AWS IAM"}),": Security v\xe0 access control"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Amazon VPC"}),": Network isolation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"AWS KMS"}),": Data encryption"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Amazon SNS"}),": Notifications v\xe0 alerts"]}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(l,{...e})}):l(e)}}}]);