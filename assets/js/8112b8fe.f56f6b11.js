"use strict";(self.webpackChunkkeep_being_human_dev=self.webpackChunkkeep_being_human_dev||[]).push([[77162],{47560:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/redshift-dense-storage-node-79ebe488e27c5b4c1136eeb1a0c9a5a6.png"},65404:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>o});var a=t(36672);const s={},r=a.createContext(s);function i(e){const n=a.useContext(r);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),a.createElement(r.Provider,{value:n},e.children)}},70333:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>i,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"diagrams/aws-nodes/aws-analytics/redshift-dense-storage-node","title":"Amazon Redshift Dense Storage Node","description":"Overview","source":"@site/docs/diagrams/aws-nodes/aws-analytics/28-redshift-dense-storage-node.md","sourceDirName":"diagrams/aws-nodes/aws-analytics","slug":"/diagrams/aws-nodes/aws-analytics/redshift-dense-storage-node","permalink":"/keep-being-human-dev/docs/diagrams/aws-nodes/aws-analytics/redshift-dense-storage-node","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/diagrams/aws-nodes/aws-analytics/28-redshift-dense-storage-node.md","tags":[],"version":"current","sidebarPosition":28,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Amazon Redshift Dense Compute Node","permalink":"/keep-being-human-dev/docs/diagrams/aws-nodes/aws-analytics/redshift-dense-compute-node"},"next":{"title":"Amazon Redshift","permalink":"/keep-being-human-dev/docs/diagrams/aws-nodes/aws-analytics/redshift"}}');var s=t(23420),r=t(65404);const i={},o="Amazon Redshift Dense Storage Node",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Main Functions",id:"main-functions",level:2},{value:"Large-Scale Storage",id:"large-scale-storage",level:3},{value:"Performance Characteristics",id:"performance-characteristics",level:3},{value:"Scalability Features",id:"scalability-features",level:3},{value:"Use Cases",id:"use-cases",level:2},{value:"Large-Scale Data Warehouse",id:"large-scale-data-warehouse",level:3},{value:"Historical Data Analytics",id:"historical-data-analytics",level:3},{value:"Architecture Diagram",id:"architecture-diagram",level:2},{value:"Node Type Specifications",id:"node-type-specifications",level:2},{value:"DS2 Instance Types",id:"ds2-instance-types",level:3},{value:"Performance Characteristics",id:"performance-characteristics-1",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Storage Optimization",id:"storage-optimization",level:3},{value:"Cost Management",id:"cost-management",level:3},{value:"Performance Tuning",id:"performance-tuning",level:3},{value:"When to Choose DS Nodes",id:"when-to-choose-ds-nodes",level:2},{value:"Ideal Scenarios",id:"ideal-scenarios",level:3},{value:"Migration Considerations",id:"migration-considerations",level:3},{value:"Monitoring and Troubleshooting",id:"monitoring-and-troubleshooting",level:2},{value:"Key Metrics",id:"key-metrics",level:3},{value:"Common Issues",id:"common-issues",level:3},{value:"Troubleshooting Steps",id:"troubleshooting-steps",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"amazon-redshift-dense-storage-node",children:"Amazon Redshift Dense Storage Node"})}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"Amazon Redshift Dense Storage (DS) nodes are optimized for large data warehouses with massive storage requirements. DS nodes use large magnetic drives to provide very large storage capacity at a lower cost per GB compared to Dense Compute nodes. They are ideal for workloads that require large amounts of storage and can tolerate slightly higher query latency in exchange for cost savings on storage."}),"\n",(0,s.jsx)(n.h2,{id:"main-functions",children:"Main Functions"}),"\n",(0,s.jsx)(n.h3,{id:"large-scale-storage",children:"Large-Scale Storage"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"High Capacity HDDs"}),": Large magnetic drives for maximum storage capacity"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cost-Effective Storage"}),": Lower cost per GB compared to SSD-based nodes"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Massive Data Warehouses"}),": Support for multi-petabyte data warehouses"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Long-Term Data Retention"}),": Ideal for historical data storage and analysis"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"performance-characteristics",children:"Performance Characteristics"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Optimized for Throughput"}),": High sequential read/write performance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Columnar Compression"}),": Excellent compression ratios for analytical data"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Parallel Processing"}),": MPP architecture for distributed query processing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Workload Optimization"}),": Tuned for large-scale analytical workloads"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"scalability-features",children:"Scalability Features"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Elastic Resize"}),": Scale cluster size without downtime"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Storage Scaling"}),": Add nodes to increase storage capacity"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Performance Tuning"}),": Optimize for large dataset queries"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Data Distribution"}),": Efficient data distribution across nodes"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"use-cases",children:"Use Cases"}),"\n",(0,s.jsx)(n.h3,{id:"large-scale-data-warehouse",children:"Large-Scale Data Warehouse"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import boto3\nimport psycopg2\nfrom datetime import datetime, timedelta\nimport pandas as pd\n\nclass RedshiftDSClusterManager:\n    def __init__(self):\n        self.redshift = boto3.client('redshift')\n        self.cloudwatch = boto3.client('cloudwatch')\n    \n    def create_ds_cluster(self, cluster_identifier, node_type='ds2.8xlarge', num_nodes=8):\n        \"\"\"Create Dense Storage Redshift cluster for large datasets\"\"\"\n        try:\n            response = self.redshift.create_cluster(\n                ClusterIdentifier=cluster_identifier,\n                NodeType=node_type,\n                MasterUsername='admin',\n                MasterUserPassword='SecurePassword123!',\n                DBName='datawarehouse',\n                NumberOfNodes=num_nodes,\n                ClusterType='multi-node',\n                VpcSecurityGroupIds=['sg-12345678'],\n                ClusterSubnetGroupName='redshift-subnet-group',\n                PubliclyAccessible=False,\n                Encrypted=True,\n                KmsKeyId='arn:aws:kms:us-east-1:123456789012:key/12345678-1234-1234-1234-123456789012',\n                EnhancedVpcRouting=True,\n                ClusterParameterGroupName='large-storage-params',\n                Tags=[\n                    {\n                        'Key': 'Environment',\n                        'Value': 'Production'\n                    },\n                    {\n                        'Key': 'NodeType',\n                        'Value': 'DenseStorage'\n                    },\n                    {\n                        'Key': 'DataRetention',\n                        'Value': '7Years'\n                    }\n                ]\n            )\n            print(f\"DS cluster creation initiated: {cluster_identifier}\")\n            return response\n        except Exception as e:\n            print(f\"Error creating DS cluster: {e}\")\n    \n    def optimize_for_large_datasets(self, cluster_identifier):\n        \"\"\"Configure cluster for large dataset optimization\"\"\"\n        try:\n            # Create parameter group optimized for large datasets\n            param_group_response = self.redshift.create_cluster_parameter_group(\n                ParameterGroupName='ds-large-dataset-optimized',\n                ParameterGroupFamily='redshift-1.0',\n                Description='Optimized parameters for DS nodes with large datasets'\n            )\n            \n            # Set parameters for large dataset performance\n            large_dataset_params = [\n                {\n                    'ParameterName': 'wlm_json_configuration',\n                    'ParameterValue': '''[\n                        {\n                            \"query_group\": \"etl_heavy\",\n                            \"query_group_wild_card\": 0,\n                            \"query_concurrency\": 3,\n                            \"memory_percent_to_use\": 70,\n                            \"max_execution_time\": 7200000\n                        },\n                        {\n                            \"query_group\": \"reporting\",\n                            \"query_group_wild_card\": 0,\n                            \"query_concurrency\": 5,\n                            \"memory_percent_to_use\": 25,\n                            \"max_execution_time\": 1800000\n                        },\n                        {\n                            \"query_concurrency\": 2,\n                            \"memory_percent_to_use\": 5\n                        }\n                    ]'''\n                },\n                {\n                    'ParameterName': 'max_cursor_result_set_size',\n                    'ParameterValue': '10000'\n                },\n                {\n                    'ParameterName': 'statement_timeout',\n                    'ParameterValue': '7200000'  # 2 hours for large queries\n                },\n                {\n                    'ParameterName': 'extra_float_digits',\n                    'ParameterValue': '2'\n                }\n            ]\n            \n            for param in large_dataset_params:\n                self.redshift.modify_cluster_parameter_group(\n                    ParameterGroupName='ds-large-dataset-optimized',\n                    Parameters=[param]\n                )\n            \n            # Apply parameter group to cluster\n            self.redshift.modify_cluster(\n                ClusterIdentifier=cluster_identifier,\n                ClusterParameterGroupName='ds-large-dataset-optimized',\n                ApplyImmediately=False  # Apply during maintenance window\n            )\n            \n            print(f\"Large dataset optimization applied to {cluster_identifier}\")\n            \n        except Exception as e:\n            print(f\"Error optimizing cluster for large datasets: {e}\")\n    \n    def implement_data_lifecycle_management(self, cluster_identifier):\n        \"\"\"Implement data lifecycle management for cost optimization\"\"\"\n        try:\n            # Create snapshot schedule for data retention\n            schedule_response = self.redshift.create_snapshot_schedule(\n                ScheduleIdentifier='weekly-retention-schedule',\n                ScheduleDefinitions=[\n                    'rate(7 days)'  # Weekly snapshots\n                ],\n                ScheduleDescription='Weekly snapshots for long-term data retention',\n                Tags=[\n                    {\n                        'Key': 'Purpose',\n                        'Value': 'DataLifecycle'\n                    }\n                ]\n            )\n            \n            # Associate schedule with cluster\n            self.redshift.modify_cluster_snapshot_schedule(\n                ClusterIdentifier=cluster_identifier,\n                ScheduleIdentifier='weekly-retention-schedule'\n            )\n            \n            # Create usage limit for cost control\n            usage_limit_response = self.redshift.create_usage_limit(\n                ClusterIdentifier=cluster_identifier,\n                FeatureType='spectrum',\n                LimitType='data-scanned',\n                Amount=1000,  # 1TB per month\n                Period='monthly',\n                BreachAction='log'\n            )\n            \n            print(f\"Data lifecycle management implemented for {cluster_identifier}\")\n            return schedule_response\n            \n        except Exception as e:\n            print(f\"Error implementing data lifecycle management: {e}\")\n    \n    def monitor_storage_utilization(self, cluster_identifier):\n        \"\"\"Monitor storage utilization and performance\"\"\"\n        try:\n            end_time = datetime.utcnow()\n            start_time = end_time - timedelta(hours=24)\n            \n            # Get storage utilization metrics\n            storage_response = self.cloudwatch.get_metric_statistics(\n                Namespace='AWS/Redshift',\n                MetricName='PercentageDiskSpaceUsed',\n                Dimensions=[\n                    {\n                        'Name': 'ClusterIdentifier',\n                        'Value': cluster_identifier\n                    }\n                ],\n                StartTime=start_time,\n                EndTime=end_time,\n                Period=3600,  # 1 hour intervals\n                Statistics=['Average', 'Maximum']\n            )\n            \n            # Get read/write IOPS\n            read_iops_response = self.cloudwatch.get_metric_statistics(\n                Namespace='AWS/Redshift',\n                MetricName='ReadIOPS',\n                Dimensions=[\n                    {\n                        'Name': 'ClusterIdentifier',\n                        'Value': cluster_identifier\n                    }\n                ],\n                StartTime=start_time,\n                EndTime=end_time,\n                Period=3600,\n                Statistics=['Average', 'Maximum']\n            )\n            \n            write_iops_response = self.cloudwatch.get_metric_statistics(\n                Namespace='AWS/Redshift',\n                MetricName='WriteIOPS',\n                Dimensions=[\n                    {\n                        'Name': 'ClusterIdentifier',\n                        'Value': cluster_identifier\n                    }\n                ],\n                StartTime=start_time,\n                EndTime=end_time,\n                Period=3600,\n                Statistics=['Average', 'Maximum']\n            )\n            \n            storage_metrics = {\n                'disk_usage': storage_response['Datapoints'],\n                'read_iops': read_iops_response['Datapoints'],\n                'write_iops': write_iops_response['Datapoints']\n            }\n            \n            return storage_metrics\n            \n        except Exception as e:\n            print(f\"Error monitoring storage utilization: {e}\")\n\n# Usage example\nds_manager = RedshiftDSClusterManager()\n\n# Create DS cluster for large datasets\nds_manager.create_ds_cluster(\n    cluster_identifier='enterprise-ds-cluster',\n    node_type='ds2.8xlarge',\n    num_nodes=16  # Large cluster for enterprise data warehouse\n)\n\n# Optimize for large datasets\nds_manager.optimize_for_large_datasets('enterprise-ds-cluster')\n\n# Implement data lifecycle management\nds_manager.implement_data_lifecycle_management('enterprise-ds-cluster')\n\n# Monitor storage utilization\nstorage_data = ds_manager.monitor_storage_utilization('enterprise-ds-cluster')\nprint(\"Storage metrics:\", storage_data)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"historical-data-analytics",children:"Historical Data Analytics"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import psycopg2\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\nclass DSHistoricalAnalytics:\n    def __init__(self, cluster_endpoint, database, username, password):\n        self.connection = psycopg2.connect(\n            host=cluster_endpoint,\n            database=database,\n            user=username,\n            password=password,\n            port=5439\n        )\n        self.connection.set_session(autocommit=True)\n    \n    def create_historical_tables(self):\n        """Create tables optimized for historical data storage"""\n        cursor = self.connection.cursor()\n        \n        # Create historical sales fact table with date-based distribution\n        cursor.execute("""\n            CREATE TABLE IF NOT EXISTS historical_sales_fact (\n                transaction_id BIGINT,\n                customer_id INTEGER,\n                product_id INTEGER,\n                store_id INTEGER,\n                transaction_date DATE,\n                transaction_timestamp TIMESTAMP,\n                quantity INTEGER,\n                unit_price DECIMAL(10,2),\n                total_amount DECIMAL(12,2),\n                discount_amount DECIMAL(10,2),\n                tax_amount DECIMAL(10,2),\n                payment_method VARCHAR(50),\n                sales_rep_id INTEGER\n            )\n            DISTSTYLE KEY\n            DISTKEY(transaction_date)\n            SORTKEY(transaction_date, customer_id)\n            ENCODE AUTO;\n        """)\n        \n        # Create customer dimension with historical tracking\n        cursor.execute("""\n            CREATE TABLE IF NOT EXISTS customer_dimension_scd (\n                customer_key BIGINT IDENTITY(1,1),\n                customer_id INTEGER,\n                customer_name VARCHAR(255),\n                email VARCHAR(255),\n                phone VARCHAR(20),\n                address VARCHAR(500),\n                city VARCHAR(100),\n                state VARCHAR(50),\n                country VARCHAR(50),\n                customer_segment VARCHAR(50),\n                effective_date DATE,\n                expiration_date DATE,\n                is_current BOOLEAN DEFAULT TRUE\n            )\n            DISTSTYLE ALL\n            SORTKEY(customer_id, effective_date)\n            ENCODE AUTO;\n        """)\n        \n        # Create product hierarchy for historical analysis\n        cursor.execute("""\n            CREATE TABLE IF NOT EXISTS product_hierarchy_historical (\n                product_id INTEGER,\n                product_name VARCHAR(255),\n                brand VARCHAR(100),\n                category VARCHAR(100),\n                subcategory VARCHAR(100),\n                department VARCHAR(100),\n                product_line VARCHAR(100),\n                launch_date DATE,\n                discontinue_date DATE,\n                cost DECIMAL(10,2),\n                list_price DECIMAL(10,2),\n                effective_date DATE,\n                expiration_date DATE\n            )\n            DISTSTYLE ALL\n            SORTKEY(product_id, effective_date)\n            ENCODE AUTO;\n        """)\n        \n        print("Historical tables created for DS nodes")\n    \n    def load_historical_data_efficiently(self, table_name, s3_path, date_column):\n        """Load historical data with optimized COPY command"""\n        cursor = self.connection.cursor()\n        \n        # Use COPY command optimized for large datasets\n        copy_command = f"""\n            COPY {table_name}\n            FROM \'{s3_path}\'\n            IAM_ROLE \'arn:aws:iam::123456789012:role/RedshiftRole\'\n            FORMAT AS PARQUET\n            COMPUPDATE ON\n            STATUPDATE ON\n            MAXERROR 1000\n            ACCEPTINVCHARS\n            DATEFORMAT \'YYYY-MM-DD\'\n            TIMEFORMAT \'YYYY-MM-DD HH:MI:SS\';\n        """\n        \n        start_time = datetime.now()\n        cursor.execute(copy_command)\n        end_time = datetime.now()\n        \n        # Get load statistics\n        cursor.execute("""\n            SELECT \n                filename,\n                lines_scanned,\n                lines_skipped,\n                load_time,\n                bytes\n            FROM stl_load_commits \n            WHERE query = pg_last_copy_id()\n            ORDER BY starttime DESC;\n        """)\n        \n        load_stats = cursor.fetchall()\n        total_time = (end_time - start_time).total_seconds()\n        \n        print(f"Historical data loaded in {total_time:.2f} seconds")\n        for stat in load_stats:\n            print(f"File: {stat[0]}, Lines: {stat[1]}, Bytes: {stat[4]}")\n    \n    def analyze_long_term_trends(self):\n        """Analyze long-term historical trends"""\n        cursor = self.connection.cursor()\n        \n        # Multi-year revenue trend analysis\n        cursor.execute("""\n            SELECT \n                EXTRACT(year FROM transaction_date) as year,\n                EXTRACT(quarter FROM transaction_date) as quarter,\n                COUNT(*) as transaction_count,\n                SUM(total_amount) as total_revenue,\n                AVG(total_amount) as avg_transaction_value,\n                COUNT(DISTINCT customer_id) as unique_customers,\n                SUM(quantity) as total_units_sold\n            FROM historical_sales_fact\n            WHERE transaction_date >= \'2015-01-01\'\n            GROUP BY 1, 2\n            ORDER BY 1, 2;\n        """)\n        \n        trend_data = cursor.fetchall()\n        \n        # Customer lifetime value analysis\n        cursor.execute("""\n            WITH customer_metrics AS (\n                SELECT \n                    customer_id,\n                    MIN(transaction_date) as first_purchase,\n                    MAX(transaction_date) as last_purchase,\n                    COUNT(*) as total_transactions,\n                    SUM(total_amount) as lifetime_value,\n                    AVG(total_amount) as avg_order_value,\n                    DATEDIFF(day, MIN(transaction_date), MAX(transaction_date)) as customer_lifespan_days\n                FROM historical_sales_fact\n                GROUP BY 1\n            )\n            SELECT \n                CASE \n                    WHEN customer_lifespan_days >= 1095 THEN \'3+ Years\'\n                    WHEN customer_lifespan_days >= 730 THEN \'2-3 Years\'\n                    WHEN customer_lifespan_days >= 365 THEN \'1-2 Years\'\n                    ELSE \'Less than 1 Year\'\n                END as customer_tenure,\n                COUNT(*) as customer_count,\n                AVG(lifetime_value) as avg_lifetime_value,\n                AVG(total_transactions) as avg_transactions,\n                AVG(avg_order_value) as avg_order_value\n            FROM customer_metrics\n            WHERE total_transactions > 1\n            GROUP BY 1\n            ORDER BY 2 DESC;\n        """)\n        \n        clv_analysis = cursor.fetchall()\n        \n        # Seasonal pattern analysis\n        cursor.execute("""\n            SELECT \n                EXTRACT(month FROM transaction_date) as month,\n                EXTRACT(dow FROM transaction_date) as day_of_week,\n                AVG(total_amount) as avg_daily_revenue,\n                COUNT(*) as avg_daily_transactions\n            FROM historical_sales_fact\n            WHERE transaction_date >= CURRENT_DATE - INTERVAL \'3 years\'\n            GROUP BY 1, 2\n            ORDER BY 1, 2;\n        """)\n        \n        seasonal_patterns = cursor.fetchall()\n        \n        return {\n            \'revenue_trends\': trend_data,\n            \'customer_lifetime_value\': clv_analysis,\n            \'seasonal_patterns\': seasonal_patterns\n        }\n    \n    def optimize_table_maintenance(self):\n        """Perform maintenance operations optimized for DS nodes"""\n        cursor = self.connection.cursor()\n        \n        # Analyze table statistics\n        cursor.execute("""\n            SELECT \n                schemaname,\n                tablename,\n                size,\n                tbl_rows,\n                skew_sortkey1,\n                skew_rows,\n                estimated_visible_rows,\n                stats_off\n            FROM svv_table_info\n            WHERE schemaname = \'public\'\n                AND size > 1000  -- Focus on large tables\n            ORDER BY size DESC;\n        """)\n        \n        table_stats = cursor.fetchall()\n        \n        # Identify tables needing VACUUM\n        tables_needing_vacuum = []\n        for stat in table_stats:\n            if stat[7] > 10:  # stats_off > 10%\n                tables_needing_vacuum.append(stat[1])\n        \n        # Perform VACUUM on tables that need it\n        for table in tables_needing_vacuum:\n            print(f"Running VACUUM on {table}...")\n            cursor.execute(f"VACUUM {table};")\n            cursor.execute(f"ANALYZE {table};")\n        \n        return {\n            \'table_statistics\': table_stats,\n            \'vacuumed_tables\': tables_needing_vacuum\n        }\n\n# Usage example\nds_analytics = DSHistoricalAnalytics(\n    cluster_endpoint=\'enterprise-ds-cluster.abc123.us-east-1.redshift.amazonaws.com\',\n    database=\'datawarehouse\',\n    username=\'admin\',\n    password=\'SecurePassword123!\'\n)\n\n# Setup historical tables\nds_analytics.create_historical_tables()\n\n# Load historical data\nds_analytics.load_historical_data_efficiently(\n    table_name=\'historical_sales_fact\',\n    s3_path=\'s3://enterprise-data-lake/historical-sales/\',\n    date_column=\'transaction_date\'\n)\n\n# Analyze long-term trends\nhistorical_analysis = ds_analytics.analyze_long_term_trends()\n\n# Perform maintenance\nmaintenance_results = ds_analytics.optimize_table_maintenance()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"architecture-diagram",children:"Architecture Diagram"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Redshift Dense Storage Node Architecture",src:t(47560).A+"",width:"3684",height:"2488"})}),"\n",(0,s.jsx)(n.h2,{id:"node-type-specifications",children:"Node Type Specifications"}),"\n",(0,s.jsx)(n.h3,{id:"ds2-instance-types",children:"DS2 Instance Types"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ds2.xlarge"}),": 4 vCPUs, 31 GB RAM, 2 TB HDD"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ds2.8xlarge"}),": 36 vCPUs, 244 GB RAM, 16 TB HDD"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"performance-characteristics-1",children:"Performance Characteristics"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Storage"}),": Large magnetic drives for maximum capacity"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Network"}),": High network performance for data transfer"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Memory"}),": Substantial RAM for query processing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"CPU"}),": Optimized for analytical workloads"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,s.jsx)(n.h3,{id:"storage-optimization",children:"Storage Optimization"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Use appropriate distribution keys for even data distribution"}),"\n",(0,s.jsx)(n.li,{children:"Implement effective sort keys for query performance"}),"\n",(0,s.jsx)(n.li,{children:"Utilize columnar compression for storage efficiency"}),"\n",(0,s.jsx)(n.li,{children:"Regular VACUUM and ANALYZE operations"}),"\n",(0,s.jsx)(n.li,{children:"Monitor storage utilization and growth patterns"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"cost-management",children:"Cost Management"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Implement data lifecycle policies"}),"\n",(0,s.jsx)(n.li,{children:"Use Reserved Instances for predictable workloads"}),"\n",(0,s.jsx)(n.li,{children:"Monitor and optimize storage usage"}),"\n",(0,s.jsx)(n.li,{children:"Consider data archival strategies"}),"\n",(0,s.jsx)(n.li,{children:"Regular cost analysis and optimization"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"performance-tuning",children:"Performance Tuning"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Optimize table design for large datasets"}),"\n",(0,s.jsx)(n.li,{children:"Use materialized views for common aggregations"}),"\n",(0,s.jsx)(n.li,{children:"Implement workload management queues"}),"\n",(0,s.jsx)(n.li,{children:"Monitor query performance and optimize"}),"\n",(0,s.jsx)(n.li,{children:"Use result caching for frequently accessed data"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"when-to-choose-ds-nodes",children:"When to Choose DS Nodes"}),"\n",(0,s.jsx)(n.h3,{id:"ideal-scenarios",children:"Ideal Scenarios"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Large data warehouses (multi-TB to PB scale)"}),"\n",(0,s.jsx)(n.li,{children:"Historical data analysis and reporting"}),"\n",(0,s.jsx)(n.li,{children:"Cost-sensitive workloads with large storage needs"}),"\n",(0,s.jsx)(n.li,{children:"Batch processing and ETL workloads"}),"\n",(0,s.jsx)(n.li,{children:"Long-term data retention requirements"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"migration-considerations",children:"Migration Considerations"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Evaluate storage vs. performance requirements"}),"\n",(0,s.jsx)(n.li,{children:"Consider RA3 nodes for better price-performance"}),"\n",(0,s.jsx)(n.li,{children:"Plan for data growth and scalability"}),"\n",(0,s.jsx)(n.li,{children:"Assess query performance requirements"}),"\n",(0,s.jsx)(n.li,{children:"Test with representative workloads"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"monitoring-and-troubleshooting",children:"Monitoring and Troubleshooting"}),"\n",(0,s.jsx)(n.h3,{id:"key-metrics",children:"Key Metrics"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Storage utilization and growth trends"}),"\n",(0,s.jsx)(n.li,{children:"Query performance and execution times"}),"\n",(0,s.jsx)(n.li,{children:"I/O throughput and latency"}),"\n",(0,s.jsx)(n.li,{children:"Table maintenance requirements"}),"\n",(0,s.jsx)(n.li,{children:"Cost per GB stored"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Storage capacity limitations"}),"\n",(0,s.jsx)(n.li,{children:"Query performance on large datasets"}),"\n",(0,s.jsx)(n.li,{children:"Table maintenance overhead"}),"\n",(0,s.jsx)(n.li,{children:"Data skew and distribution issues"}),"\n",(0,s.jsx)(n.li,{children:"Cost optimization challenges"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"troubleshooting-steps",children:"Troubleshooting Steps"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Monitor storage utilization trends"}),"\n",(0,s.jsx)(n.li,{children:"Analyze query performance patterns"}),"\n",(0,s.jsx)(n.li,{children:"Review table statistics and distribution"}),"\n",(0,s.jsx)(n.li,{children:"Optimize sort and distribution keys"}),"\n",(0,s.jsx)(n.li,{children:"Implement regular maintenance schedules"}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);