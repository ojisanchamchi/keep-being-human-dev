"use strict";(self.webpackChunkkeep_being_human_dev=self.webpackChunkkeep_being_human_dev||[]).push([[97515],{47911:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>i,default:()=>u,frontMatter:()=>s,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"aws-nodes/aws-analytics/glue","title":"AWS Glue","description":"T\u1ed5ng quan","source":"@site/docs/aws-nodes/aws-analytics/18-glue.md","sourceDirName":"aws-nodes/aws-analytics","slug":"/aws-nodes/aws-analytics/glue","permalink":"/keep-being-human-dev/docs/aws-nodes/aws-analytics/glue","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/aws-nodes/aws-analytics/18-glue.md","tags":[],"version":"current","sidebarPosition":18,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"AWS Glue Data Catalog","permalink":"/keep-being-human-dev/docs/aws-nodes/aws-analytics/glue-data-catalog"},"next":{"title":"Amazon Kinesis Data Analytics","permalink":"/keep-being-human-dev/docs/aws-nodes/aws-analytics/kinesis-data-analytics"}}');var o=t(23420),r=t(65404);const s={},i="AWS Glue",l={},c=[{value:"T\u1ed5ng quan",id:"t\u1ed5ng-quan",level:2},{value:"Ch\u1ee9c n\u0103ng ch\xednh",id:"ch\u1ee9c-n\u0103ng-ch\xednh",level:2},{value:"1. ETL Processing",id:"1-etl-processing",level:3},{value:"2. Data Catalog",id:"2-data-catalog",level:3},{value:"3. Data Crawlers",id:"3-data-crawlers",level:3},{value:"4. Job Orchestration",id:"4-job-orchestration",level:3},{value:"Use Cases ph\u1ed5 bi\u1ebfn",id:"use-cases-ph\u1ed5-bi\u1ebfn",level:2},{value:"Diagram Architecture",id:"diagram-architecture",level:2},{value:"Code \u0111\u1ec3 t\u1ea1o diagram:",id:"code-\u0111\u1ec3-t\u1ea1o-diagram",level:3},{value:"ETL Job Development",id:"etl-job-development",level:2},{value:"1. Spark ETL Job",id:"1-spark-etl-job",level:3},{value:"2. Python Shell Job",id:"2-python-shell-job",level:3},{value:"3. Streaming ETL Job",id:"3-streaming-etl-job",level:3},{value:"Workflow Orchestration",id:"workflow-orchestration",level:2},{value:"1. Glue Workflow Definition",id:"1-glue-workflow-definition",level:3},{value:"2. Advanced Workflow Management",id:"2-advanced-workflow-management",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"1. Job Performance Tuning",id:"1-job-performance-tuning",level:3},{value:"2. Cost Optimization",id:"2-cost-optimization",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"T\xedch h\u1ee3p v\u1edbi c\xe1c d\u1ecbch v\u1ee5 AWS kh\xe1c",id:"t\xedch-h\u1ee3p-v\u1edbi-c\xe1c-d\u1ecbch-v\u1ee5-aws-kh\xe1c",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"aws-glue",children:"AWS Glue"})}),"\n",(0,o.jsx)(e.h2,{id:"t\u1ed5ng-quan",children:"T\u1ed5ng quan"}),"\n",(0,o.jsx)(e.p,{children:"AWS Glue l\xe0 node t\u1ed5ng qu\xe1t \u0111\u1ea1i di\u1ec7n cho fully managed ETL (Extract, Transform, Load) service c\u1ee7a AWS. Glue gi\xfap b\u1ea1n d\u1ec5 d\xe0ng chu\u1ea9n b\u1ecb v\xe0 load d\u1eef li\u1ec7u cho analytics b\u1eb1ng c\xe1ch t\u1ef1 \u0111\u1ed9ng h\xf3a vi\u1ec7c kh\xe1m ph\xe1 d\u1eef li\u1ec7u, schema inference, v\xe0 code generation. \u0110\xe2y l\xe0 serverless service cho ph\xe9p b\u1ea1n focus v\xe0o data transformation logic thay v\xec qu\u1ea3n l\xfd infrastructure."}),"\n",(0,o.jsx)(e.h2,{id:"ch\u1ee9c-n\u0103ng-ch\xednh",children:"Ch\u1ee9c n\u0103ng ch\xednh"}),"\n",(0,o.jsx)(e.h3,{id:"1-etl-processing",children:"1. ETL Processing"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Serverless ETL"}),": Kh\xf4ng c\u1ea7n qu\u1ea3n l\xfd servers ho\u1eb7c clusters"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Auto Scaling"}),": T\u1ef1 \u0111\u1ed9ng scale based on workload"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Code Generation"}),": T\u1ef1 \u0111\u1ed9ng generate ETL code"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Multiple Languages"}),": H\u1ed7 tr\u1ee3 Python v\xe0 Scala"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"2-data-catalog",children:"2. Data Catalog"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Centralized Metadata"}),": Single source of truth cho metadata"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Schema Discovery"}),": T\u1ef1 \u0111\u1ed9ng discover v\xe0 infer schemas"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Data Classification"}),": Classify sensitive data"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Version Control"}),": Track schema evolution"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"3-data-crawlers",children:"3. Data Crawlers"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Automatic Discovery"}),": T\u1ef1 \u0111\u1ed9ng discover data sources"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Schema Inference"}),": Infer schemas t\u1eeb data"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Partition Detection"}),": Detect partition structures"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Scheduled Crawling"}),": Schedule regular crawls"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"4-job-orchestration",children:"4. Job Orchestration"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Workflow Management"}),": Orchestrate complex ETL workflows"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Dependency Management"}),": Handle job dependencies"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Error Handling"}),": Built-in error handling v\xe0 retry logic"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Monitoring"}),": Comprehensive job monitoring"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"use-cases-ph\u1ed5-bi\u1ebfn",children:"Use Cases ph\u1ed5 bi\u1ebfn"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Data Lake ETL"}),": Transform data trong data lakes"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Data Warehouse Loading"}),": Load data v\xe0o data warehouses"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Data Migration"}),": Migrate data gi\u1eefa systems"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Real-time Processing"}),": Stream processing v\u1edbi Glue Streaming"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Data Quality"}),": Implement data quality checks"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"diagram-architecture",children:"Diagram Architecture"}),"\n",(0,o.jsx)(e.p,{children:"Ki\u1ebfn tr\xfac t\u1ed5ng quan AWS Glue ecosystem:"}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.img,{alt:"AWS Glue Architecture",src:t(53699).A+"",width:"1891",height:"2311"})}),"\n",(0,o.jsx)(e.h3,{id:"code-\u0111\u1ec3-t\u1ea1o-diagram",children:"Code \u0111\u1ec3 t\u1ea1o diagram:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.analytics import Glue, GlueCrawlers, GlueDataCatalog, Athena\nfrom diagrams.aws.storage import S3\nfrom diagrams.aws.compute import Lambda\nfrom diagrams.aws.database import RDS, Redshift, Dynamodb\nfrom diagrams.aws.integration import SQS, SNS, StepFunctions\nfrom diagrams.aws.management import Cloudwatch\nfrom diagrams.aws.security import IAM\nfrom diagrams.onprem.client import Users\nfrom diagrams.onprem.database import MySQL\n\nwith Diagram("AWS Glue Architecture", show=False, direction="TB"):\n    \n    users = Users("Data Engineers")\n    \n    with Cluster("Data Sources"):\n        s3_raw = S3("Raw Data")\n        rds_db = RDS("RDS Database")\n        mysql_db = MySQL("On-premises DB")\n        dynamodb_table = Dynamodb("DynamoDB")\n        streaming_data = SQS("Streaming Data")\n    \n    with Cluster("AWS Glue Services"):\n        glue_service = Glue("AWS Glue")\n        \n        with Cluster("Core Components"):\n            crawlers = GlueCrawlers("Glue Crawlers")\n            data_catalog = GlueDataCatalog("Data Catalog")\n            etl_jobs = Glue("ETL Jobs")\n            workflows = StepFunctions("Glue Workflows")\n    \n    with Cluster("Job Execution"):\n        spark_jobs = Glue("Spark ETL Jobs")\n        python_shell = Lambda("Python Shell Jobs")\n        streaming_jobs = Glue("Streaming Jobs")\n        ray_jobs = Glue("Ray Jobs")\n    \n    with Cluster("Data Targets"):\n        s3_processed = S3("Processed Data")\n        redshift_dw = Redshift("Data Warehouse")\n        analytics_db = RDS("Analytics DB")\n        data_lake = S3("Data Lake")\n    \n    with Cluster("Analytics Integration"):\n        athena = Athena("Athena")\n        quicksight = Glue("QuickSight")\n        sagemaker = Lambda("SageMaker")\n    \n    with Cluster("Orchestration & Monitoring"):\n        step_functions = StepFunctions("Step Functions")\n        cloudwatch = Cloudwatch("CloudWatch")\n        notifications = SNS("Notifications")\n        security = IAM("IAM Roles")\n    \n    # Data discovery flow\n    s3_raw >> crawlers\n    rds_db >> crawlers\n    mysql_db >> crawlers\n    dynamodb_table >> crawlers\n    \n    crawlers >> data_catalog\n    \n    # ETL processing\n    glue_service >> etl_jobs\n    etl_jobs >> spark_jobs\n    etl_jobs >> python_shell\n    etl_jobs >> streaming_jobs\n    etl_jobs >> ray_jobs\n    \n    # Data consumption from catalog\n    data_catalog >> etl_jobs\n    data_catalog >> athena\n    \n    # Data transformation flow\n    s3_raw >> spark_jobs >> s3_processed\n    rds_db >> python_shell >> redshift_dw\n    streaming_data >> streaming_jobs >> data_lake\n    \n    # Workflow orchestration\n    workflows >> etl_jobs\n    step_functions >> workflows\n    \n    # Analytics integration\n    s3_processed >> athena\n    redshift_dw >> quicksight\n    data_lake >> sagemaker\n    \n    # User interaction\n    users >> Edge(label="Develop Jobs") >> glue_service\n    users >> Edge(label="Monitor") >> cloudwatch\n    users >> Edge(label="Query Data") >> athena\n    \n    # Monitoring and security\n    etl_jobs >> cloudwatch\n    cloudwatch >> notifications\n    security >> glue_service\n'})}),"\n",(0,o.jsx)(e.h2,{id:"etl-job-development",children:"ETL Job Development"}),"\n",(0,o.jsx)(e.h3,{id:"1-spark-etl-job",children:"1. Spark ETL Job"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom awsglue.dynamicframe import DynamicFrame\nfrom pyspark.sql import functions as F\n\n# Initialize Glue context\nargs = getResolvedOptions(sys.argv, [\'JOB_NAME\'])\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\njob.init(args[\'JOB_NAME\'], args)\n\n# Read data from Data Catalog\ndatasource = glueContext.create_dynamic_frame.from_catalog(\n    database="raw_data_db",\n    table_name="customer_events",\n    transformation_ctx="datasource"\n)\n\n# Data transformations\n# 1. Filter out invalid records\nfiltered_data = Filter.apply(\n    frame=datasource,\n    f=lambda x: x["customer_id"] is not None and x["event_type"] is not None,\n    transformation_ctx="filtered_data"\n)\n\n# 2. Convert to DataFrame for complex transformations\ndf = filtered_data.toDF()\n\n# 3. Add derived columns\ndf_transformed = df.withColumn(\n    "event_date", \n    F.to_date(F.col("timestamp"))\n).withColumn(\n    "event_hour",\n    F.hour(F.col("timestamp"))\n).withColumn(\n    "is_weekend",\n    F.when(F.dayofweek(F.col("timestamp")).isin([1, 7]), True).otherwise(False)\n)\n\n# 4. Aggregate data\ndf_aggregated = df_transformed.groupBy(\n    "customer_id", "event_date", "event_type"\n).agg(\n    F.count("*").alias("event_count"),\n    F.sum("amount").alias("total_amount"),\n    F.avg("amount").alias("avg_amount"),\n    F.max("timestamp").alias("last_event_time")\n)\n\n# Convert back to DynamicFrame\ndynamic_frame_aggregated = DynamicFrame.fromDF(\n    df_aggregated, \n    glueContext, \n    "dynamic_frame_aggregated"\n)\n\n# Write to S3 in Parquet format with partitioning\nglueContext.write_dynamic_frame.from_options(\n    frame=dynamic_frame_aggregated,\n    connection_type="s3",\n    connection_options={\n        "path": "s3://processed-data/customer-daily-metrics/",\n        "partitionKeys": ["event_date"]\n    },\n    format="parquet",\n    transformation_ctx="write_to_s3"\n)\n\n# Update Data Catalog\nglueContext.write_dynamic_frame.from_catalog(\n    frame=dynamic_frame_aggregated,\n    database="processed_data_db",\n    table_name="customer_daily_metrics",\n    transformation_ctx="write_to_catalog"\n)\n\njob.commit()\n'})}),"\n",(0,o.jsx)(e.h3,{id:"2-python-shell-job",children:"2. Python Shell Job"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import boto3\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport logging\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef main():\n    # Initialize AWS clients\n    glue = boto3.client('glue')\n    s3 = boto3.client('s3')\n    \n    try:\n        # Read configuration from Glue job parameters\n        job_name = sys.argv[1] if len(sys.argv) > 1 else 'data-quality-check'\n        \n        # Get job parameters\n        response = glue.get_job(JobName=job_name)\n        job_params = response['Job']['DefaultArguments']\n        \n        source_bucket = job_params.get('--source-bucket')\n        source_prefix = job_params.get('--source-prefix')\n        target_bucket = job_params.get('--target-bucket')\n        \n        # Data quality checks\n        quality_results = perform_data_quality_checks(\n            s3, source_bucket, source_prefix\n        )\n        \n        # Generate quality report\n        report = generate_quality_report(quality_results)\n        \n        # Save report to S3\n        save_report_to_s3(s3, target_bucket, report)\n        \n        # Send notifications if issues found\n        if quality_results['has_issues']:\n            send_quality_alert(quality_results)\n        \n        logger.info(\"Data quality check completed successfully\")\n        \n    except Exception as e:\n        logger.error(f\"Job failed: {str(e)}\")\n        raise\n\ndef perform_data_quality_checks(s3_client, bucket, prefix):\n    \"\"\"Perform comprehensive data quality checks\"\"\"\n    \n    quality_results = {\n        'timestamp': datetime.utcnow().isoformat(),\n        'checks': [],\n        'has_issues': False\n    }\n    \n    # List files in S3 prefix\n    response = s3_client.list_objects_v2(\n        Bucket=bucket,\n        Prefix=prefix\n    )\n    \n    for obj in response.get('Contents', []):\n        file_key = obj['Key']\n        \n        if file_key.endswith('.parquet'):\n            # Read parquet file\n            df = pd.read_parquet(f's3://{bucket}/{file_key}')\n            \n            # Perform checks\n            file_checks = {\n                'file': file_key,\n                'row_count': len(df),\n                'null_checks': check_null_values(df),\n                'duplicate_checks': check_duplicates(df),\n                'data_type_checks': check_data_types(df),\n                'range_checks': check_value_ranges(df)\n            }\n            \n            # Check if any issues found\n            if any([\n                file_checks['null_checks']['has_issues'],\n                file_checks['duplicate_checks']['has_issues'],\n                file_checks['data_type_checks']['has_issues'],\n                file_checks['range_checks']['has_issues']\n            ]):\n                quality_results['has_issues'] = True\n            \n            quality_results['checks'].append(file_checks)\n    \n    return quality_results\n\ndef check_null_values(df):\n    \"\"\"Check for null values in critical columns\"\"\"\n    \n    critical_columns = ['customer_id', 'event_type', 'timestamp']\n    null_counts = {}\n    has_issues = False\n    \n    for col in critical_columns:\n        if col in df.columns:\n            null_count = df[col].isnull().sum()\n            null_counts[col] = null_count\n            \n            if null_count > 0:\n                has_issues = True\n    \n    return {\n        'has_issues': has_issues,\n        'null_counts': null_counts\n    }\n\ndef check_duplicates(df):\n    \"\"\"Check for duplicate records\"\"\"\n    \n    if 'event_id' in df.columns:\n        duplicate_count = df['event_id'].duplicated().sum()\n        return {\n            'has_issues': duplicate_count > 0,\n            'duplicate_count': duplicate_count\n        }\n    \n    return {'has_issues': False, 'duplicate_count': 0}\n\nif __name__ == \"__main__\":\n    main()\n"})}),"\n",(0,o.jsx)(e.h3,{id:"3-streaming-etl-job",children:"3. Streaming ETL Job"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'from awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom pyspark.sql import DataFrame, functions as F\nfrom pyspark.sql.types import *\nimport sys\n\n# Initialize\nargs = getResolvedOptions(sys.argv, [\'JOB_NAME\'])\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\njob.init(args[\'JOB_NAME\'], args)\n\n# Define schema for incoming data\nschema = StructType([\n    StructField("event_id", StringType(), True),\n    StructField("customer_id", StringType(), True),\n    StructField("event_type", StringType(), True),\n    StructField("timestamp", TimestampType(), True),\n    StructField("amount", DoubleType(), True),\n    StructField("properties", MapType(StringType(), StringType()), True)\n])\n\n# Read from Kinesis stream\nkinesis_options = {\n    "streamName": "customer-events-stream",\n    "startingPosition": "TRIM_HORIZON",\n    "inferSchema": "false"\n}\n\nstreaming_df = glueContext.create_data_frame.from_options(\n    connection_type="kinesis",\n    connection_options=kinesis_options,\n    transformation_ctx="streaming_df"\n)\n\n# Parse JSON data\nparsed_df = streaming_df.select(\n    F.from_json(F.col("data"), schema).alias("parsed_data")\n).select("parsed_data.*")\n\n# Real-time transformations\ndef process_batch(df, epoch_id):\n    """Process each micro-batch"""\n    \n    if df.count() > 0:\n        # Add processing timestamp\n        processed_df = df.withColumn(\n            "processing_time", \n            F.current_timestamp()\n        ).withColumn(\n            "event_date",\n            F.to_date(F.col("timestamp"))\n        )\n        \n        # Filter and validate\n        valid_df = processed_df.filter(\n            (F.col("customer_id").isNotNull()) &\n            (F.col("event_type").isNotNull()) &\n            (F.col("amount") >= 0)\n        )\n        \n        # Write to S3 (append mode for streaming)\n        valid_df.write \\\n            .mode("append") \\\n            .partitionBy("event_date") \\\n            .parquet("s3://streaming-output/processed-events/")\n        \n        # Write to DynamoDB for real-time lookups\n        write_to_dynamodb(valid_df)\n        \n        print(f"Processed {valid_df.count()} records in batch {epoch_id}")\n\ndef write_to_dynamodb(df):\n    """Write aggregated data to DynamoDB"""\n    \n    # Aggregate by customer and event type\n    aggregated = df.groupBy("customer_id", "event_type") \\\n        .agg(\n            F.count("*").alias("event_count"),\n            F.sum("amount").alias("total_amount"),\n            F.max("timestamp").alias("last_event_time")\n        )\n    \n    # Convert to format suitable for DynamoDB\n    dynamodb_df = aggregated.select(\n        F.col("customer_id").alias("pk"),\n        F.concat(F.lit("EVENT#"), F.col("event_type")).alias("sk"),\n        F.col("event_count"),\n        F.col("total_amount"),\n        F.col("last_event_time")\n    )\n    \n    # Write to DynamoDB using Glue DynamoDB connector\n    glueContext.write_dynamic_frame.from_options(\n        frame=DynamicFrame.fromDF(dynamodb_df, glueContext, "dynamodb_df"),\n        connection_type="dynamodb",\n        connection_options={\n            "dynamodb.output.tableName": "customer-event-aggregates",\n            "dynamodb.throughput.write.percent": "0.5"\n        }\n    )\n\n# Start streaming query\nquery = parsed_df.writeStream \\\n    .foreachBatch(process_batch) \\\n    .outputMode("append") \\\n    .option("checkpointLocation", "s3://glue-checkpoints/streaming-job/") \\\n    .trigger(processingTime=\'30 seconds\') \\\n    .start()\n\nquery.awaitTermination()\njob.commit()\n'})}),"\n",(0,o.jsx)(e.h2,{id:"workflow-orchestration",children:"Workflow Orchestration"}),"\n",(0,o.jsx)(e.h3,{id:"1-glue-workflow-definition",children:"1. Glue Workflow Definition"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import boto3\n\nglue = boto3.client('glue')\n\n# Create workflow\nworkflow_config = {\n    'Name': 'data-processing-workflow',\n    'Description': 'End-to-end data processing workflow',\n    'DefaultRunProperties': {\n        'glue:version': '3.0'\n    },\n    'MaxConcurrentRuns': 1\n}\n\nglue.create_workflow(**workflow_config)\n\n# Create triggers\ntriggers = [\n    {\n        'Name': 'start-crawlers-trigger',\n        'Type': 'ON_DEMAND',\n        'Actions': [\n            {\n                'CrawlerName': 'raw-data-crawler'\n            },\n            {\n                'CrawlerName': 'reference-data-crawler'\n            }\n        ],\n        'WorkflowName': 'data-processing-workflow'\n    },\n    {\n        'Name': 'etl-jobs-trigger',\n        'Type': 'CONDITIONAL',\n        'Predicate': {\n            'Conditions': [\n                {\n                    'LogicalOperator': 'EQUALS',\n                    'CrawlerName': 'raw-data-crawler',\n                    'CrawlState': 'SUCCEEDED'\n                },\n                {\n                    'LogicalOperator': 'EQUALS',\n                    'CrawlerName': 'reference-data-crawler',\n                    'CrawlState': 'SUCCEEDED'\n                }\n            ],\n            'Logical': 'AND'\n        },\n        'Actions': [\n            {\n                'JobName': 'data-cleaning-job'\n            }\n        ],\n        'WorkflowName': 'data-processing-workflow'\n    },\n    {\n        'Name': 'aggregation-trigger',\n        'Type': 'CONDITIONAL',\n        'Predicate': {\n            'Conditions': [\n                {\n                    'LogicalOperator': 'EQUALS',\n                    'JobName': 'data-cleaning-job',\n                    'State': 'SUCCEEDED'\n                }\n            ]\n        },\n        'Actions': [\n            {\n                'JobName': 'data-aggregation-job'\n            },\n            {\n                'JobName': 'data-quality-job'\n            }\n        ],\n        'WorkflowName': 'data-processing-workflow'\n    }\n]\n\nfor trigger in triggers:\n    glue.create_trigger(**trigger)\n"})}),"\n",(0,o.jsx)(e.h3,{id:"2-advanced-workflow-management",children:"2. Advanced Workflow Management"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"class GlueWorkflowManager:\n    def __init__(self, glue_client):\n        self.glue = glue_client\n    \n    def create_complex_workflow(self, workflow_config):\n        \"\"\"Create complex workflow with error handling\"\"\"\n        \n        # Create main workflow\n        self.glue.create_workflow(\n            Name=workflow_config['name'],\n            Description=workflow_config['description'],\n            DefaultRunProperties=workflow_config.get('properties', {})\n        )\n        \n        # Create jobs with error handling\n        for job_config in workflow_config['jobs']:\n            self.create_job_with_error_handling(job_config)\n        \n        # Create triggers with dependencies\n        for trigger_config in workflow_config['triggers']:\n            self.create_conditional_trigger(trigger_config)\n    \n    def create_job_with_error_handling(self, job_config):\n        \"\"\"Create job with comprehensive error handling\"\"\"\n        \n        # Main job\n        main_job = {\n            'Name': job_config['name'],\n            'Role': job_config['role'],\n            'Command': {\n                'Name': 'glueetl',\n                'ScriptLocation': job_config['script_location'],\n                'PythonVersion': '3'\n            },\n            'DefaultArguments': job_config.get('arguments', {}),\n            'MaxRetries': job_config.get('max_retries', 2),\n            'Timeout': job_config.get('timeout', 2880),\n            'NotificationProperty': {\n                'NotifyDelayAfter': 60\n            }\n        }\n        \n        self.glue.create_job(**main_job)\n        \n        # Error handling job\n        if job_config.get('error_handler'):\n            error_job = {\n                'Name': f\"{job_config['name']}-error-handler\",\n                'Role': job_config['role'],\n                'Command': {\n                    'Name': 'pythonshell',\n                    'ScriptLocation': job_config['error_handler']['script'],\n                    'PythonVersion': '3'\n                },\n                'DefaultArguments': {\n                    '--failed-job-name': job_config['name'],\n                    '--notification-topic': job_config['error_handler']['sns_topic']\n                }\n            }\n            \n            self.glue.create_job(**error_job)\n    \n    def monitor_workflow_execution(self, workflow_name, run_id):\n        \"\"\"Monitor workflow execution with detailed logging\"\"\"\n        \n        import time\n        \n        while True:\n            # Get workflow run status\n            response = self.glue.get_workflow_run(\n                Name=workflow_name,\n                RunId=run_id\n            )\n            \n            run_status = response['Run']['Status']\n            \n            if run_status in ['COMPLETED', 'STOPPED', 'ERROR']:\n                break\n            \n            # Get detailed status of jobs and crawlers\n            self.log_workflow_progress(workflow_name, run_id)\n            \n            time.sleep(30)\n        \n        return run_status\n    \n    def log_workflow_progress(self, workflow_name, run_id):\n        \"\"\"Log detailed workflow progress\"\"\"\n        \n        # Get workflow run details\n        response = self.glue.get_workflow_run(\n            Name=workflow_name,\n            RunId=run_id,\n            IncludeGraph=True\n        )\n        \n        graph = response['Run']['Graph']\n        \n        # Log job statuses\n        for node in graph.get('Nodes', []):\n            if node['Type'] == 'JOB':\n                job_runs = node.get('JobDetails', {}).get('JobRuns', [])\n                for job_run in job_runs:\n                    print(f\"Job {node['Name']}: {job_run['JobRunState']}\")\n            \n            elif node['Type'] == 'CRAWLER':\n                crawler_details = node.get('CrawlerDetails', {})\n                if crawler_details:\n                    print(f\"Crawler {node['Name']}: {crawler_details.get('LastCrawl', {}).get('Status', 'UNKNOWN')}\")\n"})}),"\n",(0,o.jsx)(e.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,o.jsx)(e.h3,{id:"1-job-performance-tuning",children:"1. Job Performance Tuning"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"# Optimized Glue job configuration\noptimized_job_config = {\n    'Name': 'optimized-etl-job',\n    'Role': 'GlueServiceRole',\n    'Command': {\n        'Name': 'glueetl',\n        'ScriptLocation': 's3://glue-scripts/optimized-etl.py',\n        'PythonVersion': '3'\n    },\n    'DefaultArguments': {\n        '--job-language': 'python',\n        '--job-bookmark-option': 'job-bookmark-enable',\n        '--enable-metrics': 'true',\n        '--enable-continuous-cloudwatch-log': 'true',\n        '--enable-spark-ui': 'true',\n        '--spark-event-logs-path': 's3://glue-logs/spark-events/',\n        \n        # Performance optimizations\n        '--conf': 'spark.sql.adaptive.enabled=true',\n        '--conf': 'spark.sql.adaptive.coalescePartitions.enabled=true',\n        '--conf': 'spark.sql.adaptive.skewJoin.enabled=true',\n        '--conf': 'spark.serializer=org.apache.spark.serializer.KryoSerializer',\n        \n        # Memory optimizations\n        '--conf': 'spark.sql.execution.arrow.pyspark.enabled=true',\n        '--conf': 'spark.sql.execution.arrow.maxRecordsPerBatch=10000'\n    },\n    'MaxCapacity': 10,  # Use appropriate DPU count\n    'WorkerType': 'G.1X',\n    'NumberOfWorkers': 10,\n    'Timeout': 2880,\n    'MaxRetries': 1\n}\n"})}),"\n",(0,o.jsx)(e.h3,{id:"2-cost-optimization",children:"2. Cost Optimization"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"class GlueCostOptimizer:\n    def __init__(self, glue_client, cloudwatch_client):\n        self.glue = glue_client\n        self.cloudwatch = cloudwatch_client\n    \n    def analyze_job_costs(self, job_name, days=30):\n        \"\"\"Analyze job costs over time\"\"\"\n        \n        from datetime import datetime, timedelta\n        \n        end_time = datetime.utcnow()\n        start_time = end_time - timedelta(days=days)\n        \n        # Get job runs\n        job_runs = self.glue.get_job_runs(\n            JobName=job_name,\n            MaxResults=100\n        )\n        \n        cost_analysis = {\n            'total_dpu_hours': 0,\n            'total_runs': len(job_runs['JobRuns']),\n            'avg_duration_minutes': 0,\n            'cost_breakdown': []\n        }\n        \n        total_duration = 0\n        \n        for run in job_runs['JobRuns']:\n            if run['JobRunState'] == 'SUCCEEDED':\n                start = run['StartedOn']\n                end = run.get('CompletedOn', datetime.utcnow())\n                duration_hours = (end - start).total_seconds() / 3600\n                \n                dpu_count = run.get('MaxCapacity', 2)\n                dpu_hours = duration_hours * dpu_count\n                \n                cost_analysis['total_dpu_hours'] += dpu_hours\n                total_duration += duration_hours * 60  # Convert to minutes\n                \n                cost_analysis['cost_breakdown'].append({\n                    'run_id': run['Id'],\n                    'duration_hours': duration_hours,\n                    'dpu_count': dpu_count,\n                    'dpu_hours': dpu_hours,\n                    'estimated_cost': dpu_hours * 0.44  # $0.44 per DPU-hour\n                })\n        \n        if cost_analysis['total_runs'] > 0:\n            cost_analysis['avg_duration_minutes'] = total_duration / cost_analysis['total_runs']\n        \n        return cost_analysis\n    \n    def recommend_optimizations(self, cost_analysis):\n        \"\"\"Recommend cost optimizations\"\"\"\n        \n        recommendations = []\n        \n        # Check for over-provisioning\n        if cost_analysis['avg_duration_minutes'] < 10:\n            recommendations.append({\n                'type': 'REDUCE_DPU',\n                'description': 'Job completes quickly, consider reducing DPU count',\n                'potential_savings': '20-40%'\n            })\n        \n        # Check for long-running jobs\n        if cost_analysis['avg_duration_minutes'] > 120:\n            recommendations.append({\n                'type': 'OPTIMIZE_CODE',\n                'description': 'Long-running job, optimize ETL logic and use job bookmarks',\n                'potential_savings': '30-50%'\n            })\n        \n        # Check frequency\n        if cost_analysis['total_runs'] > days * 24:  # More than hourly\n            recommendations.append({\n                'type': 'BATCH_PROCESSING',\n                'description': 'High frequency job, consider batching data',\n                'potential_savings': '25-35%'\n            })\n        \n        return recommendations\n"})}),"\n",(0,o.jsx)(e.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Job Design"}),": Design modular v\xe0 reusable ETL jobs"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Performance"}),": Optimize Spark configurations"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Cost Management"}),": Use appropriate DPU counts v\xe0 job bookmarks"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Error Handling"}),": Implement comprehensive error handling"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Monitoring"}),": Set up detailed monitoring v\xe0 alerting"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Security"}),": Use IAM roles v\xe0 encrypt data"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Testing"}),": Test jobs thoroughly before production"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Documentation"}),": Document job logic v\xe0 dependencies"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"t\xedch-h\u1ee3p-v\u1edbi-c\xe1c-d\u1ecbch-v\u1ee5-aws-kh\xe1c",children:"T\xedch h\u1ee3p v\u1edbi c\xe1c d\u1ecbch v\u1ee5 AWS kh\xe1c"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Amazon S3"}),": Primary data storage"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"AWS Glue Data Catalog"}),": Metadata management"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Amazon Athena"}),": Query processed data"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Amazon EMR"}),": Big data processing"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Amazon Redshift"}),": Data warehouse loading"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"AWS Lambda"}),": Event-driven processing"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Amazon Kinesis"}),": Real-time data streaming"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"AWS Step Functions"}),": Workflow orchestration"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Amazon CloudWatch"}),": Monitoring v\xe0 logging"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"AWS IAM"}),": Security v\xe0 access control"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Amazon SNS"}),": Notifications v\xe0 alerts"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"AWS Lake Formation"}),": Data lake governance"]}),"\n"]})]})}function u(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},53699:(n,e,t)=>{t.d(e,{A:()=>a});const a=t.p+"assets/images/glue-b4e2cbb004f9d7b59e5caa9f6165fce3.png"},65404:(n,e,t)=>{t.d(e,{R:()=>s,x:()=>i});var a=t(36672);const o={},r=a.createContext(o);function s(n){const e=a.useContext(r);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function i(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:s(n.components),a.createElement(r.Provider,{value:e},n.children)}}}]);