"use strict";(self.webpackChunkkeep_being_human_dev=self.webpackChunkkeep_being_human_dev||[]).push([[41725],{17398:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"aws-nodes/aws-analytics/kinesis-data-analytics","title":"Amazon Kinesis Data Analytics","description":"T\u1ed5ng quan","source":"@site/docs/aws-nodes/aws-analytics/19-kinesis-data-analytics.md","sourceDirName":"aws-nodes/aws-analytics","slug":"/aws-nodes/aws-analytics/kinesis-data-analytics","permalink":"/keep-being-human-dev/docs/aws-nodes/aws-analytics/kinesis-data-analytics","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/aws-nodes/aws-analytics/19-kinesis-data-analytics.md","tags":[],"version":"current","sidebarPosition":19,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"AWS Glue","permalink":"/keep-being-human-dev/docs/aws-nodes/aws-analytics/glue"},"next":{"title":"Amazon Kinesis Data Firehose","permalink":"/keep-being-human-dev/docs/aws-nodes/aws-analytics/kinesis-data-firehose"}}');var a=i(23420),s=i(65404);const r={},o="Amazon Kinesis Data Analytics",l={},c=[{value:"T\u1ed5ng quan",id:"t\u1ed5ng-quan",level:2},{value:"Ch\u1ee9c n\u0103ng ch\xednh",id:"ch\u1ee9c-n\u0103ng-ch\xednh",level:2},{value:"1. Real-time Stream Processing",id:"1-real-time-stream-processing",level:3},{value:"2. Multiple Runtime Environments",id:"2-multiple-runtime-environments",level:3},{value:"3. Data Sources Integration",id:"3-data-sources-integration",level:3},{value:"4. Output Destinations",id:"4-output-destinations",level:3},{value:"Use Cases ph\u1ed5 bi\u1ebfn",id:"use-cases-ph\u1ed5-bi\u1ebfn",level:2},{value:"Diagram Architecture",id:"diagram-architecture",level:2},{value:"Code \u0111\u1ec3 t\u1ea1o diagram:",id:"code-\u0111\u1ec3-t\u1ea1o-diagram",level:3},{value:"SQL-based Analytics",id:"sql-based-analytics",level:2},{value:"1. Stream Processing v\u1edbi SQL",id:"1-stream-processing-v\u1edbi-sql",level:3},{value:"2. Windowing Functions",id:"2-windowing-functions",level:3},{value:"3. Reference Data Joins",id:"3-reference-data-joins",level:3},{value:"Apache Flink Applications",id:"apache-flink-applications",level:2},{value:"1. Flink DataStream API",id:"1-flink-datastream-api",level:3},{value:"2. Complex Event Processing (CEP)",id:"2-complex-event-processing-cep",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"1. Application Scaling",id:"1-application-scaling",level:3},{value:"2. SQL Query Optimization",id:"2-sql-query-optimization",level:3},{value:"Monitoring v\xe0 Troubleshooting",id:"monitoring-v\xe0-troubleshooting",level:2},{value:"1. Application Metrics",id:"1-application-metrics",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"T\xedch h\u1ee3p v\u1edbi c\xe1c d\u1ecbch v\u1ee5 AWS kh\xe1c",id:"t\xedch-h\u1ee3p-v\u1edbi-c\xe1c-d\u1ecbch-v\u1ee5-aws-kh\xe1c",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"amazon-kinesis-data-analytics",children:"Amazon Kinesis Data Analytics"})}),"\n",(0,a.jsx)(e.h2,{id:"t\u1ed5ng-quan",children:"T\u1ed5ng quan"}),"\n",(0,a.jsx)(e.p,{children:"Amazon Kinesis Data Analytics l\xe0 node \u0111\u1ea1i di\u1ec7n cho d\u1ecbch v\u1ee5 ph\xe2n t\xedch d\u1eef li\u1ec7u streaming real-time c\u1ee7a AWS. D\u1ecbch v\u1ee5 n\xe0y cho ph\xe9p b\u1ea1n x\u1eed l\xfd v\xe0 ph\xe2n t\xedch streaming data b\u1eb1ng SQL ho\u1eb7c Apache Flink, cung c\u1ea5p kh\u1ea3 n\u0103ng t\u1ea1o insights t\u1eeb d\u1eef li\u1ec7u \u0111ang chuy\u1ec3n \u0111\u1ed9ng m\xe0 kh\xf4ng c\u1ea7n qu\u1ea3n l\xfd infrastructure ph\u1ee9c t\u1ea1p."}),"\n",(0,a.jsx)(e.h2,{id:"ch\u1ee9c-n\u0103ng-ch\xednh",children:"Ch\u1ee9c n\u0103ng ch\xednh"}),"\n",(0,a.jsx)(e.h3,{id:"1-real-time-stream-processing",children:"1. Real-time Stream Processing"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"SQL Queries"}),": X\u1eed l\xfd streaming data b\u1eb1ng SQL chu\u1ea9n"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Apache Flink"}),": Advanced stream processing v\u1edbi Flink applications"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Windowing Functions"}),": Time-based v\xe0 count-based windows"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Pattern Detection"}),": Ph\xe1t hi\u1ec7n patterns trong streaming data"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"2-multiple-runtime-environments",children:"2. Multiple Runtime Environments"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"SQL Runtime"}),": Serverless SQL-based analytics"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Apache Flink Runtime"}),": Java/Scala applications v\u1edbi Flink"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Apache Beam"}),": Portable data processing pipelines"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Managed Scaling"}),": T\u1ef1 \u0111\u1ed9ng scale based on throughput"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"3-data-sources-integration",children:"3. Data Sources Integration"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Kinesis Data Streams"}),": Direct integration v\u1edbi data streams"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Kinesis Data Firehose"}),": Processed data delivery"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Amazon MSK"}),": Apache Kafka integration"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Reference Data"}),": Static data joins t\u1eeb S3"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"4-output-destinations",children:"4. Output Destinations"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Real-time Dashboards"}),": Live analytics dashboards"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Data Lakes"}),": Processed data to S3"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Databases"}),": Results to RDS, DynamoDB"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Alerting Systems"}),": Notifications via SNS, Lambda"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"use-cases-ph\u1ed5-bi\u1ebfn",children:"Use Cases ph\u1ed5 bi\u1ebfn"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Real-time Fraud Detection"}),": Ph\xe1t hi\u1ec7n gian l\u1eadn t\u1ee9c th\u1eddi"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"IoT Analytics"}),": Ph\xe2n t\xedch d\u1eef li\u1ec7u IoT real-time"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Clickstream Analysis"}),": Ph\xe2n t\xedch h\xe0nh vi ng\u01b0\u1eddi d\xf9ng"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Log Analytics"}),": X\u1eed l\xfd v\xe0 ph\xe2n t\xedch logs real-time"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Financial Trading"}),": Real-time market data analysis"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"diagram-architecture",children:"Diagram Architecture"}),"\n",(0,a.jsx)(e.p,{children:"Ki\u1ebfn tr\xfac Amazon Kinesis Data Analytics v\u1edbi real-time processing:"}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.img,{alt:"Amazon Kinesis Data Analytics Architecture",src:i(33164).A+"",width:"2101",height:"1811"})}),"\n",(0,a.jsx)(e.h3,{id:"code-\u0111\u1ec3-t\u1ea1o-diagram",children:"Code \u0111\u1ec3 t\u1ea1o diagram:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.analytics import KinesisDataAnalytics, KinesisDataStreams, KinesisDataFirehose\nfrom diagrams.aws.storage import S3\nfrom diagrams.aws.compute import Lambda\nfrom diagrams.aws.database import RDS, Dynamodb\nfrom diagrams.aws.integration import SQS, SNS\nfrom diagrams.aws.management import Cloudwatch\nfrom diagrams.aws.security import IAM\nfrom diagrams.onprem.client import Users\nfrom diagrams.onprem.analytics import Flink\n\nwith Diagram("Amazon Kinesis Data Analytics Architecture", show=False, direction="TB"):\n    \n    users = Users("Business Users")\n    \n    with Cluster("Data Sources"):\n        web_apps = Lambda("Web Applications")\n        mobile_apps = Lambda("Mobile Apps")\n        iot_devices = Lambda("IoT Devices")\n        log_sources = Lambda("Log Sources")\n    \n    with Cluster("Streaming Ingestion"):\n        data_streams = KinesisDataStreams("Kinesis Data Streams")\n        firehose = KinesisDataFirehose("Kinesis Data Firehose")\n        kafka_source = SQS("Apache Kafka/MSK")\n    \n    with Cluster("Stream Processing"):\n        analytics_app = KinesisDataAnalytics("Kinesis Data Analytics")\n        \n        with Cluster("Processing Engines"):\n            sql_runtime = KinesisDataAnalytics("SQL Runtime")\n            flink_runtime = Flink("Flink Runtime")\n            beam_runtime = Lambda("Beam Runtime")\n    \n    with Cluster("Reference Data"):\n        s3_reference = S3("Reference Data")\n        lookup_tables = Dynamodb("Lookup Tables")\n    \n    with Cluster("Real-time Outputs"):\n        real_time_dashboard = Lambda("Real-time Dashboard")\n        alerts = SNS("Real-time Alerts")\n        lambda_functions = Lambda("Lambda Functions")\n        kinesis_output = KinesisDataStreams("Output Streams")\n    \n    with Cluster("Batch Outputs"):\n        s3_results = S3("Analytics Results")\n        data_warehouse = RDS("Data Warehouse")\n        firehose_output = KinesisDataFirehose("Delivery Streams")\n    \n    with Cluster("Monitoring & Management"):\n        cloudwatch = Cloudwatch("CloudWatch")\n        security = IAM("IAM Roles")\n    \n    # Data ingestion flow\n    web_apps >> Edge(label="Events") >> data_streams\n    mobile_apps >> Edge(label="Analytics") >> data_streams\n    iot_devices >> Edge(label="Sensor Data") >> data_streams\n    log_sources >> Edge(label="Logs") >> firehose\n    \n    # Stream processing input\n    data_streams >> Edge(label="Stream Data") >> analytics_app\n    firehose >> Edge(label="Buffered Data") >> analytics_app\n    kafka_source >> Edge(label="Kafka Topics") >> analytics_app\n    \n    # Processing engines\n    analytics_app >> sql_runtime\n    analytics_app >> flink_runtime\n    analytics_app >> beam_runtime\n    \n    # Reference data joins\n    s3_reference >> Edge(label="Static Data") >> analytics_app\n    lookup_tables >> Edge(label="Lookups") >> analytics_app\n    \n    # Real-time outputs\n    sql_runtime >> real_time_dashboard\n    flink_runtime >> alerts\n    beam_runtime >> lambda_functions\n    analytics_app >> kinesis_output\n    \n    # Batch outputs\n    analytics_app >> firehose_output >> s3_results\n    analytics_app >> Edge(label="Aggregated Results") >> data_warehouse\n    \n    # User interaction\n    users >> Edge(label="View Dashboards") >> real_time_dashboard\n    users >> Edge(label="Receive Alerts") >> alerts\n    \n    # Monitoring and security\n    analytics_app >> Edge(label="Metrics") >> cloudwatch\n    security >> Edge(label="Access Control") >> analytics_app\n'})}),"\n",(0,a.jsx)(e.h2,{id:"sql-based-analytics",children:"SQL-based Analytics"}),"\n",(0,a.jsx)(e.h3,{id:"1-stream-processing-v\u1edbi-sql",children:"1. Stream Processing v\u1edbi SQL"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-sql",children:"-- Real-time fraud detection\nCREATE STREAM fraud_detection_stream (\n    transaction_id VARCHAR(50),\n    user_id VARCHAR(50),\n    amount DECIMAL(10,2),\n    merchant_category VARCHAR(20),\n    location VARCHAR(100),\n    timestamp TIMESTAMP,\n    ROWTIME TIMESTAMP\n);\n\n-- Detect suspicious patterns\nCREATE STREAM suspicious_transactions AS\nSELECT \n    user_id,\n    COUNT(*) as transaction_count,\n    SUM(amount) as total_amount,\n    AVG(amount) as avg_amount,\n    ROWTIME_RANGE_START as window_start,\n    ROWTIME_RANGE_END as window_end\nFROM SOURCE_SQL_STREAM_001\nWHERE amount > 1000\nGROUP BY \n    user_id,\n    RANGE_INTERVAL '5' MINUTE;\n\n-- Alert on anomalies\nCREATE STREAM fraud_alerts AS\nSELECT \n    user_id,\n    transaction_count,\n    total_amount,\n    'HIGH_VELOCITY' as alert_type,\n    window_start\nFROM suspicious_transactions\nWHERE transaction_count > 10 \n   OR total_amount > 50000;\n"})}),"\n",(0,a.jsx)(e.h3,{id:"2-windowing-functions",children:"2. Windowing Functions"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-sql",children:"-- Tumbling window (non-overlapping)\nCREATE STREAM hourly_metrics AS\nSELECT \n    merchant_category,\n    COUNT(*) as transaction_count,\n    SUM(amount) as total_revenue,\n    AVG(amount) as avg_transaction_size,\n    ROWTIME_RANGE_START as hour_start\nFROM SOURCE_SQL_STREAM_001\nGROUP BY \n    merchant_category,\n    RANGE_INTERVAL '1' HOUR;\n\n-- Sliding window (overlapping)\nCREATE STREAM sliding_averages AS\nSELECT \n    user_id,\n    AVG(amount) as moving_avg_5min,\n    COUNT(*) as transaction_count_5min,\n    ROWTIME as event_time\nFROM SOURCE_SQL_STREAM_001\nGROUP BY \n    user_id,\n    RANGE_INTERVAL '5' MINUTE PRECEDING;\n\n-- Session window (gap-based)\nCREATE STREAM user_sessions AS\nSELECT \n    user_id,\n    COUNT(*) as actions_in_session,\n    MIN(ROWTIME) as session_start,\n    MAX(ROWTIME) as session_end,\n    (MAX(ROWTIME) - MIN(ROWTIME)) MINUTE as session_duration\nFROM SOURCE_SQL_STREAM_001\nGROUP BY \n    user_id,\n    SESSION_INTERVAL '30' MINUTE;\n"})}),"\n",(0,a.jsx)(e.h3,{id:"3-reference-data-joins",children:"3. Reference Data Joins"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-sql",children:"-- Join streaming data with reference data\nCREATE STREAM enriched_transactions AS\nSELECT \n    t.transaction_id,\n    t.user_id,\n    t.amount,\n    t.merchant_category,\n    u.user_tier,\n    u.risk_score,\n    u.registration_date,\n    CASE \n        WHEN u.user_tier = 'PREMIUM' AND t.amount > u.daily_limit THEN 'REVIEW'\n        WHEN u.risk_score > 0.8 THEN 'HIGH_RISK'\n        ELSE 'NORMAL'\n    END as transaction_status\nFROM SOURCE_SQL_STREAM_001 t\nLEFT JOIN \"USER_REFERENCE_DATA\" u\nON t.user_id = u.user_id;\n"})}),"\n",(0,a.jsx)(e.h2,{id:"apache-flink-applications",children:"Apache Flink Applications"}),"\n",(0,a.jsx)(e.h3,{id:"1-flink-datastream-api",children:"1. Flink DataStream API"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-java",children:'// Flink application for complex event processing\npublic class FraudDetectionApp {\n    \n    public static void main(String[] args) throws Exception {\n        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n        \n        // Configure checkpointing\n        env.enableCheckpointing(60000);\n        env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);\n        \n        // Read from Kinesis Data Streams\n        Properties kinesisProps = new Properties();\n        kinesisProps.setProperty(ConsumerConfigConstants.AWS_REGION, "us-west-2");\n        kinesisProps.setProperty(ConsumerConfigConstants.STREAM_INITIAL_POSITION, "LATEST");\n        \n        DataStream<Transaction> transactions = env\n            .addSource(new FlinkKinesisConsumer<>("transaction-stream", \n                                                 new TransactionDeserializer(), \n                                                 kinesisProps))\n            .assignTimestampsAndWatermarks(\n                WatermarkStrategy.<Transaction>forBoundedOutOfOrderness(Duration.ofSeconds(10))\n                    .withTimestampAssigner((transaction, timestamp) -> transaction.getTimestamp())\n            );\n        \n        // Fraud detection logic\n        DataStream<FraudAlert> alerts = transactions\n            .keyBy(Transaction::getUserId)\n            .process(new FraudDetectionFunction())\n            .filter(alert -> alert.getRiskScore() > 0.8);\n        \n        // Output to multiple sinks\n        alerts.addSink(new FlinkKinesisProducer<>("fraud-alerts-stream", \n                                                 new AlertSerializer(), \n                                                 kinesisProps));\n        \n        alerts.addSink(new DynamoDBSink<>("fraud-alerts-table"));\n        \n        env.execute("Real-time Fraud Detection");\n    }\n}\n\n// Custom process function for fraud detection\npublic class FraudDetectionFunction extends KeyedProcessFunction<String, Transaction, FraudAlert> {\n    \n    private ValueState<TransactionProfile> profileState;\n    private MapState<Long, Integer> hourlyCountState;\n    \n    @Override\n    public void open(Configuration parameters) {\n        profileState = getRuntimeContext().getState(\n            new ValueStateDescriptor<>("profile", TransactionProfile.class)\n        );\n        \n        hourlyCountState = getRuntimeContext().getMapState(\n            new MapStateDescriptor<>("hourly-count", Long.class, Integer.class)\n        );\n    }\n    \n    @Override\n    public void processElement(Transaction transaction, Context ctx, Collector<FraudAlert> out) \n            throws Exception {\n        \n        TransactionProfile profile = profileState.value();\n        if (profile == null) {\n            profile = new TransactionProfile();\n        }\n        \n        // Update profile with new transaction\n        profile.updateWith(transaction);\n        profileState.update(profile);\n        \n        // Check for velocity-based fraud\n        long hourWindow = transaction.getTimestamp() / (60 * 60 * 1000);\n        Integer hourlyCount = hourlyCountState.get(hourWindow);\n        hourlyCount = (hourlyCount == null) ? 1 : hourlyCount + 1;\n        hourlyCountState.put(hourWindow, hourlyCount);\n        \n        // Generate alert if suspicious\n        if (hourlyCount > 50 || transaction.getAmount() > profile.getAverageAmount() * 10) {\n            FraudAlert alert = new FraudAlert(\n                transaction.getUserId(),\n                transaction.getTransactionId(),\n                calculateRiskScore(transaction, profile, hourlyCount),\n                "VELOCITY_FRAUD",\n                System.currentTimeMillis()\n            );\n            out.collect(alert);\n        }\n        \n        // Clean up old hourly counts\n        cleanupOldCounts(ctx.timestamp());\n    }\n    \n    private double calculateRiskScore(Transaction transaction, TransactionProfile profile, int hourlyCount) {\n        double velocityScore = Math.min(hourlyCount / 50.0, 1.0);\n        double amountScore = Math.min(transaction.getAmount() / profile.getMaxAmount(), 1.0);\n        double locationScore = profile.isNewLocation(transaction.getLocation()) ? 0.5 : 0.0;\n        \n        return (velocityScore * 0.4) + (amountScore * 0.4) + (locationScore * 0.2);\n    }\n}\n'})}),"\n",(0,a.jsx)(e.h3,{id:"2-complex-event-processing-cep",children:"2. Complex Event Processing (CEP)"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-java",children:'// Pattern detection with Flink CEP\nimport org.apache.flink.cep.CEP;\nimport org.apache.flink.cep.PatternStream;\nimport org.apache.flink.cep.pattern.Pattern;\nimport org.apache.flink.cep.pattern.conditions.SimpleCondition;\n\npublic class PatternDetectionApp {\n    \n    public static void main(String[] args) throws Exception {\n        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n        \n        DataStream<LoginEvent> loginEvents = env\n            .addSource(new FlinkKinesisConsumer<>("login-events", \n                                                 new LoginEventDeserializer(), \n                                                 kinesisProps));\n        \n        // Define suspicious login pattern\n        Pattern<LoginEvent, ?> suspiciousPattern = Pattern.<LoginEvent>begin("first")\n            .where(new SimpleCondition<LoginEvent>() {\n                @Override\n                public boolean filter(LoginEvent event) {\n                    return event.getStatus().equals("FAILED");\n                }\n            })\n            .next("second")\n            .where(new SimpleCondition<LoginEvent>() {\n                @Override\n                public boolean filter(LoginEvent event) {\n                    return event.getStatus().equals("FAILED");\n                }\n            })\n            .next("third")\n            .where(new SimpleCondition<LoginEvent>() {\n                @Override\n                public boolean filter(LoginEvent event) {\n                    return event.getStatus().equals("SUCCESS");\n                }\n            })\n            .within(Time.minutes(5));\n        \n        // Apply pattern to stream\n        PatternStream<LoginEvent> patternStream = CEP.pattern(\n            loginEvents.keyBy(LoginEvent::getUserId),\n            suspiciousPattern\n        );\n        \n        // Extract matches and generate alerts\n        DataStream<SecurityAlert> alerts = patternStream.select(\n            (Map<String, List<LoginEvent>> pattern) -> {\n                List<LoginEvent> firstEvents = pattern.get("first");\n                List<LoginEvent> thirdEvents = pattern.get("third");\n                \n                return new SecurityAlert(\n                    firstEvents.get(0).getUserId(),\n                    "BRUTE_FORCE_ATTEMPT",\n                    "Multiple failed logins followed by success",\n                    System.currentTimeMillis()\n                );\n            }\n        );\n        \n        alerts.addSink(new SecurityAlertSink());\n        \n        env.execute("Login Pattern Detection");\n    }\n}\n'})}),"\n",(0,a.jsx)(e.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,a.jsx)(e.h3,{id:"1-application-scaling",children:"1. Application Scaling"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-java",children:'// Auto-scaling configuration\npublic class ScalingConfiguration {\n    \n    public static void configureAutoScaling(StreamExecutionEnvironment env) {\n        // Set parallelism based on throughput\n        env.setParallelism(4);\n        \n        // Configure memory\n        Configuration config = new Configuration();\n        config.setString("taskmanager.memory.process.size", "2g");\n        config.setString("taskmanager.memory.flink.size", "1.5g");\n        \n        // Optimize checkpointing\n        env.enableCheckpointing(30000); // 30 seconds\n        env.getCheckpointConfig().setMinPauseBetweenCheckpoints(10000);\n        env.getCheckpointConfig().setCheckpointTimeout(60000);\n        env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);\n        \n        // Configure state backend\n        env.setStateBackend(new RocksDBStateBackend("s3://checkpoints/flink/"));\n    }\n}\n'})}),"\n",(0,a.jsx)(e.h3,{id:"2-sql-query-optimization",children:"2. SQL Query Optimization"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-sql",children:"-- Optimized aggregation query\nCREATE STREAM optimized_metrics AS\nSELECT \n    merchant_id,\n    COUNT(*) as transaction_count,\n    SUM(amount) as total_amount,\n    -- Use approximate functions for better performance\n    APPROX_COUNT_DISTINCT(user_id) as unique_users,\n    ROWTIME_RANGE_START as window_start\nFROM SOURCE_SQL_STREAM_001\n-- Filter early to reduce processing\nWHERE amount > 0 AND merchant_id IS NOT NULL\nGROUP BY \n    merchant_id,\n    -- Use smaller windows for lower latency\n    RANGE_INTERVAL '1' MINUTE;\n\n-- Partitioned processing for high throughput\nCREATE STREAM partitioned_processing AS\nSELECT \n    partition_key,\n    event_type,\n    COUNT(*) as event_count,\n    AVG(processing_time) as avg_processing_time\nFROM (\n    SELECT \n        MOD(ABS(HASH_CODE(user_id)), 10) as partition_key,\n        event_type,\n        (ROWTIME - event_timestamp) SECOND as processing_time,\n        ROWTIME\n    FROM SOURCE_SQL_STREAM_001\n)\nGROUP BY \n    partition_key,\n    event_type,\n    RANGE_INTERVAL '30' SECOND;\n"})}),"\n",(0,a.jsx)(e.h2,{id:"monitoring-v\xe0-troubleshooting",children:"Monitoring v\xe0 Troubleshooting"}),"\n",(0,a.jsx)(e.h3,{id:"1-application-metrics",children:"1. Application Metrics"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import boto3\nimport json\n\nclass KinesisAnalyticsMonitor:\n    def __init__(self):\n        self.cloudwatch = boto3.client('cloudwatch')\n        self.kinesisanalytics = boto3.client('kinesisanalyticsv2')\n    \n    def get_application_metrics(self, application_name, start_time, end_time):\n        \"\"\"Get comprehensive application metrics\"\"\"\n        \n        metrics = [\n            'inputProcessing.DroppedRecords',\n            'inputProcessing.ProcessedRecords',\n            'inputProcessing.ProcessedBytes',\n            'outputDelivery.DeliveredRecords',\n            'outputDelivery.DeliveryDelay',\n            'kpu.utilization',\n            'downtime',\n            'uptime'\n        ]\n        \n        results = {}\n        \n        for metric in metrics:\n            response = self.cloudwatch.get_metric_statistics(\n                Namespace='AWS/KinesisAnalytics',\n                MetricName=metric,\n                Dimensions=[\n                    {\n                        'Name': 'Application',\n                        'Value': application_name\n                    }\n                ],\n                StartTime=start_time,\n                EndTime=end_time,\n                Period=300,\n                Statistics=['Average', 'Maximum', 'Sum']\n            )\n            \n            results[metric] = response['Datapoints']\n        \n        return results\n    \n    def check_application_health(self, application_name):\n        \"\"\"Check application health and performance\"\"\"\n        \n        response = self.kinesisanalytics.describe_application(\n            ApplicationName=application_name\n        )\n        \n        app_detail = response['ApplicationDetail']\n        \n        health_check = {\n            'application_name': application_name,\n            'status': app_detail['ApplicationStatus'],\n            'version': app_detail['ApplicationVersionId'],\n            'runtime_environment': app_detail['RuntimeEnvironment'],\n            'last_update': app_detail['LastUpdateTimestamp'].isoformat(),\n            'issues': []\n        }\n        \n        # Check for common issues\n        if app_detail['ApplicationStatus'] != 'RUNNING':\n            health_check['issues'].append(f\"Application not running: {app_detail['ApplicationStatus']}\")\n        \n        # Check KPU utilization\n        kpu_metrics = self.get_recent_kpu_utilization(application_name)\n        if kpu_metrics and kpu_metrics > 80:\n            health_check['issues'].append(f\"High KPU utilization: {kpu_metrics}%\")\n        \n        return health_check\n    \n    def create_custom_alarms(self, application_name):\n        \"\"\"Create CloudWatch alarms for the application\"\"\"\n        \n        alarms = [\n            {\n                'AlarmName': f'{application_name}-high-error-rate',\n                'MetricName': 'inputProcessing.DroppedRecords',\n                'Threshold': 100,\n                'ComparisonOperator': 'GreaterThanThreshold'\n            },\n            {\n                'AlarmName': f'{application_name}-high-kpu-utilization',\n                'MetricName': 'kpu.utilization',\n                'Threshold': 80,\n                'ComparisonOperator': 'GreaterThanThreshold'\n            },\n            {\n                'AlarmName': f'{application_name}-application-downtime',\n                'MetricName': 'downtime',\n                'Threshold': 0,\n                'ComparisonOperator': 'GreaterThanThreshold'\n            }\n        ]\n        \n        for alarm in alarms:\n            self.cloudwatch.put_metric_alarm(\n                AlarmName=alarm['AlarmName'],\n                ComparisonOperator=alarm['ComparisonOperator'],\n                EvaluationPeriods=2,\n                MetricName=alarm['MetricName'],\n                Namespace='AWS/KinesisAnalytics',\n                Period=300,\n                Statistic='Average',\n                Threshold=alarm['Threshold'],\n                ActionsEnabled=True,\n                AlarmActions=[\n                    'arn:aws:sns:region:account:kinesis-analytics-alerts'\n                ],\n                AlarmDescription=f'Alarm for {application_name}',\n                Dimensions=[\n                    {\n                        'Name': 'Application',\n                        'Value': application_name\n                    }\n                ]\n            )\n"})}),"\n",(0,a.jsx)(e.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Application Design"}),": Design cho fault tolerance v\xe0 scalability"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"State Management"}),": S\u1eed d\u1ee5ng appropriate state backends"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Checkpointing"}),": Configure proper checkpointing intervals"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Resource Sizing"}),": Right-size KPUs based on throughput"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Error Handling"}),": Implement comprehensive error handling"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Monitoring"}),": Set up detailed monitoring v\xe0 alerting"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Testing"}),": Thorough testing v\u1edbi realistic data volumes"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Security"}),": Implement proper IAM roles v\xe0 VPC configuration"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"t\xedch-h\u1ee3p-v\u1edbi-c\xe1c-d\u1ecbch-v\u1ee5-aws-kh\xe1c",children:"T\xedch h\u1ee3p v\u1edbi c\xe1c d\u1ecbch v\u1ee5 AWS kh\xe1c"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Amazon Kinesis Data Streams"}),": Primary data source"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Amazon Kinesis Data Firehose"}),": Data delivery"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Amazon S3"}),": Reference data v\xe0 output storage"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Amazon DynamoDB"}),": Real-time lookups v\xe0 results"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"AWS Lambda"}),": Event-driven processing"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Amazon SNS"}),": Real-time notifications"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Amazon CloudWatch"}),": Monitoring v\xe0 alerting"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"AWS IAM"}),": Security v\xe0 access control"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Amazon VPC"}),": Network isolation"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Amazon MSK"}),": Apache Kafka integration"]}),"\n"]})]})}function u(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(d,{...n})}):d(n)}},33164:(n,e,i)=>{i.d(e,{A:()=>t});const t=i.p+"assets/images/kinesis-data-analytics-cf4c2e9dce83774989c74355909a5671.png"},65404:(n,e,i)=>{i.d(e,{R:()=>r,x:()=>o});var t=i(36672);const a={},s=t.createContext(a);function r(n){const e=t.useContext(s);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:r(n.components),t.createElement(s.Provider,{value:e},n.children)}}}]);