"use strict";(self.webpackChunkkeep_being_human_dev=self.webpackChunkkeep_being_human_dev||[]).push([[35958],{63099:(e,n,r)=>{r.d(n,{A:()=>a});const a=r.p+"assets/images/emr-engine-be4415ba0491af50c580836e92b3fccf.png"},65404:(e,n,r)=>{r.d(n,{R:()=>t,x:()=>o});var a=r(36672);const s={},i=a.createContext(s);function t(e){const n=a.useContext(i);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),a.createElement(i.Provider,{value:n},e.children)}},98790:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>g,frontMatter:()=>t,metadata:()=>a,toc:()=>l});const a=JSON.parse('{"id":"diagrams/aws-nodes/aws-analytics/emr-engine","title":"EMR Engine","description":"T\u1ed5ng quan","source":"@site/docs/diagrams/aws-nodes/aws-analytics/13-emr-engine.md","sourceDirName":"diagrams/aws-nodes/aws-analytics","slug":"/diagrams/aws-nodes/aws-analytics/emr-engine","permalink":"/keep-being-human-dev/docs/diagrams/aws-nodes/aws-analytics/emr-engine","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/diagrams/aws-nodes/aws-analytics/13-emr-engine.md","tags":[],"version":"current","sidebarPosition":13,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"EMR Engine MapR M7","permalink":"/keep-being-human-dev/docs/diagrams/aws-nodes/aws-analytics/emr-engine-mapr-m7"},"next":{"title":"EMR HDFS Cluster","permalink":"/keep-being-human-dev/docs/diagrams/aws-nodes/aws-analytics/emr-hdfs-cluster"}}');var s=r(23420),i=r(65404);const t={},o="EMR Engine",c={},l=[{value:"T\u1ed5ng quan",id:"t\u1ed5ng-quan",level:2},{value:"Ch\u1ee9c n\u0103ng ch\xednh",id:"ch\u1ee9c-n\u0103ng-ch\xednh",level:2},{value:"1. Multi-Engine Support",id:"1-multi-engine-support",level:3},{value:"2. Processing Modes",id:"2-processing-modes",level:3},{value:"3. Resource Management",id:"3-resource-management",level:3},{value:"4. Storage Integration",id:"4-storage-integration",level:3},{value:"Use Cases ph\u1ed5 bi\u1ebfn",id:"use-cases-ph\u1ed5-bi\u1ebfn",level:2},{value:"Diagram Architecture",id:"diagram-architecture",level:2},{value:"Code \u0111\u1ec3 t\u1ea1o diagram:",id:"code-\u0111\u1ec3-t\u1ea1o-diagram",level:3},{value:"Processing Engines",id:"processing-engines",level:2},{value:"1. Apache Spark",id:"1-apache-spark",level:3},{value:"2. Apache Flink",id:"2-apache-flink",level:3},{value:"3. Presto",id:"3-presto",level:3},{value:"4. Apache Hive",id:"4-apache-hive",level:3},{value:"Resource Management",id:"resource-management",level:2},{value:"1. YARN Configuration",id:"1-yarn-configuration",level:3},{value:"2. Kubernetes Integration",id:"2-kubernetes-integration",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"1. Spark Tuning",id:"1-spark-tuning",level:3},{value:"2. Flink Tuning",id:"2-flink-tuning",level:3},{value:"Monitoring v\xe0 Debugging",id:"monitoring-v\xe0-debugging",level:2},{value:"1. Spark Monitoring",id:"1-spark-monitoring",level:3},{value:"2. Application Logging",id:"2-application-logging",level:3},{value:"Security Best Practices",id:"security-best-practices",level:2},{value:"1. Data Encryption",id:"1-data-encryption",level:3},{value:"2. Access Control",id:"2-access-control",level:3},{value:"Cost Optimization",id:"cost-optimization",level:2},{value:"1. Spot Instance Strategy",id:"1-spot-instance-strategy",level:3},{value:"2. Auto Termination",id:"2-auto-termination",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"T\xedch h\u1ee3p v\u1edbi c\xe1c d\u1ecbch v\u1ee5 AWS kh\xe1c",id:"t\xedch-h\u1ee3p-v\u1edbi-c\xe1c-d\u1ecbch-v\u1ee5-aws-kh\xe1c",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"emr-engine",children:"EMR Engine"})}),"\n",(0,s.jsx)(n.h2,{id:"t\u1ed5ng-quan",children:"T\u1ed5ng quan"}),"\n",(0,s.jsx)(n.p,{children:"EMR Engine l\xe0 node t\u1ed5ng qu\xe1t \u0111\u1ea1i di\u1ec7n cho c\xe1c processing engines trong Amazon EMR ecosystem. Node n\xe0y bi\u1ec3u th\u1ecb c\xe1c c\xf4ng c\u1ee5 x\u1eed l\xfd d\u1eef li\u1ec7u m\u1ea1nh m\u1ebd nh\u01b0 Apache Spark, Apache Hadoop, Apache Flink, Presto, v\xe0 nhi\u1ec1u framework kh\xe1c ch\u1ea1y tr\xean EMR cluster. EMR Engine cung c\u1ea5p kh\u1ea3 n\u0103ng x\u1eed l\xfd d\u1eef li\u1ec7u linh ho\u1ea1t t\u1eeb batch processing \u0111\u1ebfn real-time analytics."}),"\n",(0,s.jsx)(n.h2,{id:"ch\u1ee9c-n\u0103ng-ch\xednh",children:"Ch\u1ee9c n\u0103ng ch\xednh"}),"\n",(0,s.jsx)(n.h3,{id:"1-multi-engine-support",children:"1. Multi-Engine Support"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Apache Spark"}),": In-memory processing cho big data"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Apache Hadoop"}),": Distributed computing framework"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Apache Flink"}),": Stream processing engine"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Presto"}),": Distributed SQL query engine"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Apache Hive"}),": Data warehouse software"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Apache HBase"}),": NoSQL database"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"2-processing-modes",children:"2. Processing Modes"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Batch Processing"}),": Large-scale data processing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Stream Processing"}),": Real-time data processing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Interactive Analytics"}),": Ad-hoc queries v\xe0 exploration"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Machine Learning"}),": ML model training v\xe0 inference"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"3-resource-management",children:"3. Resource Management"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"YARN"}),": Resource negotiation v\xe0 scheduling"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Kubernetes"}),": Container orchestration"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Mesos"}),": Distributed systems kernel"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Standalone"}),": Framework-specific resource management"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"4-storage-integration",children:"4. Storage Integration"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"HDFS"}),": Hadoop Distributed File System"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"S3"}),": Amazon Simple Storage Service"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"EBS"}),": Elastic Block Store"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Local Storage"}),": Instance store volumes"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"use-cases-ph\u1ed5-bi\u1ebfn",children:"Use Cases ph\u1ed5 bi\u1ebfn"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ETL Pipelines"}),": Data transformation workflows"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Data Lake Analytics"}),": Large-scale data analysis"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-time Processing"}),": Streaming data processing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Machine Learning"}),": ML model development"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Business Intelligence"}),": Interactive analytics"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"diagram-architecture",children:"Diagram Architecture"}),"\n",(0,s.jsx)(n.p,{children:"Ki\u1ebfn tr\xfac EMR Engine v\u1edbi multiple processing frameworks:"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"EMR Engine Architecture",src:r(63099).A+"",width:"2480",height:"1531"})}),"\n",(0,s.jsx)(n.h3,{id:"code-\u0111\u1ec3-t\u1ea1o-diagram",children:"Code \u0111\u1ec3 t\u1ea1o diagram:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.analytics import EMREngine, Kinesis, Glue\nfrom diagrams.aws.storage import S3\nfrom diagrams.aws.compute import EC2\nfrom diagrams.aws.database import RDS, Dynamodb\nfrom diagrams.aws.ml import SagemakerModel\nfrom diagrams.aws.management import Cloudwatch\nfrom diagrams.aws.security import IAM\nfrom diagrams.onprem.client import Users\nfrom diagrams.onprem.analytics import Spark\n\nwith Diagram("EMR Engine Architecture", show=False, direction="TB"):\n    \n    users = Users("Data Engineers")\n    \n    with Cluster("Data Sources"):\n        s3_data = S3("Data Lake")\n        rds_db = RDS("Transactional DB")\n        kinesis_stream = Kinesis("Streaming Data")\n        external_data = EC2("External APIs")\n    \n    with Cluster("EMR Cluster"):\n        emr_engine = EMREngine("EMR Processing\\nEngines")\n        \n        with Cluster("Processing Frameworks"):\n            spark_engine = Spark("Apache Spark")\n            hadoop_engine = EC2("Hadoop MapReduce")\n            flink_engine = EC2("Apache Flink")\n            presto_engine = EC2("Presto")\n            hive_engine = EC2("Apache Hive")\n    \n    with Cluster("Resource Management"):\n        yarn_rm = EC2("YARN ResourceManager")\n        yarn_nm = [EC2("NodeManager 1"), EC2("NodeManager 2")]\n        k8s_master = EC2("Kubernetes Master")\n    \n    with Cluster("Storage Layer"):\n        hdfs_storage = S3("HDFS Storage")\n        s3_storage = S3("S3 Storage")\n        local_storage = EC2("Local Storage")\n    \n    with Cluster("Output & Analytics"):\n        processed_data = S3("Processed Data")\n        ml_models = SagemakerModel("ML Models")\n        data_catalog = Glue("Data Catalog")\n        analytics_api = EC2("Analytics API")\n    \n    with Cluster("Monitoring & Management"):\n        monitoring = Cloudwatch("CloudWatch")\n        security = IAM("Security")\n    \n    # Data ingestion\n    s3_data >> Edge(label="Batch Data") >> emr_engine\n    rds_db >> Edge(label="CDC") >> emr_engine\n    kinesis_stream >> Edge(label="Streaming") >> emr_engine\n    external_data >> Edge(label="API Data") >> emr_engine\n    \n    # Processing engines\n    emr_engine >> spark_engine\n    emr_engine >> hadoop_engine\n    emr_engine >> flink_engine\n    emr_engine >> presto_engine\n    emr_engine >> hive_engine\n    \n    # Resource management\n    emr_engine >> yarn_rm\n    yarn_rm >> yarn_nm\n    emr_engine >> k8s_master\n    \n    # Storage integration\n    spark_engine >> hdfs_storage\n    hadoop_engine >> s3_storage\n    flink_engine >> local_storage\n    \n    # Output generation\n    spark_engine >> processed_data\n    flink_engine >> analytics_api\n    hive_engine >> data_catalog\n    spark_engine >> ml_models\n    \n    # User interaction\n    users >> Edge(label="Submit Jobs") >> emr_engine\n    users >> Edge(label="Query Data") >> presto_engine\n    users >> Edge(label="Stream Processing") >> flink_engine\n    \n    # Monitoring\n    emr_engine >> monitoring\n    security >> emr_engine\n'})}),"\n",(0,s.jsx)(n.h2,{id:"processing-engines",children:"Processing Engines"}),"\n",(0,s.jsx)(n.h3,{id:"1-apache-spark",children:"1. Apache Spark"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Spark application example\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\n\nspark = SparkSession.builder \\\n    .appName("EMR-Spark-Processing") \\\n    .config("spark.sql.adaptive.enabled", "true") \\\n    .getOrCreate()\n\n# Read data from S3\ndf = spark.read.parquet("s3://data-lake/raw/events/")\n\n# Data transformation\nprocessed_df = df \\\n    .filter(col("event_type") == "purchase") \\\n    .groupBy("user_id", "product_category") \\\n    .agg(\n        sum("amount").alias("total_spent"),\n        count("*").alias("purchase_count")\n    ) \\\n    .orderBy(desc("total_spent"))\n\n# Write results back to S3\nprocessed_df.write \\\n    .mode("overwrite") \\\n    .parquet("s3://data-lake/processed/user_purchases/")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"2-apache-flink",children:"2. Apache Flink"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:'// Flink streaming application\nStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\n// Configure checkpointing\nenv.enableCheckpointing(60000);\nenv.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);\n\n// Data source\nDataStream<Event> events = env\n    .addSource(new KinesisSource<>(kinesisConfig))\n    .map(new EventDeserializer());\n\n// Stream processing\nDataStream<ProcessedEvent> processed = events\n    .keyBy(Event::getUserId)\n    .window(TumblingProcessingTimeWindows.of(Time.minutes(5)))\n    .aggregate(new EventAggregator());\n\n// Sink to S3\nprocessed.addSink(new S3Sink<>(s3Config));\n\nenv.execute("Real-time Event Processing");\n'})}),"\n",(0,s.jsx)(n.h3,{id:"3-presto",children:"3. Presto"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"-- Presto query example\nWITH user_metrics AS (\n  SELECT \n    user_id,\n    COUNT(*) as total_events,\n    SUM(CASE WHEN event_type = 'purchase' THEN amount ELSE 0 END) as total_spent,\n    AVG(session_duration) as avg_session_duration\n  FROM events \n  WHERE date_partition >= '2023-01-01'\n  GROUP BY user_id\n),\nuser_segments AS (\n  SELECT \n    user_id,\n    total_spent,\n    CASE \n      WHEN total_spent > 1000 THEN 'high_value'\n      WHEN total_spent > 100 THEN 'medium_value'\n      ELSE 'low_value'\n    END as user_segment\n  FROM user_metrics\n)\nSELECT \n  user_segment,\n  COUNT(*) as user_count,\n  AVG(total_spent) as avg_spent_per_user\nFROM user_segments\nGROUP BY user_segment\nORDER BY avg_spent_per_user DESC;\n"})}),"\n",(0,s.jsx)(n.h3,{id:"4-apache-hive",children:"4. Apache Hive"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"-- Hive data processing\nCREATE EXTERNAL TABLE IF NOT EXISTS raw_events (\n  event_id STRING,\n  user_id STRING,\n  event_type STRING,\n  timestamp BIGINT,\n  properties MAP<STRING, STRING>\n)\nPARTITIONED BY (date_partition STRING)\nSTORED AS PARQUET\nLOCATION 's3://data-lake/raw/events/';\n\n-- Add partitions\nMSCK REPAIR TABLE raw_events;\n\n-- Create processed table\nCREATE TABLE user_daily_summary\nSTORED AS PARQUET\nAS\nSELECT \n  user_id,\n  date_partition,\n  COUNT(*) as event_count,\n  COUNT(DISTINCT event_type) as unique_event_types,\n  SUM(CASE WHEN event_type = 'purchase' THEN 1 ELSE 0 END) as purchase_count\nFROM raw_events\nWHERE date_partition >= '2023-01-01'\nGROUP BY user_id, date_partition;\n"})}),"\n",(0,s.jsx)(n.h2,{id:"resource-management",children:"Resource Management"}),"\n",(0,s.jsx)(n.h3,{id:"1-yarn-configuration",children:"1. YARN Configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:"\x3c!-- yarn-site.xml --\x3e\n<configuration>\n  <property>\n    <name>yarn.resourcemanager.hostname</name>\n    <value>master-node</value>\n  </property>\n  \n  <property>\n    <name>yarn.scheduler.maximum-allocation-mb</name>\n    <value>14336</value>\n  </property>\n  \n  <property>\n    <name>yarn.scheduler.maximum-allocation-vcores</name>\n    <value>8</value>\n  </property>\n  \n  <property>\n    <name>yarn.nodemanager.resource.memory-mb</name>\n    <value>14336</value>\n  </property>\n  \n  <property>\n    <name>yarn.nodemanager.resource.cpu-vcores</name>\n    <value>8</value>\n  </property>\n</configuration>\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-kubernetes-integration",children:"2. Kubernetes Integration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'# Spark on Kubernetes\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: spark-config\ndata:\n  spark-defaults.conf: |\n    spark.kubernetes.container.image=spark:3.3.0\n    spark.kubernetes.authenticate.driver.serviceAccountName=spark\n    spark.kubernetes.executor.instances=10\n    spark.kubernetes.executor.request.cores=2\n    spark.kubernetes.executor.limit.cores=4\n    spark.kubernetes.executor.request.memory=4g\n    spark.kubernetes.executor.limit.memory=8g\n\n---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: spark-job\nspec:\n  template:\n    spec:\n      containers:\n      - name: spark-driver\n        image: spark:3.3.0\n        command: ["/opt/spark/bin/spark-submit"]\n        args:\n        - "--master"\n        - "k8s://https://kubernetes.default.svc:443"\n        - "--deploy-mode"\n        - "client"\n        - "s3://spark-jobs/data-processing.py"\n'})}),"\n",(0,s.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,s.jsx)(n.h3,{id:"1-spark-tuning",children:"1. Spark Tuning"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Spark configuration for performance\nspark_config = {\n    # Memory management\n    "spark.executor.memory": "8g",\n    "spark.executor.memoryFraction": "0.8",\n    "spark.executor.memoryStorageFraction": "0.2",\n    \n    # CPU optimization\n    "spark.executor.cores": "4",\n    "spark.executor.instances": "20",\n    "spark.default.parallelism": "160",\n    \n    # Shuffle optimization\n    "spark.sql.shuffle.partitions": "400",\n    "spark.shuffle.compress": "true",\n    "spark.shuffle.spill.compress": "true",\n    \n    # Serialization\n    "spark.serializer": "org.apache.spark.serializer.KryoSerializer",\n    "spark.kryo.registrationRequired": "false",\n    \n    # Adaptive query execution\n    "spark.sql.adaptive.enabled": "true",\n    "spark.sql.adaptive.coalescePartitions.enabled": "true",\n    "spark.sql.adaptive.skewJoin.enabled": "true"\n}\n\nspark = SparkSession.builder \\\n    .appName("Optimized-Spark-Job") \\\n    .config(conf=SparkConf().setAll(spark_config.items())) \\\n    .getOrCreate()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"2-flink-tuning",children:"2. Flink Tuning"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:'// Flink performance configuration\nConfiguration config = new Configuration();\n\n// Memory configuration\nconfig.setString("taskmanager.memory.process.size", "8g");\nconfig.setString("taskmanager.memory.flink.size", "6g");\nconfig.setString("taskmanager.memory.managed.fraction", "0.4");\n\n// Parallelism\nconfig.setInteger("parallelism.default", 16);\nconfig.setInteger("taskmanager.numberOfTaskSlots", 4);\n\n// Checkpointing\nconfig.setLong("execution.checkpointing.interval", 60000);\nconfig.setString("execution.checkpointing.mode", "EXACTLY_ONCE");\nconfig.setLong("execution.checkpointing.timeout", 600000);\n\n// Network\nconfig.setString("taskmanager.network.memory.fraction", "0.1");\nconfig.setInteger("taskmanager.network.numberOfBuffers", 8192);\n\nStreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(config);\n'})}),"\n",(0,s.jsx)(n.h2,{id:"monitoring-v\xe0-debugging",children:"Monitoring v\xe0 Debugging"}),"\n",(0,s.jsx)(n.h3,{id:"1-spark-monitoring",children:"1. Spark Monitoring"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Custom Spark metrics\nfrom pyspark.util import AccumulatorParam\n\nclass ListAccumulatorParam(AccumulatorParam):\n    def zero(self, value):\n        return []\n    \n    def addInPlace(self, list1, list2):\n        return list1 + list2\n\n# Create custom accumulator\nerror_accumulator = spark.sparkContext.accumulator([], ListAccumulatorParam())\n\ndef process_record(record):\n    try:\n        # Process record\n        return transform_record(record)\n    except Exception as e:\n        error_accumulator.add([str(e)])\n        return None\n\n# Use in RDD operations\nprocessed_rdd = input_rdd.map(process_record).filter(lambda x: x is not None)\n\n# Check errors after action\nprocessed_rdd.count()\nprint(f"Errors encountered: {error_accumulator.value}")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"2-application-logging",children:"2. Application Logging"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Structured logging for EMR applications\nimport logging\nimport json\nfrom datetime import datetime\n\nclass EMRLogger:\n    def __init__(self, app_name):\n        self.logger = logging.getLogger(app_name)\n        self.logger.setLevel(logging.INFO)\n        \n        handler = logging.StreamHandler()\n        formatter = logging.Formatter(\n            \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\'\n        )\n        handler.setFormatter(formatter)\n        self.logger.addHandler(handler)\n    \n    def log_job_start(self, job_id, job_type, input_path):\n        self.logger.info(json.dumps({\n            "event": "job_start",\n            "job_id": job_id,\n            "job_type": job_type,\n            "input_path": input_path,\n            "timestamp": datetime.utcnow().isoformat()\n        }))\n    \n    def log_job_complete(self, job_id, records_processed, duration):\n        self.logger.info(json.dumps({\n            "event": "job_complete",\n            "job_id": job_id,\n            "records_processed": records_processed,\n            "duration_seconds": duration,\n            "timestamp": datetime.utcnow().isoformat()\n        }))\n\n# Usage\nlogger = EMRLogger("data-processing-job")\nlogger.log_job_start("job-123", "etl", "s3://data/input/")\n'})}),"\n",(0,s.jsx)(n.h2,{id:"security-best-practices",children:"Security Best Practices"}),"\n",(0,s.jsx)(n.h3,{id:"1-data-encryption",children:"1. Data Encryption"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Spark with encryption\nspark = SparkSession.builder \\\n    .appName("Secure-Spark-Job") \\\n    .config("spark.sql.execution.arrow.pyspark.enabled", "true") \\\n    .config("spark.hadoop.fs.s3a.server-side-encryption-algorithm", "AES256") \\\n    .config("spark.hadoop.fs.s3a.server-side-encryption.key", "arn:aws:kms:region:account:key/key-id") \\\n    .config("spark.authenticate", "true") \\\n    .config("spark.network.crypto.enabled", "true") \\\n    .config("spark.io.encryption.enabled", "true") \\\n    .getOrCreate()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"2-access-control",children:"2. Access Control"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Kerberos authentication\nkinit -kt /etc/security/keytabs/spark.keytab spark/hostname@REALM\n\n# HDFS permissions\nhdfs dfsadmin -setDefaultAcl /data/sensitive user:analyst:r--,group:analytics:r--,other::---\n\n# Hive authorization\nSET hive.security.authorization.enabled=true;\nSET hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory;\n"})}),"\n",(0,s.jsx)(n.h2,{id:"cost-optimization",children:"Cost Optimization"}),"\n",(0,s.jsx)(n.h3,{id:"1-spot-instance-strategy",children:"1. Spot Instance Strategy"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# EMR cluster with Spot instances\nimport boto3\n\nemr = boto3.client('emr')\n\ncluster_config = {\n    'Name': 'cost-optimized-cluster',\n    'ReleaseLabel': 'emr-6.9.0',\n    'Instances': {\n        'MasterInstanceType': 'm5.xlarge',\n        'SlaveInstanceType': 'm5.xlarge',\n        'InstanceCount': 10,\n        'Ec2KeyName': 'my-key',\n        'InstanceFleets': [\n            {\n                'Name': 'Master',\n                'InstanceFleetType': 'MASTER',\n                'TargetOnDemandCapacity': 1,\n                'InstanceTypeConfigs': [\n                    {\n                        'InstanceType': 'm5.xlarge',\n                        'WeightedCapacity': 1\n                    }\n                ]\n            },\n            {\n                'Name': 'Core',\n                'InstanceFleetType': 'CORE',\n                'TargetOnDemandCapacity': 2,\n                'TargetSpotCapacity': 8,\n                'InstanceTypeConfigs': [\n                    {\n                        'InstanceType': 'm5.xlarge',\n                        'WeightedCapacity': 1,\n                        'BidPrice': '0.10'\n                    },\n                    {\n                        'InstanceType': 'm5.2xlarge',\n                        'WeightedCapacity': 2,\n                        'BidPrice': '0.20'\n                    }\n                ]\n            }\n        ]\n    }\n}\n\nresponse = emr.run_job_flow(**cluster_config)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-auto-termination",children:"2. Auto Termination"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Auto-termination configuration\nauto_termination_config = {\n    'IdleTimeout': 3600,  # 1 hour\n    'MaxIdleTimeBeforeTermination': 7200  # 2 hours\n}\n\n# Step configuration with auto-termination\nstep_config = {\n    'Name': 'Data Processing Step',\n    'ActionOnFailure': 'TERMINATE_CLUSTER',\n    'HadoopJarStep': {\n        'Jar': 'command-runner.jar',\n        'Args': [\n            'spark-submit',\n            '--deploy-mode', 'cluster',\n            's3://my-bucket/spark-job.py'\n        ]\n    }\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Engine Selection"}),": Ch\u1ecdn engine ph\xf9 h\u1ee3p cho workload"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Resource Tuning"}),": Optimize memory v\xe0 CPU allocation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Data Partitioning"}),": Partition data hi\u1ec7u qu\u1ea3"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Caching Strategy"}),": Cache intermediate results"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Error Handling"}),": Implement robust error handling"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Monitoring"}),": Comprehensive monitoring setup"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Security"}),": Enable encryption v\xe0 access controls"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cost Management"}),": Use Spot instances v\xe0 auto-termination"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"t\xedch-h\u1ee3p-v\u1edbi-c\xe1c-d\u1ecbch-v\u1ee5-aws-kh\xe1c",children:"T\xedch h\u1ee3p v\u1edbi c\xe1c d\u1ecbch v\u1ee5 AWS kh\xe1c"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Amazon S3"}),": Primary data storage"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"AWS Glue"}),": Data catalog v\xe0 ETL"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Amazon Kinesis"}),": Real-time data streaming"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Amazon Redshift"}),": Data warehouse integration"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Amazon SageMaker"}),": Machine learning workflows"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"AWS Lambda"}),": Event-driven processing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Amazon CloudWatch"}),": Monitoring v\xe0 alerting"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"AWS IAM"}),": Identity v\xe0 access management"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Amazon VPC"}),": Network security"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"AWS KMS"}),": Encryption key management"]}),"\n"]})]})}function g(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);