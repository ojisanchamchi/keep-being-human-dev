---
slug: ruby-compression-tu-co-ban-den-nang-cao
title: "Ruby Compression: Tá»« CÆ¡ Báº£n Äáº¿n NÃ¢ng Cao - HÆ°á»›ng Dáº«n ToÃ n Diá»‡n"
authors: [admin]
tags: [ruby, compression, gzip, zip, performance, optimization, zlib, streaming]
---

# Ruby Compression: Tá»« CÆ¡ Báº£n Äáº¿n NÃ¢ng Cao

Compression (nÃ©n dá»¯ liá»‡u) lÃ  má»™t ká»¹ thuáº­t quan trá»ng trong láº­p trÃ¬nh, giÃºp giáº£m kÃ­ch thÆ°á»›c file, tiáº¿t kiá»‡m bÄƒng thÃ´ng vÃ  tÄƒng hiá»‡u suáº¥t á»©ng dá»¥ng. Trong bÃ i viáº¿t nÃ y, chÃºng ta sáº½ khÃ¡m phÃ¡ cÃ¡c ká»¹ thuáº­t compression trong Ruby tá»« cÆ¡ báº£n Ä‘áº¿n nÃ¢ng cao, vá»›i cÃ¡c vÃ­ dá»¥ thá»±c táº¿ vÃ  best practices.

<!--truncate-->

## Táº¡i Sao Cáº§n Compression?

TrÆ°á»›c khi Ä‘i vÃ o chi tiáº¿t, hÃ£y hiá»ƒu táº¡i sao compression láº¡i quan trá»ng:

- **Tiáº¿t kiá»‡m dung lÆ°á»£ng lÆ°u trá»¯**: Giáº£m kÃ­ch thÆ°á»›c file trÃªn disk
- **TÄƒng tá»‘c Ä‘á»™ truyá»n táº£i**: Giáº£m thá»i gian upload/download
- **Giáº£m bÄƒng thÃ´ng**: Tiáº¿t kiá»‡m chi phÃ­ network
- **Cáº£i thiá»‡n performance**: Faster I/O operations
- **Backup hiá»‡u quáº£**: NÃ©n dá»¯ liá»‡u backup

## Pháº§n 1: CÆ¡ Báº£n - LÃ m Quen Vá»›i Compression

### 1.1 NÃ©n File Vá»›i Gzip

Ruby cung cáº¥p thÆ° viá»‡n `Zlib` built-in Ä‘á»ƒ lÃ m viá»‡c vá»›i compression. ÄÃ¢y lÃ  cÃ¡ch Ä‘Æ¡n giáº£n nháº¥t Ä‘á»ƒ nÃ©n má»™t file:

```ruby
require 'zlib'

def compress_file_with_gzip(source, destination)
  Zlib::GzipWriter.open(destination) do |gz|
    File.open(source, 'rb') do |file|
      while chunk = file.read(16 * 1024) # Äá»c tá»«ng chunk 16KB
        gz.write(chunk)
      end
    end
  end
  
  puts "ÄÃ£ nÃ©n #{source} thÃ nh #{destination}"
end

# Sá»­ dá»¥ng
compress_file_with_gzip('example.txt', 'example.txt.gz')
```

**Giáº£i thÃ­ch:**
- `Zlib::GzipWriter.open`: Má»Ÿ file Ä‘á»ƒ ghi dá»¯ liá»‡u Ä‘Ã£ nÃ©n
- Äá»c file theo chunk Ä‘á»ƒ trÃ¡nh load toÃ n bá»™ vÃ o memory
- Block tá»± Ä‘á»™ng Ä‘Ã³ng file khi hoÃ n thÃ nh

### 1.2 Táº¡o ZIP Archive

Äá»ƒ táº¡o file ZIP chá»©a nhiá»u file, chÃºng ta sá»­ dá»¥ng gem `rubyzip`:

```bash
gem install rubyzip
```

```ruby
require 'zip'

def create_zip_archive(files_to_zip, output_zip)
  Zip::File.open(output_zip, Zip::File::CREATE) do |zipfile|
    files_to_zip.each do |filename|
      # Kiá»ƒm tra file tá»“n táº¡i
      if File.exist?(filename)
        zipfile.add(File.basename(filename), filename)
        puts "ÄÃ£ thÃªm #{filename} vÃ o archive"
      else
        puts "Cáº£nh bÃ¡o: File #{filename} khÃ´ng tá»“n táº¡i"
      end
    end
  end
end

# Sá»­ dá»¥ng
files = ['file1.txt', 'file2.txt', 'data.json']
create_zip_archive(files, 'archive.zip')
```

**LÆ°u Ã½ quan trá»ng:**
- LuÃ´n kiá»ƒm tra file tá»“n táº¡i trÆ°á»›c khi thÃªm vÃ o ZIP
- `File.basename()` chá»‰ láº¥y tÃªn file, khÃ´ng bao gá»“m Ä‘Æ°á»ng dáº«n
- Sá»­ dá»¥ng block Ä‘á»ƒ tá»± Ä‘á»™ng Ä‘Ã³ng ZIP file

## Pháº§n 2: Trung Cáº¥p - Ká»¹ Thuáº­t NÃ¢ng Cao

### 2.1 NÃ©n String Trong Memory

Äá»‘i vá»›i dá»¯ liá»‡u táº¡m thá»i nhÆ° JSON payload hoáº·c cache, nÃ©n trong memory sáº½ nhanh hÆ¡n:

```ruby
require 'zlib'

def compress_string_in_memory(data, level: Zlib::BEST_COMPRESSION)
  # NÃ©n dá»¯ liá»‡u
  compressed = Zlib::Deflate.deflate(data, level)
  
  puts "KÃ­ch thÆ°á»›c gá»‘c: #{data.bytesize} bytes"
  puts "KÃ­ch thÆ°á»›c sau nÃ©n: #{compressed.bytesize} bytes"
  puts "Tá»· lá»‡ nÃ©n: #{((1 - compressed.bytesize.to_f / data.bytesize) * 100).round(2)}%"
  
  compressed
end

def decompress_string(compressed_data)
  Zlib::Inflate.inflate(compressed_data)
end

# VÃ­ dá»¥ vá»›i JSON data
json_data = {
  users: (1..1000).map { |i| 
    { id: i, name: "User #{i}", email: "user#{i}@example.com" }
  }
}.to_json

compressed = compress_string_in_memory(json_data)
decompressed = decompress_string(compressed)

puts "Dá»¯ liá»‡u khÃ´i phá»¥c chÃ­nh xÃ¡c: #{decompressed == json_data}"
```

**CÃ¡c má»©c Ä‘á»™ nÃ©n:**
- `Zlib::NO_COMPRESSION`: KhÃ´ng nÃ©n (0)
- `Zlib::BEST_SPEED`: NÃ©n nhanh nháº¥t (1)
- `Zlib::DEFAULT_COMPRESSION`: CÃ¢n báº±ng (6)
- `Zlib::BEST_COMPRESSION`: NÃ©n tá»‘t nháº¥t (9)

### 2.2 NÃ©n vÃ  Giáº£i NÃ©n File Streaming

PhÆ°Æ¡ng phÃ¡p streaming giÃºp xá»­ lÃ½ file lá»›n mÃ  khÃ´ng cáº§n load toÃ n bá»™ vÃ o memory:

```ruby
require 'zlib'

class FileCompressor
  CHUNK_SIZE = 16 * 1024 # 16KB chunks
  
  def self.compress_file(input_path, output_path)
    File.open(input_path, 'rb') do |input|
      Zlib::GzipWriter.open(output_path) do |gz|
        while chunk = input.read(CHUNK_SIZE)
          gz.write(chunk)
        end
      end
    end
    
    original_size = File.size(input_path)
    compressed_size = File.size(output_path)
    compression_ratio = ((1 - compressed_size.to_f / original_size) * 100).round(2)
    
    puts "NÃ©n hoÃ n thÃ nh:"
    puts "  File gá»‘c: #{format_bytes(original_size)}"
    puts "  File nÃ©n: #{format_bytes(compressed_size)}"
    puts "  Tá»· lá»‡ nÃ©n: #{compression_ratio}%"
  end
  
  def self.decompress_file(input_path, output_path)
    Zlib::GzipReader.open(input_path) do |gz|
      File.open(output_path, 'wb') do |output|
        while chunk = gz.read(CHUNK_SIZE)
          output.write(chunk)
        end
      end
    end
    
    puts "Giáº£i nÃ©n hoÃ n thÃ nh: #{output_path}"
  end
  
  private
  
  def self.format_bytes(bytes)
    units = ['B', 'KB', 'MB', 'GB']
    size = bytes.to_f
    unit_index = 0
    
    while size >= 1024 && unit_index < units.length - 1
      size /= 1024
      unit_index += 1
    end
    
    "#{size.round(2)} #{units[unit_index]}"
  end
end

# Sá»­ dá»¥ng
FileCompressor.compress_file('large_file.log', 'large_file.log.gz')
FileCompressor.decompress_file('large_file.log.gz', 'restored_file.log')
```

## Pháº§n 3: NÃ¢ng Cao - Ká»¹ Thuáº­t ChuyÃªn Nghiá»‡p

### 3.1 Táº¡o TAR.GZ Archives

Äá»ƒ táº¡o archive vá»›i metadata vÃ  permissions, káº¿t há»£p `Archive::Tar::Minitar` vá»›i `Zlib`:

```bash
gem install minitar
```

```ruby
require 'zlib'
require 'archive/tar/minitar'
include Archive::Tar

class TarGzArchiver
  def self.create_archive(sources, output_path, compression_level: Zlib::BEST_COMPRESSION)
    Zlib::GzipWriter.open(output_path, compression_level) do |gzip|
      Minitar::Writer.open(gzip) do |tar|
        sources.each do |path|
          if File.directory?(path)
            puts "ThÃªm thÆ° má»¥c: #{path}"
            Minitar.pack_dir(path, tar)
          elsif File.file?(path)
            puts "ThÃªm file: #{path}"
            Minitar.pack_file(path, tar)
          else
            puts "Cáº£nh bÃ¡o: #{path} khÃ´ng tá»“n táº¡i"
          end
        end
      end
    end
    
    puts "Táº¡o archive thÃ nh cÃ´ng: #{output_path}"
  end
  
  def self.extract_archive(archive_path, destination_dir)
    Dir.mkdir(destination_dir) unless Dir.exist?(destination_dir)
    
    Zlib::GzipReader.open(archive_path) do |gzip|
      Minitar::Reader.open(gzip) do |tar|
        tar.each do |entry|
          destination_path = File.join(destination_dir, entry.full_name)
          
          if entry.directory?
            Dir.mkdir(destination_path) unless Dir.exist?(destination_path)
          else
            File.open(destination_path, 'wb') do |file|
              file.write(entry.read)
            end
            # KhÃ´i phá»¥c permissions
            File.chmod(entry.mode, destination_path)
          end
          
          puts "Giáº£i nÃ©n: #{entry.full_name}"
        end
      end
    end
    
    puts "Giáº£i nÃ©n hoÃ n thÃ nh vÃ o: #{destination_dir}"
  end
end

# Sá»­ dá»¥ng
sources = ['app/', 'config/', 'README.md']
TarGzArchiver.create_archive(sources, 'backup.tar.gz')
TarGzArchiver.extract_archive('backup.tar.gz', 'restored/')
```

### 3.2 Streaming Large File Compression

Äá»‘i vá»›i file ráº¥t lá»›n, cáº§n tá»‘i Æ°u hÃ³a memory usage vÃ  performance:

```ruby
require 'zlib'

class LargeFileCompressor
  DEFAULT_BUFFER_SIZE = 64 * 1024 # 64KB buffer
  
  def initialize(buffer_size: DEFAULT_BUFFER_SIZE)
    @buffer_size = buffer_size
  end
  
  def compress_large_file(input_path, output_path, level: Zlib::BEST_SPEED)
    start_time = Time.now
    bytes_processed = 0
    
    File.open(input_path, 'rb') do |input|
      Zlib::GzipWriter.open(output_path, level) do |gz|
        while chunk = input.read(@buffer_size)
          gz.write(chunk)
          bytes_processed += chunk.bytesize
          
          # Progress reporting
          if bytes_processed % (1024 * 1024) == 0 # Má»—i 1MB
            print "\rÄÃ£ xá»­ lÃ½: #{format_bytes(bytes_processed)}"
          end
        end
      end
    end
    
    end_time = Time.now
    processing_time = end_time - start_time
    
    original_size = File.size(input_path)
    compressed_size = File.size(output_path)
    
    puts "\n\nThá»‘ng kÃª nÃ©n:"
    puts "  Thá»i gian xá»­ lÃ½: #{processing_time.round(2)}s"
    puts "  Tá»‘c Ä‘á»™: #{format_bytes(original_size / processing_time)}/s"
    puts "  KÃ­ch thÆ°á»›c gá»‘c: #{format_bytes(original_size)}"
    puts "  KÃ­ch thÆ°á»›c nÃ©n: #{format_bytes(compressed_size)}"
    puts "  Tá»· lá»‡ nÃ©n: #{((1 - compressed_size.to_f / original_size) * 100).round(2)}%"
  end
  
  private
  
  def format_bytes(bytes)
    units = ['B', 'KB', 'MB', 'GB', 'TB']
    size = bytes.to_f
    unit_index = 0
    
    while size >= 1024 && unit_index < units.length - 1
      size /= 1024
      unit_index += 1
    end
    
    "#{size.round(2)} #{units[unit_index]}"
  end
end

# Sá»­ dá»¥ng
compressor = LargeFileCompressor.new(buffer_size: 128 * 1024) # 128KB buffer
compressor.compress_large_file('huge_video.mp4', 'huge_video.mp4.gz')
```

## Pháº§n 4: Expert Level - Ká»¹ Thuáº­t ChuyÃªn Gia

### 4.1 Custom Streaming Chunked Compression

ÄÃ¢y lÃ  ká»¹ thuáº­t nÃ¢ng cao nháº¥t, cho phÃ©p kiá»ƒm soÃ¡t hoÃ n toÃ n quÃ¡ trÃ¬nh compression:

```ruby
require 'zlib'

class AdvancedStreamCompressor
  CHUNK_SIZE = 16 * 1024 # 16KB per chunk
  
  def self.compress_with_metadata(input_path, output_path, level: Zlib::BEST_COMPRESSION)
    File.open(input_path, 'rb') do |infile|
      File.open(output_path, 'wb') do |outfile|
        gz = Zlib::GzipWriter.new(outfile, level)
        
        # TÃ¹y chá»‰nh Gzip header vá»›i metadata
        gz.orig_name = File.basename(input_path)
        gz.mtime = File.mtime(input_path).to_i
        gz.comment = "Compressed by Ruby Advanced Compressor"
        
        # Streaming compression vá»›i progress tracking
        total_size = File.size(input_path)
        processed = 0
        
        while chunk = infile.read(CHUNK_SIZE)
          gz.write(chunk)
          processed += chunk.bytesize
          
          # Progress bar
          progress = (processed.to_f / total_size * 100).round(1)
          print "\rProgress: #{progress}% [#{format_progress_bar(progress)}]"
        end
        
        gz.close
        puts "\nHoÃ n thÃ nh!"
      end
    end
  end
  
  def self.benchmark_compression_levels(input_path)
    puts "Benchmark cÃ¡c má»©c Ä‘á»™ nÃ©n cho file: #{File.basename(input_path)}"
    puts "=" * 60
    
    levels = [
      [Zlib::BEST_SPEED, "BEST_SPEED"],
      [Zlib::DEFAULT_COMPRESSION, "DEFAULT"],
      [Zlib::BEST_COMPRESSION, "BEST_COMPRESSION"]
    ]
    
    original_size = File.size(input_path)
    
    levels.each do |level, name|
      output_path = "#{input_path}.#{name.downcase}.gz"
      
      start_time = Time.now
      compress_with_metadata(input_path, output_path, level: level)
      end_time = Time.now
      
      compressed_size = File.size(output_path)
      compression_ratio = ((1 - compressed_size.to_f / original_size) * 100).round(2)
      processing_time = (end_time - start_time).round(2)
      
      puts "\n#{name}:"
      puts "  Thá»i gian: #{processing_time}s"
      puts "  KÃ­ch thÆ°á»›c: #{format_bytes(compressed_size)}"
      puts "  Tá»· lá»‡ nÃ©n: #{compression_ratio}%"
      puts "  Tá»‘c Ä‘á»™: #{format_bytes(original_size / processing_time)}/s"
      puts
      
      # Cleanup
      File.delete(output_path)
    end
  end
  
  private
  
  def self.format_progress_bar(progress, width: 30)
    filled = (progress / 100.0 * width).round
    empty = width - filled
    "â–ˆ" * filled + "â–‘" * empty
  end
  
  def self.format_bytes(bytes)
    units = ['B', 'KB', 'MB', 'GB', 'TB']
    size = bytes.to_f
    unit_index = 0
    
    while size >= 1024 && unit_index < units.length - 1
      size /= 1024
      unit_index += 1
    end
    
    "#{size.round(2)} #{units[unit_index]}"
  end
end

# Sá»­ dá»¥ng
AdvancedStreamCompressor.benchmark_compression_levels('large_dataset.csv')
```

### 4.2 Multi-threaded Compression

Äá»‘i vá»›i há»‡ thá»‘ng multi-core, cÃ³ thá»ƒ tÄƒng tá»‘c báº±ng parallel processing:

```ruby
require 'zlib'
require 'thread'

class ParallelCompressor
  def self.compress_multiple_files(file_list, output_dir, max_threads: 4)
    Dir.mkdir(output_dir) unless Dir.exist?(output_dir)
    
    queue = Queue.new
    file_list.each { |file| queue << file }
    
    threads = []
    mutex = Mutex.new
    completed = 0
    
    max_threads.times do
      threads << Thread.new do
        while !queue.empty?
          begin
            file_path = queue.pop(true) # non-blocking pop
            output_path = File.join(output_dir, "#{File.basename(file_path)}.gz")
            
            compress_single_file(file_path, output_path)
            
            mutex.synchronize do
              completed += 1
              puts "HoÃ n thÃ nh #{completed}/#{file_list.length}: #{File.basename(file_path)}"
            end
          rescue ThreadError
            # Queue empty
            break
          end
        end
      end
    end
    
    threads.each(&:join)
    puts "NÃ©n song song hoÃ n thÃ nh!"
  end
  
  private
  
  def self.compress_single_file(input_path, output_path)
    Zlib::GzipWriter.open(output_path, Zlib::BEST_SPEED) do |gz|
      File.open(input_path, 'rb') do |file|
        while chunk = file.read(16 * 1024)
          gz.write(chunk)
        end
      end
    end
  end
end

# Sá»­ dá»¥ng
files = Dir.glob("logs/*.log")
ParallelCompressor.compress_multiple_files(files, "compressed_logs", max_threads: 8)
```

## Best Practices vÃ  LÆ°u Ã

### 1. Chá»n Má»©c Äá»™ NÃ©n PhÃ¹ Há»£p

```ruby
# Cho real-time applications
level = Zlib::BEST_SPEED

# Cho storage/backup
level = Zlib::BEST_COMPRESSION

# CÃ¢n báº±ng tá»‘c Ä‘á»™ vÃ  kÃ­ch thÆ°á»›c
level = Zlib::DEFAULT_COMPRESSION
```

### 2. Memory Management

```ruby
# âœ… Tá»‘t: Streaming vá»›i chunk size há»£p lÃ½
CHUNK_SIZE = 16 * 1024 # 16KB

# âŒ TrÃ¡nh: Load toÃ n bá»™ file vÃ o memory
data = File.read(large_file) # CÃ³ thá»ƒ gÃ¢y OOM
```

### 3. Error Handling

```ruby
def safe_compress(input_path, output_path)
  begin
    Zlib::GzipWriter.open(output_path) do |gz|
      File.open(input_path, 'rb') do |file|
        while chunk = file.read(16 * 1024)
          gz.write(chunk)
        end
      end
    end
    puts "NÃ©n thÃ nh cÃ´ng: #{output_path}"
  rescue Errno::ENOENT
    puts "Lá»—i: File khÃ´ng tá»“n táº¡i - #{input_path}"
  rescue Zlib::Error => e
    puts "Lá»—i compression: #{e.message}"
    File.delete(output_path) if File.exist?(output_path)
  rescue => e
    puts "Lá»—i khÃ´ng xÃ¡c Ä‘á»‹nh: #{e.message}"
  end
end
```

### 4. Performance Tips

- Sá»­ dá»¥ng buffer size lÃ  bá»™i sá»‘ cá»§a 2 (16KB, 32KB, 64KB)
- Chá»n compression level phÃ¹ há»£p vá»›i use case
- Vá»›i file lá»›n, Æ°u tiÃªn `BEST_SPEED` Ä‘á»ƒ giáº£m CPU usage
- Monitor memory usage khi xá»­ lÃ½ file lá»›n
- Sá»­ dá»¥ng parallel processing cho multiple files

## Káº¿t Luáº­n

Compression trong Ruby lÃ  má»™t ká»¹ nÄƒng quan trá»ng cho má»i developer. Tá»« viá»‡c nÃ©n file Ä‘Æ¡n giáº£n vá»›i Gzip Ä‘áº¿n cÃ¡c ká»¹ thuáº­t streaming phá»©c táº¡p, Ruby cung cáº¥p Ä‘áº§y Ä‘á»§ cÃ´ng cá»¥ Ä‘á»ƒ xá»­ lÃ½ má»i tÃ¬nh huá»‘ng.

Nhá»¯ng Ä‘iá»ƒm chÃ­nh cáº§n nhá»›:

1. **CÆ¡ báº£n**: Sá»­ dá»¥ng `Zlib` cho Gzip vÃ  `rubyzip` cho ZIP
2. **Trung cáº¥p**: Streaming Ä‘á»ƒ xá»­ lÃ½ file lá»›n, nÃ©n in-memory cho dá»¯ liá»‡u táº¡m
3. **NÃ¢ng cao**: TAR.GZ archives vá»›i metadata, parallel processing
4. **Expert**: Custom streaming vá»›i progress tracking vÃ  optimization

HÃ£y lá»±a chá»n ká»¹ thuáº­t phÃ¹ há»£p vá»›i nhu cáº§u cá»¥ thá»ƒ cá»§a dá»± Ã¡n vÃ  luÃ´n cÃ¢n nháº¯c giá»¯a tá»‘c Ä‘á»™ nÃ©n vÃ  kÃ­ch thÆ°á»›c file Ä‘áº§u ra.

ChÃºc cÃ¡c báº¡n Ã¡p dá»¥ng thÃ nh cÃ´ng cÃ¡c ká»¹ thuáº­t compression trong Ruby! ğŸš€
