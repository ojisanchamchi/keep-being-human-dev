"use strict";(self.webpackChunkkeep_being_human_dev=self.webpackChunkkeep_being_human_dev||[]).push([[77889],{5762:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/kinesis-video-streams-d589f4d2ee8833cf367859f590058a80.png"},63760:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>m,frontMatter:()=>r,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"diagrams/aws-nodes/aws-analytics/kinesis-video-streams","title":"Amazon Kinesis Video Streams","description":"T\u1ed5ng quan","source":"@site/docs/diagrams/aws-nodes/aws-analytics/22-kinesis-video-streams.md","sourceDirName":"diagrams/aws-nodes/aws-analytics","slug":"/diagrams/aws-nodes/aws-analytics/kinesis-video-streams","permalink":"/keep-being-human-dev/docs/diagrams/aws-nodes/aws-analytics/kinesis-video-streams","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/diagrams/aws-nodes/aws-analytics/22-kinesis-video-streams.md","tags":[],"version":"current","sidebarPosition":22,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Amazon Kinesis Data Streams","permalink":"/keep-being-human-dev/docs/diagrams/aws-nodes/aws-analytics/kinesis-data-streams"},"next":{"title":"Amazon Kinesis","permalink":"/keep-being-human-dev/docs/diagrams/aws-nodes/aws-analytics/kinesis"}}');var t=s(23420),a=s(65404);const r={},o="Amazon Kinesis Video Streams",c={},d=[{value:"T\u1ed5ng quan",id:"t\u1ed5ng-quan",level:2},{value:"Ch\u1ee9c n\u0103ng ch\xednh",id:"ch\u1ee9c-n\u0103ng-ch\xednh",level:2},{value:"1. Video Ingestion",id:"1-video-ingestion",level:3},{value:"2. Stream Management",id:"2-stream-management",level:3},{value:"3. Processing Integration",id:"3-processing-integration",level:3},{value:"4. Playback v\xe0 Access",id:"4-playback-v\xe0-access",level:3},{value:"Use Cases ph\u1ed5 bi\u1ebfn",id:"use-cases-ph\u1ed5-bi\u1ebfn",level:2},{value:"Diagram Architecture",id:"diagram-architecture",level:2},{value:"Code \u0111\u1ec3 t\u1ea1o diagram:",id:"code-\u0111\u1ec3-t\u1ea1o-diagram",level:3},{value:"Video Stream Setup",id:"video-stream-setup",level:2},{value:"1. Stream Creation v\xe0 Configuration",id:"1-stream-creation-v\xe0-configuration",level:3},{value:"2. Video Producer Implementation",id:"2-video-producer-implementation",level:3},{value:"3. Advanced Producer v\u1edbi GStreamer",id:"3-advanced-producer-v\u1edbi-gstreamer",level:3},{value:"Video Processing v\xe0 Analytics",id:"video-processing-v\xe0-analytics",level:2},{value:"1. Real-time Video Analysis",id:"1-real-time-video-analysis",level:3},{value:"2. Custom Video Processing v\u1edbi Lambda",id:"2-custom-video-processing-v\u1edbi-lambda",level:3},{value:"Playback v\xe0 Streaming",id:"playback-v\xe0-streaming",level:2},{value:"1. HLS Playback Implementation",id:"1-hls-playback-implementation",level:3},{value:"2. WebRTC Implementation",id:"2-webrtc-implementation",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"T\xedch h\u1ee3p v\u1edbi c\xe1c d\u1ecbch v\u1ee5 AWS kh\xe1c",id:"t\xedch-h\u1ee3p-v\u1edbi-c\xe1c-d\u1ecbch-v\u1ee5-aws-kh\xe1c",level:2}];function l(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"amazon-kinesis-video-streams",children:"Amazon Kinesis Video Streams"})}),"\n",(0,t.jsx)(n.h2,{id:"t\u1ed5ng-quan",children:"T\u1ed5ng quan"}),"\n",(0,t.jsx)(n.p,{children:"Amazon Kinesis Video Streams l\xe0 node \u0111\u1ea1i di\u1ec7n cho d\u1ecbch v\u1ee5 streaming video \u0111\u01b0\u1ee3c qu\u1ea3n l\xfd ho\xe0n to\xe0n c\u1ee7a AWS. D\u1ecbch v\u1ee5 n\xe0y gi\xfap b\u1ea1n d\u1ec5 d\xe0ng stream video t\u1eeb connected devices \u0111\u1ebfn AWS cho analytics, machine learning, playback, v\xe0 c\xe1c processing kh\xe1c. Kinesis Video Streams t\u1ef1 \u0111\u1ed9ng provision v\xe0 scale infrastructure c\u1ea7n thi\u1ebft \u0111\u1ec3 ingest streaming video data t\u1eeb millions of devices."}),"\n",(0,t.jsx)(n.h2,{id:"ch\u1ee9c-n\u0103ng-ch\xednh",children:"Ch\u1ee9c n\u0103ng ch\xednh"}),"\n",(0,t.jsx)(n.h3,{id:"1-video-ingestion",children:"1. Video Ingestion"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multi-device Support"}),": Cameras, smartphones, drones, IoT devices"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Real-time Streaming"}),": Low-latency video streaming"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Durable Storage"}),": Automatic storage v\u1edbi configurable retention"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multiple Formats"}),": H.264, H.265, v\xe0 custom formats"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"2-stream-management",children:"2. Stream Management"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Auto Scaling"}),": T\u1ef1 \u0111\u1ed9ng scale based on ingestion rate"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fragment-based Storage"}),": Efficient video fragment management"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Time-indexed Access"}),": Access video by timestamp"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Metadata Support"}),": Custom metadata v\u1edbi video fragments"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"3-processing-integration",children:"3. Processing Integration"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Amazon Rekognition"}),": Video analysis v\xe0 object detection"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"AWS Lambda"}),": Custom video processing"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Amazon SageMaker"}),": Machine learning inference"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Third-party Analytics"}),": Integration v\u1edbi external services"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"4-playback-v\xe0-access",children:"4. Playback v\xe0 Access"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"HLS Streaming"}),": HTTP Live Streaming support"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"DASH Streaming"}),": Dynamic Adaptive Streaming"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"WebRTC"}),": Real-time communication"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Custom Applications"}),": SDK-based access"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"use-cases-ph\u1ed5-bi\u1ebfn",children:"Use Cases ph\u1ed5 bi\u1ebfn"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Security Surveillance"}),": Real-time monitoring v\xe0 alerts"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Smart Home"}),": Home automation v\xe0 monitoring"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Industrial IoT"}),": Equipment monitoring v\xe0 predictive maintenance"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Retail Analytics"}),": Customer behavior analysis"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Healthcare"}),": Remote patient monitoring"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"diagram-architecture",children:"Diagram Architecture"}),"\n",(0,t.jsx)(n.p,{children:"Ki\u1ebfn tr\xfac Amazon Kinesis Video Streams v\u1edbi video processing pipeline:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Amazon Kinesis Video Streams Architecture",src:s(5762).A+"",width:"2176",height:"1811"})}),"\n",(0,t.jsx)(n.h3,{id:"code-\u0111\u1ec3-t\u1ea1o-diagram",children:"Code \u0111\u1ec3 t\u1ea1o diagram:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.analytics import KinesisVideoStreams\nfrom diagrams.aws.storage import S3\nfrom diagrams.aws.compute import Lambda, EC2\nfrom diagrams.aws.ml import Rekognition, SagemakerModel\nfrom diagrams.aws.integration import SQS, SNS\nfrom diagrams.aws.management import Cloudwatch\nfrom diagrams.aws.security import IAM\nfrom diagrams.onprem.client import Users\nfrom diagrams.onprem.iot import IotSensor\n\nwith Diagram("Amazon Kinesis Video Streams Architecture", show=False, direction="TB"):\n    \n    users = Users("Operators")\n    \n    with Cluster("Video Sources"):\n        security_cameras = IotSensor("Security Cameras")\n        mobile_devices = Lambda("Mobile Devices")\n        drones = IotSensor("Drones")\n        iot_cameras = IotSensor("IoT Cameras")\n        webcams = EC2("Web Cameras")\n    \n    with Cluster("Kinesis Video Streams"):\n        video_stream = KinesisVideoStreams("Video Stream")\n        \n        with Cluster("Stream Components"):\n            fragments = KinesisVideoStreams("Video Fragments")\n            metadata = KinesisVideoStreams("Metadata")\n            index = KinesisVideoStreams("Time Index")\n    \n    with Cluster("Real-time Processing"):\n        rekognition = Rekognition("Video Analysis")\n        lambda_processor = Lambda("Custom Processing")\n        ml_inference = SagemakerModel("ML Inference")\n    \n    with Cluster("Storage & Archival"):\n        s3_archive = S3("Video Archive")\n        s3_thumbnails = S3("Thumbnails")\n        s3_analytics = S3("Analytics Results")\n    \n    with Cluster("Playback & Streaming"):\n        hls_endpoint = Lambda("HLS Endpoint")\n        dash_endpoint = Lambda("DASH Endpoint")\n        webrtc_endpoint = Lambda("WebRTC Endpoint")\n        custom_player = EC2("Custom Player")\n    \n    with Cluster("Alerts & Notifications"):\n        alert_system = SNS("Alert System")\n        notification_queue = SQS("Notification Queue")\n        dashboard = Lambda("Monitoring Dashboard")\n    \n    with Cluster("Monitoring & Security"):\n        cloudwatch = Cloudwatch("CloudWatch")\n        security = IAM("IAM Roles")\n    \n    # Video ingestion\n    security_cameras >> Edge(label="RTSP/WebRTC") >> video_stream\n    mobile_devices >> Edge(label="SDK") >> video_stream\n    drones >> Edge(label="Live Feed") >> video_stream\n    iot_cameras >> Edge(label="H.264/H.265") >> video_stream\n    webcams >> Edge(label="USB/IP") >> video_stream\n    \n    # Stream components\n    video_stream >> fragments\n    video_stream >> metadata\n    video_stream >> index\n    \n    # Real-time processing\n    fragments >> Edge(label="Frame Analysis") >> rekognition\n    fragments >> Edge(label="Custom Logic") >> lambda_processor\n    fragments >> Edge(label="ML Models") >> ml_inference\n    \n    # Storage and archival\n    fragments >> Edge(label="Archive") >> s3_archive\n    rekognition >> Edge(label="Thumbnails") >> s3_thumbnails\n    lambda_processor >> Edge(label="Results") >> s3_analytics\n    \n    # Playback endpoints\n    fragments >> hls_endpoint\n    fragments >> dash_endpoint\n    fragments >> webrtc_endpoint\n    fragments >> custom_player\n    \n    # Alerts and notifications\n    rekognition >> Edge(label="Object Detection") >> alert_system\n    ml_inference >> Edge(label="Anomaly Detection") >> notification_queue\n    lambda_processor >> Edge(label="Custom Alerts") >> dashboard\n    \n    # User interaction\n    users >> Edge(label="Live View") >> hls_endpoint\n    users >> Edge(label="Playback") >> custom_player\n    users >> Edge(label="Monitor") >> dashboard\n    \n    # Monitoring and security\n    video_stream >> Edge(label="Metrics") >> cloudwatch\n    security >> Edge(label="Access Control") >> video_stream\n'})}),"\n",(0,t.jsx)(n.h2,{id:"video-stream-setup",children:"Video Stream Setup"}),"\n",(0,t.jsx)(n.h3,{id:"1-stream-creation-v\xe0-configuration",children:"1. Stream Creation v\xe0 Configuration"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import boto3\nimport json\nfrom datetime import datetime\n\nkvs = boto3.client('kinesisvideo')\n\ndef create_video_stream(stream_name, retention_hours=24):\n    \"\"\"Create Kinesis Video Stream\"\"\"\n    \n    try:\n        response = kvs.create_stream(\n            StreamName=stream_name,\n            DataRetentionInHours=retention_hours,\n            MediaType='video/h264',\n            DeviceName='security-camera-01',\n            Tags={\n                'Environment': 'production',\n                'Application': 'security-monitoring',\n                'Owner': 'security-team'\n            }\n        )\n        \n        print(f\"Video stream {stream_name} created successfully\")\n        return response\n        \n    except kvs.exceptions.ResourceInUseException:\n        print(f\"Stream {stream_name} already exists\")\n        return None\n\ndef configure_stream_settings(stream_name):\n    \"\"\"Configure advanced stream settings\"\"\"\n    \n    # Update stream configuration\n    kvs.update_stream(\n        StreamName=stream_name,\n        CurrentVersion='1.0',\n        MediaType='video/h264,audio/aac',  # Video with audio\n        DeviceName='security-camera-01'\n    )\n    \n    # Set up stream processor for real-time analysis\n    kvs.create_stream_processor(\n        ProcessorName=f'{stream_name}-processor',\n        Inputs=[\n            {\n                'KinesisVideoStream': {\n                    'StreamName': stream_name\n                }\n            }\n        ],\n        Output={\n            'KinesisDataStream': {\n                'StreamName': f'{stream_name}-analysis-results'\n            }\n        },\n        Settings={\n            'FaceSearch': {\n                'CollectionId': 'security-faces',\n                'FaceMatchThreshold': 85.0\n            }\n        },\n        RoleArn='arn:aws:iam::account:role/KinesisVideoStreamProcessorRole'\n    )\n\ndef get_stream_endpoint(stream_name, api_name):\n    \"\"\"Get stream endpoint for different APIs\"\"\"\n    \n    response = kvs.get_data_endpoint(\n        StreamName=stream_name,\n        APIName=api_name  # GET_MEDIA, GET_MEDIA_FOR_FRAGMENT_LIST, PUT_MEDIA, etc.\n    )\n    \n    return response['DataEndpoint']\n"})}),"\n",(0,t.jsx)(n.h3,{id:"2-video-producer-implementation",children:"2. Video Producer Implementation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import cv2\nimport boto3\nimport threading\nimport time\nfrom datetime import datetime\n\nclass KinesisVideoProducer:\n    def __init__(self, stream_name, region=\'us-west-2\'):\n        self.stream_name = stream_name\n        self.region = region\n        self.kvs = boto3.client(\'kinesisvideo\', region_name=region)\n        self.is_streaming = False\n        \n    def start_camera_stream(self, camera_index=0):\n        """Start streaming from camera"""\n        \n        # Get PUT_MEDIA endpoint\n        endpoint = self.kvs.get_data_endpoint(\n            StreamName=self.stream_name,\n            APIName=\'PUT_MEDIA\'\n        )[\'DataEndpoint\']\n        \n        # Initialize camera\n        cap = cv2.VideoCapture(camera_index)\n        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n        cap.set(cv2.CAP_PROP_FPS, 30)\n        \n        self.is_streaming = True\n        \n        try:\n            while self.is_streaming:\n                ret, frame = cap.read()\n                if not ret:\n                    break\n                \n                # Encode frame\n                encoded_frame = self.encode_frame(frame)\n                \n                # Send to Kinesis Video Streams\n                self.send_frame(encoded_frame, endpoint)\n                \n                # Control frame rate\n                time.sleep(1/30)  # 30 FPS\n                \n        finally:\n            cap.release()\n    \n    def encode_frame(self, frame):\n        """Encode frame to H.264"""\n        \n        # Convert BGR to RGB\n        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        \n        # Encode to H.264 (simplified - use proper encoder in production)\n        encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), 90]\n        _, encoded_frame = cv2.imencode(\'.jpg\', rgb_frame, encode_param)\n        \n        return encoded_frame.tobytes()\n    \n    def send_frame(self, frame_data, endpoint):\n        """Send frame to Kinesis Video Streams"""\n        \n        # Create PUT_MEDIA client\n        kvs_media = boto3.client(\n            \'kinesis-video-media\',\n            endpoint_url=endpoint,\n            region_name=self.region\n        )\n        \n        try:\n            # Send frame with metadata\n            response = kvs_media.put_media(\n                StreamName=self.stream_name,\n                FragmentTimecodeType=\'ABSOLUTE\',\n                ProducerStartTimestamp=datetime.utcnow(),\n                Payload=frame_data\n            )\n            \n        except Exception as e:\n            print(f"Error sending frame: {str(e)}")\n    \n    def stop_streaming(self):\n        """Stop video streaming"""\n        self.is_streaming = False\n\n# Usage example\ndef start_security_camera():\n    """Start security camera streaming"""\n    \n    producer = KinesisVideoProducer(\'security-camera-01\')\n    \n    # Start streaming in separate thread\n    streaming_thread = threading.Thread(\n        target=producer.start_camera_stream,\n        args=(0,)  # Camera index\n    )\n    streaming_thread.start()\n    \n    return producer, streaming_thread\n'})}),"\n",(0,t.jsx)(n.h3,{id:"3-advanced-producer-v\u1edbi-gstreamer",children:"3. Advanced Producer v\u1edbi GStreamer"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import gi\ngi.require_version(\'Gst\', \'1.0\')\nfrom gi.repository import Gst, GLib\nimport boto3\n\nclass GStreamerKinesisProducer:\n    def __init__(self, stream_name):\n        self.stream_name = stream_name\n        self.kvs = boto3.client(\'kinesisvideo\')\n        \n        # Initialize GStreamer\n        Gst.init(None)\n        \n        # Create pipeline\n        self.pipeline = self.create_pipeline()\n        \n    def create_pipeline(self):\n        """Create GStreamer pipeline for Kinesis Video Streams"""\n        \n        # Get stream endpoint\n        endpoint = self.kvs.get_data_endpoint(\n            StreamName=self.stream_name,\n            APIName=\'PUT_MEDIA\'\n        )[\'DataEndpoint\']\n        \n        # Pipeline elements\n        pipeline_str = f"""\n        v4l2src device=/dev/video0 ! \n        videoconvert ! \n        video/x-raw,format=I420,width=1280,height=720,framerate=30/1 ! \n        x264enc speed-preset=ultrafast tune=zerolatency key-int-max=45 bframes=0 ! \n        video/x-h264,stream-format=avc,alignment=au,profile=baseline ! \n        kvssink stream-name={self.stream_name} \n                 aws-region=us-west-2 \n                 endpoint-uri={endpoint}\n        """\n        \n        pipeline = Gst.parse_launch(pipeline_str)\n        return pipeline\n    \n    def start_streaming(self):\n        """Start GStreamer pipeline"""\n        \n        # Set pipeline to playing state\n        ret = self.pipeline.set_state(Gst.State.PLAYING)\n        \n        if ret == Gst.StateChangeReturn.FAILURE:\n            print("Failed to start pipeline")\n            return False\n        \n        print(f"Started streaming to {self.stream_name}")\n        return True\n    \n    def stop_streaming(self):\n        """Stop GStreamer pipeline"""\n        \n        self.pipeline.set_state(Gst.State.NULL)\n        print("Stopped streaming")\n'})}),"\n",(0,t.jsx)(n.h2,{id:"video-processing-v\xe0-analytics",children:"Video Processing v\xe0 Analytics"}),"\n",(0,t.jsx)(n.h3,{id:"1-real-time-video-analysis",children:"1. Real-time Video Analysis"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import boto3\nimport json\nfrom datetime import datetime\n\nclass VideoAnalyticsProcessor:\n    def __init__(self, stream_name):\n        self.stream_name = stream_name\n        self.rekognition = boto3.client('rekognition')\n        self.kvs = boto3.client('kinesisvideo')\n        \n    def start_face_detection(self):\n        \"\"\"Start real-time face detection\"\"\"\n        \n        response = self.rekognition.start_face_detection(\n            Video={\n                'KinesisVideoStream': {\n                    'Arn': f'arn:aws:kinesisvideo:region:account:stream/{self.stream_name}'\n                }\n            },\n            NotificationChannel={\n                'SNSTopicArn': 'arn:aws:sns:region:account:video-analysis-results',\n                'RoleArn': 'arn:aws:iam::account:role/RekognitionServiceRole'\n            },\n            FaceAttributes='ALL',\n            JobTag='security-face-detection'\n        )\n        \n        return response['JobId']\n    \n    def start_person_tracking(self):\n        \"\"\"Start person tracking in video stream\"\"\"\n        \n        response = self.rekognition.start_person_tracking(\n            Video={\n                'KinesisVideoStream': {\n                    'Arn': f'arn:aws:kinesisvideo:region:account:stream/{self.stream_name}'\n                }\n            },\n            NotificationChannel={\n                'SNSTopicArn': 'arn:aws:sns:region:account:person-tracking-results',\n                'RoleArn': 'arn:aws:iam::account:role/RekognitionServiceRole'\n            },\n            JobTag='security-person-tracking'\n        )\n        \n        return response['JobId']\n    \n    def start_label_detection(self):\n        \"\"\"Start object and scene detection\"\"\"\n        \n        response = self.rekognition.start_label_detection(\n            Video={\n                'KinesisVideoStream': {\n                    'Arn': f'arn:aws:kinesisvideo:region:account:stream/{self.stream_name}'\n                }\n            },\n            MinConfidence=80.0,\n            NotificationChannel={\n                'SNSTopicArn': 'arn:aws:sns:region:account:label-detection-results',\n                'RoleArn': 'arn:aws:iam::account:role/RekognitionServiceRole'\n            },\n            Features=['GENERAL_LABELS', 'MODERATION_LABELS'],\n            JobTag='security-object-detection'\n        )\n        \n        return response['JobId']\n    \n    def process_analysis_results(self, job_id, job_type):\n        \"\"\"Process analysis results\"\"\"\n        \n        if job_type == 'FACE_DETECTION':\n            response = self.rekognition.get_face_detection(JobId=job_id)\n        elif job_type == 'PERSON_TRACKING':\n            response = self.rekognition.get_person_tracking(JobId=job_id)\n        elif job_type == 'LABEL_DETECTION':\n            response = self.rekognition.get_label_detection(JobId=job_id)\n        \n        # Process results\n        results = []\n        for item in response.get('Faces', response.get('Persons', response.get('Labels', []))):\n            result = {\n                'timestamp': item.get('Timestamp', 0),\n                'confidence': item.get('Face', item.get('Person', item.get('Label', {}))).get('Confidence', 0),\n                'bounding_box': item.get('Face', item.get('Person', {})).get('BoundingBox', {}),\n                'attributes': item.get('Face', {}).get('Attributes', {}),\n                'detected_at': datetime.utcnow().isoformat()\n            }\n            results.append(result)\n        \n        return results\n"})}),"\n",(0,t.jsx)(n.h3,{id:"2-custom-video-processing-v\u1edbi-lambda",children:"2. Custom Video Processing v\u1edbi Lambda"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import json\nimport boto3\nimport base64\nfrom datetime import datetime\n\ndef lambda_handler(event, context):\n    \"\"\"Process Kinesis Video Stream fragments\"\"\"\n    \n    kvs = boto3.client('kinesisvideo')\n    \n    # Parse SNS notification from Rekognition\n    for record in event['Records']:\n        if record['EventSource'] == 'aws:sns':\n            message = json.loads(record['Sns']['Message'])\n            \n            if message['Status'] == 'SUCCEEDED':\n                # Process successful analysis\n                process_video_analysis(message)\n            else:\n                # Handle failed analysis\n                handle_analysis_failure(message)\n    \n    return {\n        'statusCode': 200,\n        'body': json.dumps('Video processing completed')\n    }\n\ndef process_video_analysis(message):\n    \"\"\"Process video analysis results\"\"\"\n    \n    job_id = message['JobId']\n    job_tag = message.get('JobTag', '')\n    \n    if 'face-detection' in job_tag:\n        process_face_detection_results(job_id)\n    elif 'person-tracking' in job_tag:\n        process_person_tracking_results(job_id)\n    elif 'object-detection' in job_tag:\n        process_object_detection_results(job_id)\n\ndef process_face_detection_results(job_id):\n    \"\"\"Process face detection results\"\"\"\n    \n    rekognition = boto3.client('rekognition')\n    dynamodb = boto3.resource('dynamodb')\n    \n    # Get face detection results\n    response = rekognition.get_face_detection(JobId=job_id)\n    \n    # Store results in DynamoDB\n    table = dynamodb.Table('video-analysis-results')\n    \n    for face in response['Faces']:\n        # Check if face matches known individuals\n        if face['Face']['Confidence'] > 90:\n            # Search in face collection\n            search_response = rekognition.search_faces_by_image(\n                CollectionId='security-faces',\n                Image={'Bytes': get_face_image_bytes(face)},\n                MaxFaces=1,\n                FaceMatchThreshold=85\n            )\n            \n            # Store result\n            table.put_item(\n                Item={\n                    'job_id': job_id,\n                    'timestamp': face['Timestamp'],\n                    'face_confidence': face['Face']['Confidence'],\n                    'bounding_box': face['Face']['BoundingBox'],\n                    'matched_face': search_response.get('FaceMatches', []),\n                    'created_at': datetime.utcnow().isoformat()\n                }\n            )\n            \n            # Send alert if unknown face detected\n            if not search_response.get('FaceMatches'):\n                send_security_alert('Unknown face detected', face)\n\ndef send_security_alert(alert_type, detection_data):\n    \"\"\"Send security alert\"\"\"\n    \n    sns = boto3.client('sns')\n    \n    message = {\n        'alert_type': alert_type,\n        'timestamp': datetime.utcnow().isoformat(),\n        'confidence': detection_data.get('Face', {}).get('Confidence', 0),\n        'location': detection_data.get('Face', {}).get('BoundingBox', {}),\n        'stream_name': 'security-camera-01'\n    }\n    \n    sns.publish(\n        TopicArn='arn:aws:sns:region:account:security-alerts',\n        Message=json.dumps(message),\n        Subject=f'Security Alert: {alert_type}'\n    )\n"})}),"\n",(0,t.jsx)(n.h2,{id:"playback-v\xe0-streaming",children:"Playback v\xe0 Streaming"}),"\n",(0,t.jsx)(n.h3,{id:"1-hls-playback-implementation",children:"1. HLS Playback Implementation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import boto3\nfrom datetime import datetime, timedelta\n\nclass KinesisVideoPlayback:\n    def __init__(self, stream_name):\n        self.stream_name = stream_name\n        self.kvs = boto3.client('kinesisvideo')\n        \n    def get_hls_streaming_session(self, start_time=None, end_time=None):\n        \"\"\"Get HLS streaming session URL\"\"\"\n        \n        # Get HLS endpoint\n        endpoint = self.kvs.get_data_endpoint(\n            StreamName=self.stream_name,\n            APIName='GET_HLS_STREAMING_SESSION_URL'\n        )['DataEndpoint']\n        \n        # Create HLS client\n        kvs_archived_media = boto3.client(\n            'kinesis-video-archived-media',\n            endpoint_url=endpoint\n        )\n        \n        # Configure playback parameters\n        playback_config = {\n            'StreamName': self.stream_name,\n            'PlaybackMode': 'LIVE'  # or 'ON_DEMAND'\n        }\n        \n        if start_time and end_time:\n            playback_config.update({\n                'PlaybackMode': 'ON_DEMAND',\n                'HLSFragmentSelector': {\n                    'FragmentSelectorType': 'SERVER_TIMESTAMP',\n                    'TimestampRange': {\n                        'StartTimestamp': start_time,\n                        'EndTimestamp': end_time\n                    }\n                }\n            })\n        \n        # Get HLS streaming session URL\n        response = kvs_archived_media.get_hls_streaming_session_url(**playback_config)\n        \n        return response['HLSStreamingSessionURL']\n    \n    def get_dash_streaming_session(self, start_time=None, end_time=None):\n        \"\"\"Get DASH streaming session URL\"\"\"\n        \n        # Get DASH endpoint\n        endpoint = self.kvs.get_data_endpoint(\n            StreamName=self.stream_name,\n            APIName='GET_DASH_STREAMING_SESSION_URL'\n        )['DataEndpoint']\n        \n        # Create DASH client\n        kvs_archived_media = boto3.client(\n            'kinesis-video-archived-media',\n            endpoint_url=endpoint\n        )\n        \n        # Configure DASH parameters\n        dash_config = {\n            'StreamName': self.stream_name,\n            'PlaybackMode': 'LIVE'\n        }\n        \n        if start_time and end_time:\n            dash_config.update({\n                'PlaybackMode': 'ON_DEMAND',\n                'DASHFragmentSelector': {\n                    'FragmentSelectorType': 'SERVER_TIMESTAMP',\n                    'TimestampRange': {\n                        'StartTimestamp': start_time,\n                        'EndTimestamp': end_time\n                    }\n                }\n            })\n        \n        response = kvs_archived_media.get_dash_streaming_session_url(**dash_config)\n        \n        return response['DASHStreamingSessionURL']\n    \n    def get_media_for_fragment_list(self, fragment_numbers):\n        \"\"\"Get media for specific fragments\"\"\"\n        \n        # Get media endpoint\n        endpoint = self.kvs.get_data_endpoint(\n            StreamName=self.stream_name,\n            APIName='GET_MEDIA_FOR_FRAGMENT_LIST'\n        )['DataEndpoint']\n        \n        # Create media client\n        kvs_archived_media = boto3.client(\n            'kinesis-video-archived-media',\n            endpoint_url=endpoint\n        )\n        \n        response = kvs_archived_media.get_media_for_fragment_list(\n            StreamName=self.stream_name,\n            Fragments=fragment_numbers\n        )\n        \n        return response['Payload']\n"})}),"\n",(0,t.jsx)(n.h3,{id:"2-webrtc-implementation",children:"2. WebRTC Implementation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-javascript",children:"// WebRTC viewer implementation\nclass KinesisVideoWebRTCViewer {\n    constructor(streamName, region) {\n        this.streamName = streamName;\n        this.region = region;\n        this.peerConnection = null;\n        this.localVideo = null;\n        this.remoteVideo = null;\n    }\n    \n    async startViewer() {\n        // Get signaling channel endpoint\n        const kinesisVideo = new AWS.KinesisVideo({ region: this.region });\n        \n        const endpoint = await kinesisVideo.getDataEndpoint({\n            StreamName: this.streamName,\n            APIName: 'GET_SIGNALING_CHANNEL_ENDPOINT'\n        }).promise();\n        \n        // Create WebRTC peer connection\n        this.peerConnection = new RTCPeerConnection({\n            iceServers: [\n                { urls: 'stun:stun.kinesisvideo.us-west-2.amazonaws.com:443' }\n            ]\n        });\n        \n        // Handle remote stream\n        this.peerConnection.ontrack = (event) => {\n            if (this.remoteVideo) {\n                this.remoteVideo.srcObject = event.streams[0];\n            }\n        };\n        \n        // Connect to signaling channel\n        await this.connectToSignalingChannel(endpoint.DataEndpoint);\n    }\n    \n    async connectToSignalingChannel(endpoint) {\n        // WebSocket connection to signaling channel\n        const ws = new WebSocket(endpoint);\n        \n        ws.onopen = () => {\n            console.log('Connected to signaling channel');\n            this.sendViewerMessage();\n        };\n        \n        ws.onmessage = async (event) => {\n            const message = JSON.parse(event.data);\n            await this.handleSignalingMessage(message);\n        };\n    }\n    \n    async handleSignalingMessage(message) {\n        switch (message.messageType) {\n            case 'SDP_OFFER':\n                await this.handleOffer(message.messagePayload);\n                break;\n            case 'ICE_CANDIDATE':\n                await this.handleIceCandidate(message.messagePayload);\n                break;\n        }\n    }\n    \n    async handleOffer(offer) {\n        await this.peerConnection.setRemoteDescription(offer);\n        \n        const answer = await this.peerConnection.createAnswer();\n        await this.peerConnection.setLocalDescription(answer);\n        \n        // Send answer back through signaling channel\n        this.sendSignalingMessage('SDP_ANSWER', answer);\n    }\n}\n"})}),"\n",(0,t.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Stream Configuration"}),": Configure appropriate retention v\xe0 resolution"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Security"}),": Use IAM roles v\xe0 encrypt streams"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cost Optimization"}),": Monitor usage v\xe0 optimize retention periods"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Performance"}),": Optimize encoding settings cho bandwidth"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Monitoring"}),": Set up comprehensive monitoring"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Error Handling"}),": Implement robust error handling"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Scalability"}),": Design cho multiple concurrent streams"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Integration"}),": Leverage AWS services cho processing"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"t\xedch-h\u1ee3p-v\u1edbi-c\xe1c-d\u1ecbch-v\u1ee5-aws-kh\xe1c",children:"T\xedch h\u1ee3p v\u1edbi c\xe1c d\u1ecbch v\u1ee5 AWS kh\xe1c"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Amazon Rekognition Video"}),": Video analysis v\xe0 object detection"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"AWS Lambda"}),": Custom video processing"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Amazon SageMaker"}),": Machine learning inference"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Amazon S3"}),": Video archival v\xe0 storage"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Amazon DynamoDB"}),": Metadata v\xe0 results storage"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Amazon SNS"}),": Real-time notifications"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Amazon CloudWatch"}),": Monitoring v\xe0 alerting"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"AWS IAM"}),": Security v\xe0 access control"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Amazon API Gateway"}),": REST APIs cho video access"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"AWS IoT Core"}),": IoT device integration"]}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}},65404:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>o});var i=s(36672);const t={},a=i.createContext(t);function r(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);