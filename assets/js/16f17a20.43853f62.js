"use strict";(self.webpackChunkkeep_being_human_dev=self.webpackChunkkeep_being_human_dev||[]).push([[26127],{33579:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>m,frontMatter:()=>r,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"diagrams/aws-nodes/aws-analytics/lake-formation","title":"AWS Lake Formation","description":"Overview","source":"@site/docs/diagrams/aws-nodes/aws-analytics/24-lake-formation.md","sourceDirName":"diagrams/aws-nodes/aws-analytics","slug":"/diagrams/aws-nodes/aws-analytics/lake-formation","permalink":"/keep-being-human-dev/docs/diagrams/aws-nodes/aws-analytics/lake-formation","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/diagrams/aws-nodes/aws-analytics/24-lake-formation.md","tags":[],"version":"current","sidebarPosition":24,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Amazon Kinesis","permalink":"/keep-being-human-dev/docs/diagrams/aws-nodes/aws-analytics/kinesis"},"next":{"title":"Amazon Managed Streaming for Apache Kafka (MSK)","permalink":"/keep-being-human-dev/docs/diagrams/aws-nodes/aws-analytics/managed-streaming-for-kafka"}}');var i=a(23420),t=a(65404);const r={},l="AWS Lake Formation",o={},c=[{value:"Overview",id:"overview",level:2},{value:"Main Functions",id:"main-functions",level:2},{value:"Data Lake Setup and Management",id:"data-lake-setup-and-management",level:3},{value:"Security and Access Control",id:"security-and-access-control",level:3},{value:"Data Transformation and Preparation",id:"data-transformation-and-preparation",level:3},{value:"Integration and Analytics",id:"integration-and-analytics",level:3},{value:"Use Cases",id:"use-cases",level:2},{value:"Enterprise Data Lake",id:"enterprise-data-lake",level:3},{value:"Data Governance and Compliance",id:"data-governance-and-compliance",level:3},{value:"Multi-Account Data Sharing",id:"multi-account-data-sharing",level:3},{value:"Architecture Diagram",id:"architecture-diagram",level:2},{value:"AWS Service Integrations",id:"aws-service-integrations",level:2},{value:"Analytics Services",id:"analytics-services",level:3},{value:"Data Services",id:"data-services",level:3},{value:"Security and Governance",id:"security-and-governance",level:3},{value:"Machine Learning",id:"machine-learning",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Security",id:"security",level:3},{value:"Performance",id:"performance",level:3},{value:"Cost Optimization",id:"cost-optimization",level:3},{value:"Governance",id:"governance",level:3},{value:"Monitoring and Troubleshooting",id:"monitoring-and-troubleshooting",level:2},{value:"Key Metrics",id:"key-metrics",level:3},{value:"Common Issues",id:"common-issues",level:3},{value:"Troubleshooting Steps",id:"troubleshooting-steps",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"aws-lake-formation",children:"AWS Lake Formation"})}),"\n",(0,i.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(n.p,{children:"AWS Lake Formation is a service that makes it easy to set up a secure data lake in days. A data lake is a centralized, curated, and secured repository that stores all your data, both in its original form and prepared for analysis. Lake Formation simplifies and automates many of the complex manual steps usually required to create data lakes including collecting, cleansing, moving, and cataloging data, and securely making that data available for analytics and machine learning."}),"\n",(0,i.jsx)(n.h2,{id:"main-functions",children:"Main Functions"}),"\n",(0,i.jsx)(n.h3,{id:"data-lake-setup-and-management",children:"Data Lake Setup and Management"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Automated Data Lake Creation"}),": Simplifies the process of setting up a secure data lake"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Data Ingestion"}),": Automated collection and ingestion of data from various sources"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Data Cataloging"}),": Automatic discovery and cataloging of data assets"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Schema Evolution"}),": Handles changes in data structure over time"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"security-and-access-control",children:"Security and Access Control"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fine-Grained Access Control"}),": Column, row, and cell-level security policies"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Centralized Permissions"}),": Single place to manage data access across AWS services"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Data Encryption"}),": Automatic encryption of data at rest and in transit"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Audit and Compliance"}),": Comprehensive logging and monitoring of data access"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"data-transformation-and-preparation",children:"Data Transformation and Preparation"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ETL Workflows"}),": Built-in data transformation capabilities"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Data Quality"}),": Automated data validation and quality checks"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Format Conversion"}),": Convert data between different formats (Parquet, ORC, etc.)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Partitioning"}),": Automatic data partitioning for better performance"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"integration-and-analytics",children:"Integration and Analytics"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"AWS Service Integration"}),": Native integration with Athena, EMR, Redshift, and more"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Third-Party Connectors"}),": Support for external analytics tools"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Machine Learning"}),": Integration with SageMaker and other ML services"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Real-time Analytics"}),": Support for streaming data ingestion and analysis"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"use-cases",children:"Use Cases"}),"\n",(0,i.jsx)(n.h3,{id:"enterprise-data-lake",children:"Enterprise Data Lake"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import boto3\nfrom datetime import datetime\n\n# Initialize Lake Formation client\nlakeformation = boto3.client('lakeformation')\nglue = boto3.client('glue')\n\nclass DataLakeManager:\n    def __init__(self):\n        self.lf_client = lakeformation\n        self.glue_client = glue\n    \n    def create_data_lake_location(self, s3_path, role_arn):\n        \"\"\"Register S3 location as data lake location\"\"\"\n        try:\n            response = self.lf_client.register_resource(\n                ResourceArn=s3_path,\n                UseServiceLinkedRole=False,\n                RoleArn=role_arn\n            )\n            print(f\"Data lake location registered: {s3_path}\")\n            return response\n        except Exception as e:\n            print(f\"Error registering location: {e}\")\n    \n    def grant_database_permissions(self, principal, database_name, permissions):\n        \"\"\"Grant permissions to database\"\"\"\n        try:\n            response = self.lf_client.grant_permissions(\n                Principal={'DataLakePrincipalIdentifier': principal},\n                Resource={\n                    'Database': {\n                        'Name': database_name\n                    }\n                },\n                Permissions=permissions\n            )\n            print(f\"Permissions granted to {principal} on database {database_name}\")\n            return response\n        except Exception as e:\n            print(f\"Error granting permissions: {e}\")\n    \n    def grant_table_permissions(self, principal, database_name, table_name, \n                              permissions, column_names=None):\n        \"\"\"Grant fine-grained table permissions\"\"\"\n        try:\n            resource = {\n                'Table': {\n                    'DatabaseName': database_name,\n                    'Name': table_name\n                }\n            }\n            \n            # Add column-level permissions if specified\n            if column_names:\n                resource['TableWithColumns'] = {\n                    'DatabaseName': database_name,\n                    'Name': table_name,\n                    'ColumnNames': column_names\n                }\n                del resource['Table']\n            \n            response = self.lf_client.grant_permissions(\n                Principal={'DataLakePrincipalIdentifier': principal},\n                Resource=resource,\n                Permissions=permissions\n            )\n            print(f\"Table permissions granted to {principal}\")\n            return response\n        except Exception as e:\n            print(f\"Error granting table permissions: {e}\")\n\n# Usage example\ndata_lake = DataLakeManager()\n\n# Register S3 bucket as data lake location\ndata_lake.create_data_lake_location(\n    s3_path='arn:aws:s3:::my-data-lake-bucket/',\n    role_arn='arn:aws:iam::123456789012:role/LakeFormationServiceRole'\n)\n\n# Grant database permissions\ndata_lake.grant_database_permissions(\n    principal='arn:aws:iam::123456789012:user/analyst',\n    database_name='sales_database',\n    permissions=['DESCRIBE']\n)\n\n# Grant column-level permissions\ndata_lake.grant_table_permissions(\n    principal='arn:aws:iam::123456789012:user/analyst',\n    database_name='sales_database',\n    table_name='customer_data',\n    permissions=['SELECT'],\n    column_names=['customer_id', 'purchase_amount', 'purchase_date']\n)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"data-governance-and-compliance",children:"Data Governance and Compliance"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import boto3\nimport json\nfrom datetime import datetime, timedelta\n\nclass DataGovernanceManager:\n    def __init__(self):\n        self.lf_client = boto3.client('lakeformation')\n        self.cloudtrail = boto3.client('cloudtrail')\n    \n    def create_data_filter(self, table_name, database_name, filter_expression):\n        \"\"\"Create row-level security filter\"\"\"\n        try:\n            response = self.lf_client.create_data_cells_filter(\n                TableData={\n                    'TableCatalogId': '123456789012',\n                    'DatabaseName': database_name,\n                    'TableName': table_name,\n                    'Name': f\"{table_name}_filter\",\n                    'RowFilter': {\n                        'FilterExpression': filter_expression\n                    }\n                }\n            )\n            print(f\"Data filter created for table {table_name}\")\n            return response\n        except Exception as e:\n            print(f\"Error creating data filter: {e}\")\n    \n    def audit_data_access(self, start_time, end_time):\n        \"\"\"Audit data access through CloudTrail\"\"\"\n        try:\n            response = self.cloudtrail.lookup_events(\n                LookupAttributes=[\n                    {\n                        'AttributeKey': 'EventSource',\n                        'AttributeValue': 'lakeformation.amazonaws.com'\n                    }\n                ],\n                StartTime=start_time,\n                EndTime=end_time\n            )\n            \n            access_events = []\n            for event in response['Events']:\n                if 'GetDataAccess' in event['EventName']:\n                    access_events.append({\n                        'timestamp': event['EventTime'],\n                        'user': event.get('Username', 'Unknown'),\n                        'resource': event.get('Resources', []),\n                        'source_ip': event.get('SourceIPAddress')\n                    })\n            \n            return access_events\n        except Exception as e:\n            print(f\"Error auditing data access: {e}\")\n    \n    def generate_compliance_report(self):\n        \"\"\"Generate compliance report\"\"\"\n        try:\n            # Get all registered locations\n            locations = self.lf_client.describe_resource()\n            \n            # Get all databases\n            databases = self.lf_client.get_databases()\n            \n            # Get permissions summary\n            permissions = self.lf_client.list_permissions()\n            \n            report = {\n                'timestamp': datetime.now().isoformat(),\n                'registered_locations': len(locations.get('ResourceInfoList', [])),\n                'databases': len(databases.get('DatabaseList', [])),\n                'active_permissions': len(permissions.get('PrincipalResourcePermissions', [])),\n                'compliance_status': 'COMPLIANT'\n            }\n            \n            return report\n        except Exception as e:\n            print(f\"Error generating compliance report: {e}\")\n\n# Usage example\ngovernance = DataGovernanceManager()\n\n# Create row-level filter for sensitive data\ngovernance.create_data_filter(\n    table_name='employee_data',\n    database_name='hr_database',\n    filter_expression='department = \"HR\" OR manager_id = current_user_id()'\n)\n\n# Audit recent data access\nend_time = datetime.now()\nstart_time = end_time - timedelta(days=7)\naccess_log = governance.audit_data_access(start_time, end_time)\n\nprint(f\"Found {len(access_log)} data access events in the last 7 days\")\n"})}),"\n",(0,i.jsx)(n.h3,{id:"multi-account-data-sharing",children:"Multi-Account Data Sharing"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import boto3\n\nclass CrossAccountDataSharing:\n    def __init__(self):\n        self.lf_client = boto3.client('lakeformation')\n        self.ram_client = boto3.client('ram')\n    \n    def create_resource_share(self, share_name, resource_arns, principals):\n        \"\"\"Create RAM resource share for cross-account access\"\"\"\n        try:\n            response = self.ram_client.create_resource_share(\n                name=share_name,\n                resourceArns=resource_arns,\n                principals=principals,\n                allowExternalPrincipals=True,\n                tags=[\n                    {\n                        'key': 'Purpose',\n                        'value': 'DataLakeSharing'\n                    }\n                ]\n            )\n            print(f\"Resource share created: {share_name}\")\n            return response\n        except Exception as e:\n            print(f\"Error creating resource share: {e}\")\n    \n    def grant_cross_account_permissions(self, external_account_id, \n                                     database_name, table_name, permissions):\n        \"\"\"Grant permissions to external account\"\"\"\n        try:\n            principal = f\"arn:aws:organizations::123456789012:account/o-example/{external_account_id}\"\n            \n            response = self.lf_client.grant_permissions(\n                Principal={'DataLakePrincipalIdentifier': principal},\n                Resource={\n                    'Table': {\n                        'DatabaseName': database_name,\n                        'Name': table_name\n                    }\n                },\n                Permissions=permissions\n            )\n            print(f\"Cross-account permissions granted to {external_account_id}\")\n            return response\n        except Exception as e:\n            print(f\"Error granting cross-account permissions: {e}\")\n    \n    def create_data_share_invitation(self, recipient_account, database_name):\n        \"\"\"Create invitation for data sharing\"\"\"\n        try:\n            response = self.lf_client.create_lake_formation_opt_in(\n                Principal={'DataLakePrincipalIdentifier': f\"arn:aws:iam::{recipient_account}:root\"},\n                Resource={\n                    'Database': {\n                        'Name': database_name\n                    }\n                }\n            )\n            print(f\"Data share invitation created for account {recipient_account}\")\n            return response\n        except Exception as e:\n            print(f\"Error creating data share invitation: {e}\")\n\n# Usage example\ncross_account = CrossAccountDataSharing()\n\n# Create resource share\ncross_account.create_resource_share(\n    share_name='analytics-data-share',\n    resource_arns=['arn:aws:glue:us-east-1:123456789012:database/analytics_db'],\n    principals=['123456789013']  # Target account ID\n)\n\n# Grant cross-account table permissions\ncross_account.grant_cross_account_permissions(\n    external_account_id='123456789013',\n    database_name='analytics_db',\n    table_name='sales_data',\n    permissions=['SELECT', 'DESCRIBE']\n)\n"})}),"\n",(0,i.jsx)(n.h2,{id:"architecture-diagram",children:"Architecture Diagram"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"AWS Lake Formation Architecture",src:a(36095).A+"",width:"2069",height:"1784"})}),"\n",(0,i.jsx)(n.h2,{id:"aws-service-integrations",children:"AWS Service Integrations"}),"\n",(0,i.jsx)(n.h3,{id:"analytics-services",children:"Analytics Services"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Amazon Athena"}),": Query data lake using SQL"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Amazon EMR"}),": Big data processing and analytics"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Amazon Redshift"}),": Data warehouse integration"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Amazon QuickSight"}),": Business intelligence and visualization"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"data-services",children:"Data Services"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"AWS Glue"}),": ETL and data cataloging"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Amazon S3"}),": Primary data storage"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"AWS DataSync"}),": Data transfer and synchronization"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Amazon Kinesis"}),": Real-time data streaming"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"security-and-governance",children:"Security and Governance"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"AWS IAM"}),": Identity and access management"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"AWS CloudTrail"}),": Audit logging"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"AWS Config"}),": Configuration compliance"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Amazon Macie"}),": Data classification and protection"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"machine-learning",children:"Machine Learning"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Amazon SageMaker"}),": ML model development"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Amazon Comprehend"}),": Natural language processing"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Amazon Rekognition"}),": Image and video analysis"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,i.jsx)(n.h3,{id:"security",children:"Security"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Implement least privilege access principles"}),"\n",(0,i.jsx)(n.li,{children:"Use fine-grained permissions for sensitive data"}),"\n",(0,i.jsx)(n.li,{children:"Enable comprehensive audit logging"}),"\n",(0,i.jsx)(n.li,{children:"Encrypt data at rest and in transit"}),"\n",(0,i.jsx)(n.li,{children:"Regular access reviews and compliance checks"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"performance",children:"Performance"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Optimize data partitioning strategies"}),"\n",(0,i.jsx)(n.li,{children:"Use appropriate file formats (Parquet, ORC)"}),"\n",(0,i.jsx)(n.li,{children:"Implement data lifecycle policies"}),"\n",(0,i.jsx)(n.li,{children:"Monitor query performance and costs"}),"\n",(0,i.jsx)(n.li,{children:"Use data compaction for better performance"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"cost-optimization",children:"Cost Optimization"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Implement data tiering strategies"}),"\n",(0,i.jsx)(n.li,{children:"Use S3 storage classes appropriately"}),"\n",(0,i.jsx)(n.li,{children:"Monitor and optimize compute costs"}),"\n",(0,i.jsx)(n.li,{children:"Implement automated data archival"}),"\n",(0,i.jsx)(n.li,{children:"Regular cost analysis and optimization"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"governance",children:"Governance"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Establish clear data ownership"}),"\n",(0,i.jsx)(n.li,{children:"Implement data quality standards"}),"\n",(0,i.jsx)(n.li,{children:"Create comprehensive data documentation"}),"\n",(0,i.jsx)(n.li,{children:"Regular compliance audits"}),"\n",(0,i.jsx)(n.li,{children:"Automated policy enforcement"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"monitoring-and-troubleshooting",children:"Monitoring and Troubleshooting"}),"\n",(0,i.jsx)(n.h3,{id:"key-metrics",children:"Key Metrics"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Data access patterns and frequency"}),"\n",(0,i.jsx)(n.li,{children:"Permission grant/revoke activities"}),"\n",(0,i.jsx)(n.li,{children:"Data quality metrics"}),"\n",(0,i.jsx)(n.li,{children:"Storage utilization and costs"}),"\n",(0,i.jsx)(n.li,{children:"Query performance metrics"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Permission denied errors"}),"\n",(0,i.jsx)(n.li,{children:"Slow query performance"}),"\n",(0,i.jsx)(n.li,{children:"Data quality issues"}),"\n",(0,i.jsx)(n.li,{children:"Cross-account access problems"}),"\n",(0,i.jsx)(n.li,{children:"Compliance violations"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"troubleshooting-steps",children:"Troubleshooting Steps"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Check Lake Formation permissions"}),"\n",(0,i.jsx)(n.li,{children:"Verify IAM roles and policies"}),"\n",(0,i.jsx)(n.li,{children:"Review CloudTrail logs"}),"\n",(0,i.jsx)(n.li,{children:"Validate data formats and schemas"}),"\n",(0,i.jsx)(n.li,{children:"Monitor resource utilization"}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},36095:(e,n,a)=>{a.d(n,{A:()=>s});const s=a.p+"assets/images/lake-formation-03ab563cfa6fc06bc6dd0784a0a10ad8.png"},65404:(e,n,a)=>{a.d(n,{R:()=>r,x:()=>l});var s=a(36672);const i={},t=s.createContext(i);function r(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);