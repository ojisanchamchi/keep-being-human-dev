"use strict";(self.webpackChunkkeep_being_human_dev=self.webpackChunkkeep_being_human_dev||[]).push([[94274],{65404:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>o});var s=i(36672);const a={},t=s.createContext(a);function r(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),s.createElement(t.Provider,{value:n},e.children)}},65596:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/kinesis-7e89564af753fd48d26278ee8d6b0450.png"},80909:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>r,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"aws-nodes/aws-analytics/kinesis","title":"Amazon Kinesis","description":"T\u1ed5ng quan","source":"@site/docs/aws-nodes/aws-analytics/23-kinesis.md","sourceDirName":"aws-nodes/aws-analytics","slug":"/aws-nodes/aws-analytics/kinesis","permalink":"/keep-being-human-dev/docs/aws-nodes/aws-analytics/kinesis","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/aws-nodes/aws-analytics/23-kinesis.md","tags":[],"version":"current","sidebarPosition":23,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Amazon Kinesis Video Streams","permalink":"/keep-being-human-dev/docs/aws-nodes/aws-analytics/kinesis-video-streams"},"next":{"title":"AWS Lake Formation","permalink":"/keep-being-human-dev/docs/aws-nodes/aws-analytics/lake-formation"}}');var a=i(23420),t=i(65404);const r={},o="Amazon Kinesis",l={},c=[{value:"T\u1ed5ng quan",id:"t\u1ed5ng-quan",level:2},{value:"Ch\u1ee9c n\u0103ng ch\xednh",id:"ch\u1ee9c-n\u0103ng-ch\xednh",level:2},{value:"1. Real-time Data Streaming",id:"1-real-time-data-streaming",level:3},{value:"2. Multiple Service Options",id:"2-multiple-service-options",level:3},{value:"3. Processing Flexibility",id:"3-processing-flexibility",level:3},{value:"4. Integration Ecosystem",id:"4-integration-ecosystem",level:3},{value:"Use Cases ph\u1ed5 bi\u1ebfn",id:"use-cases-ph\u1ed5-bi\u1ebfn",level:2},{value:"Diagram Architecture",id:"diagram-architecture",level:2},{value:"Code \u0111\u1ec3 t\u1ea1o diagram:",id:"code-\u0111\u1ec3-t\u1ea1o-diagram",level:3},{value:"Kinesis Services Overview",id:"kinesis-services-overview",level:2},{value:"1. Kinesis Data Streams",id:"1-kinesis-data-streams",level:3},{value:"2. Kinesis Data Firehose",id:"2-kinesis-data-firehose",level:3},{value:"3. Kinesis Data Analytics",id:"3-kinesis-data-analytics",level:3},{value:"4. Kinesis Video Streams",id:"4-kinesis-video-streams",level:3},{value:"End-to-End Pipeline Implementation",id:"end-to-end-pipeline-implementation",level:2},{value:"1. Complete Streaming Pipeline",id:"1-complete-streaming-pipeline",level:3},{value:"Cost Optimization Strategies",id:"cost-optimization-strategies",level:2},{value:"1. Multi-Service Cost Analysis",id:"1-multi-service-cost-analysis",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"T\xedch h\u1ee3p v\u1edbi c\xe1c d\u1ecbch v\u1ee5 AWS kh\xe1c",id:"t\xedch-h\u1ee3p-v\u1edbi-c\xe1c-d\u1ecbch-v\u1ee5-aws-kh\xe1c",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"amazon-kinesis",children:"Amazon Kinesis"})}),"\n",(0,a.jsx)(n.h2,{id:"t\u1ed5ng-quan",children:"T\u1ed5ng quan"}),"\n",(0,a.jsx)(n.p,{children:"Amazon Kinesis l\xe0 node t\u1ed5ng qu\xe1t \u0111\u1ea1i di\u1ec7n cho family of services c\u1ee7a AWS d\xe0nh cho real-time data streaming v\xe0 analytics. Kinesis gi\xfap b\u1ea1n d\u1ec5 d\xe0ng collect, process, v\xe0 analyze real-time streaming data \u0111\u1ec3 c\xf3 th\u1ec3 react nhanh ch\xf3ng v\xe0 make informed decisions t\u1eeb incoming information. Platform n\xe0y bao g\u1ed3m nhi\u1ec1u services kh\xe1c nhau \u0111\u1ec3 handle different aspects c\u1ee7a streaming data pipeline."}),"\n",(0,a.jsx)(n.h2,{id:"ch\u1ee9c-n\u0103ng-ch\xednh",children:"Ch\u1ee9c n\u0103ng ch\xednh"}),"\n",(0,a.jsx)(n.h3,{id:"1-real-time-data-streaming",children:"1. Real-time Data Streaming"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"High Throughput"}),": X\u1eed l\xfd terabytes of data per hour"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Low Latency"}),": Sub-second processing latency"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Scalable"}),": Auto-scale theo data volume"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Durable"}),": Reliable data delivery v\xe0 storage"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"2-multiple-service-options",children:"2. Multiple Service Options"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Kinesis Data Streams"}),": Core streaming platform"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Kinesis Data Firehose"}),": Managed data delivery"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Kinesis Data Analytics"}),": Real-time analytics"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Kinesis Video Streams"}),": Video streaming platform"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"3-processing-flexibility",children:"3. Processing Flexibility"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Real-time Processing"}),": Immediate data processing"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Batch Processing"}),": Scheduled data processing"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Stream Analytics"}),": SQL-based analytics"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Custom Applications"}),": SDK-based development"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"4-integration-ecosystem",children:"4. Integration Ecosystem"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"AWS Services"}),": Native integration v\u1edbi AWS services"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Third-party Tools"}),": Integration v\u1edbi external tools"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"APIs v\xe0 SDKs"}),": Comprehensive development support"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Monitoring"}),": Built-in monitoring v\xe0 alerting"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"use-cases-ph\u1ed5-bi\u1ebfn",children:"Use Cases ph\u1ed5 bi\u1ebfn"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Real-time Analytics"}),": Live dashboards v\xe0 metrics"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"IoT Data Processing"}),": Sensor data streams"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Log Aggregation"}),": Centralized logging"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Fraud Detection"}),": Real-time fraud prevention"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Clickstream Analysis"}),": User behavior tracking"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"diagram-architecture",children:"Diagram Architecture"}),"\n",(0,a.jsx)(n.p,{children:"Ki\u1ebfn tr\xfac t\u1ed5ng quan Amazon Kinesis ecosystem:"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Amazon Kinesis Architecture",src:i(65596).A+"",width:"2464",height:"1794"})}),"\n",(0,a.jsx)(n.h3,{id:"code-\u0111\u1ec3-t\u1ea1o-diagram",children:"Code \u0111\u1ec3 t\u1ea1o diagram:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.analytics import Kinesis, KinesisDataStreams, KinesisDataFirehose, KinesisDataAnalytics, KinesisVideoStreams\nfrom diagrams.aws.storage import S3\nfrom diagrams.aws.compute import Lambda, EC2\nfrom diagrams.aws.database import Dynamodb, RDS, Redshift\nfrom diagrams.aws.ml import Rekognition, SagemakerModel\nfrom diagrams.aws.integration import SQS, SNS\nfrom diagrams.aws.management import Cloudwatch\nfrom diagrams.aws.security import IAM\nfrom diagrams.onprem.client import Users\nfrom diagrams.onprem.iot import IotSensor\n\nwith Diagram("Amazon Kinesis Architecture", show=False, direction="TB"):\n    \n    users = Users("Applications & Users")\n    \n    with Cluster("Data Sources"):\n        web_apps = Lambda("Web Applications")\n        mobile_apps = Lambda("Mobile Apps")\n        iot_devices = IotSensor("IoT Devices")\n        databases = RDS("Databases")\n        video_sources = Lambda("Video Sources")\n    \n    with Cluster("Amazon Kinesis Platform"):\n        kinesis_platform = Kinesis("Amazon Kinesis")\n        \n        with Cluster("Kinesis Services"):\n            data_streams = KinesisDataStreams("Data Streams")\n            data_firehose = KinesisDataFirehose("Data Firehose")\n            data_analytics = KinesisDataAnalytics("Data Analytics")\n            video_streams = KinesisVideoStreams("Video Streams")\n    \n    with Cluster("Stream Processing"):\n        lambda_processor = Lambda("Lambda Functions")\n        custom_apps = EC2("Custom Applications")\n        analytics_apps = Lambda("Analytics Apps")\n        ml_inference = SagemakerModel("ML Inference")\n    \n    with Cluster("Data Destinations"):\n        s3_data_lake = S3("Data Lake")\n        redshift_dw = Redshift("Data Warehouse")\n        dynamodb_table = Dynamodb("Real-time DB")\n        opensearch = Lambda("OpenSearch")\n    \n    with Cluster("Analytics & Insights"):\n        real_time_dashboard = Lambda("Real-time Dashboards")\n        bi_tools = Lambda("BI Tools")\n        ml_models = SagemakerModel("ML Models")\n        alerts = SNS("Alert System")\n    \n    with Cluster("Video Processing"):\n        rekognition = Rekognition("Video Analysis")\n        video_archive = S3("Video Archive")\n        live_streaming = Lambda("Live Streaming")\n    \n    with Cluster("Monitoring & Management"):\n        cloudwatch = Cloudwatch("CloudWatch")\n        security = IAM("IAM & Security")\n    \n    # Data ingestion\n    web_apps >> Edge(label="Events") >> data_streams\n    mobile_apps >> Edge(label="Analytics") >> data_streams\n    iot_devices >> Edge(label="Sensor Data") >> data_streams\n    databases >> Edge(label="CDC") >> data_streams\n    video_sources >> Edge(label="Video Feed") >> video_streams\n    \n    # Kinesis platform\n    kinesis_platform >> data_streams\n    kinesis_platform >> data_firehose\n    kinesis_platform >> data_analytics\n    kinesis_platform >> video_streams\n    \n    # Stream processing\n    data_streams >> lambda_processor\n    data_streams >> custom_apps\n    data_analytics >> analytics_apps\n    video_streams >> rekognition\n    \n    # Data delivery\n    data_firehose >> s3_data_lake\n    data_firehose >> redshift_dw\n    lambda_processor >> dynamodb_table\n    analytics_apps >> opensearch\n    \n    # Analytics and insights\n    dynamodb_table >> real_time_dashboard\n    s3_data_lake >> bi_tools\n    opensearch >> ml_models\n    rekognition >> alerts\n    \n    # Video processing\n    video_streams >> video_archive\n    video_streams >> live_streaming\n    \n    # User interaction\n    users >> Edge(label="View Analytics") >> real_time_dashboard\n    users >> Edge(label="BI Reports") >> bi_tools\n    users >> Edge(label="Live Video") >> live_streaming\n    \n    # Monitoring\n    kinesis_platform >> cloudwatch\n    security >> kinesis_platform\n'})}),"\n",(0,a.jsx)(n.h2,{id:"kinesis-services-overview",children:"Kinesis Services Overview"}),"\n",(0,a.jsx)(n.h3,{id:"1-kinesis-data-streams",children:"1. Kinesis Data Streams"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# Core streaming service for real-time data\nclass KinesisDataStreamsManager:\n    def __init__(self):\n        self.kinesis = boto3.client('kinesis')\n    \n    def create_stream_architecture(self, stream_config):\n        \"\"\"Create complete streaming architecture\"\"\"\n        \n        # Create main data stream\n        main_stream = self.kinesis.create_stream(\n            StreamName=stream_config['name'],\n            ShardCount=stream_config['shard_count']\n        )\n        \n        # Create consumer applications\n        consumers = []\n        for consumer_config in stream_config['consumers']:\n            consumer = self.create_consumer_application(\n                stream_config['name'],\n                consumer_config\n            )\n            consumers.append(consumer)\n        \n        return {\n            'stream': main_stream,\n            'consumers': consumers\n        }\n    \n    def create_consumer_application(self, stream_name, consumer_config):\n        \"\"\"Create consumer application\"\"\"\n        \n        if consumer_config['type'] == 'lambda':\n            return self.create_lambda_consumer(stream_name, consumer_config)\n        elif consumer_config['type'] == 'kcl':\n            return self.create_kcl_consumer(stream_name, consumer_config)\n        elif consumer_config['type'] == 'kda':\n            return self.create_kda_consumer(stream_name, consumer_config)\n"})}),"\n",(0,a.jsx)(n.h3,{id:"2-kinesis-data-firehose",children:"2. Kinesis Data Firehose"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# Managed delivery service\nclass KinesisDataFirehoseManager:\n    def __init__(self):\n        self.firehose = boto3.client('firehose')\n    \n    def create_delivery_pipeline(self, pipeline_config):\n        \"\"\"Create complete delivery pipeline\"\"\"\n        \n        pipelines = []\n        \n        for destination in pipeline_config['destinations']:\n            if destination['type'] == 's3':\n                pipeline = self.create_s3_delivery_stream(destination)\n            elif destination['type'] == 'redshift':\n                pipeline = self.create_redshift_delivery_stream(destination)\n            elif destination['type'] == 'opensearch':\n                pipeline = self.create_opensearch_delivery_stream(destination)\n            \n            pipelines.append(pipeline)\n        \n        return pipelines\n    \n    def create_s3_delivery_stream(self, config):\n        \"\"\"Create S3 delivery stream with transformation\"\"\"\n        \n        return self.firehose.create_delivery_stream(\n            DeliveryStreamName=config['name'],\n            DeliveryStreamType='DirectPut',\n            ExtendedS3DestinationConfiguration={\n                'RoleARN': config['role_arn'],\n                'BucketARN': config['bucket_arn'],\n                'Prefix': config.get('prefix', ''),\n                'BufferingHints': {\n                    'SizeInMBs': config.get('buffer_size', 5),\n                    'IntervalInSeconds': config.get('buffer_interval', 300)\n                },\n                'CompressionFormat': config.get('compression', 'GZIP'),\n                'ProcessingConfiguration': {\n                    'Enabled': True,\n                    'Processors': [\n                        {\n                            'Type': 'Lambda',\n                            'Parameters': [\n                                {\n                                    'ParameterName': 'LambdaArn',\n                                    'ParameterValue': config['transform_lambda_arn']\n                                }\n                            ]\n                        }\n                    ]\n                } if config.get('transform_lambda_arn') else {'Enabled': False}\n            }\n        )\n"})}),"\n",(0,a.jsx)(n.h3,{id:"3-kinesis-data-analytics",children:"3. Kinesis Data Analytics"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# Real-time analytics service\nclass KinesisDataAnalyticsManager:\n    def __init__(self):\n        self.kda = boto3.client('kinesisanalyticsv2')\n    \n    def create_analytics_application(self, app_config):\n        \"\"\"Create Kinesis Data Analytics application\"\"\"\n        \n        if app_config['runtime'] == 'SQL':\n            return self.create_sql_application(app_config)\n        elif app_config['runtime'] == 'FLINK':\n            return self.create_flink_application(app_config)\n    \n    def create_sql_application(self, config):\n        \"\"\"Create SQL-based analytics application\"\"\"\n        \n        return self.kda.create_application(\n            ApplicationName=config['name'],\n            ApplicationDescription=config['description'],\n            RuntimeEnvironment='SQL-1_0',\n            ServiceExecutionRole=config['service_role'],\n            ApplicationConfiguration={\n                'SqlApplicationConfiguration': {\n                    'Inputs': [\n                        {\n                            'NamePrefix': 'SOURCE_SQL_STREAM',\n                            'KinesisStreamsInput': {\n                                'ResourceARN': config['input_stream_arn']\n                            },\n                            'InputSchema': {\n                                'RecordFormat': {\n                                    'RecordFormatType': 'JSON',\n                                    'MappingParameters': {\n                                        'JSONMappingParameters': {\n                                            'RecordRowPath': '$'\n                                        }\n                                    }\n                                },\n                                'RecordColumns': config['input_schema']\n                            }\n                        }\n                    ],\n                    'Outputs': [\n                        {\n                            'Name': 'DESTINATION_SQL_STREAM',\n                            'KinesisStreamsOutput': {\n                                'ResourceARN': config['output_stream_arn']\n                            },\n                            'DestinationSchema': {\n                                'RecordFormatType': 'JSON'\n                            }\n                        }\n                    ]\n                }\n            }\n        )\n    \n    def create_flink_application(self, config):\n        \"\"\"Create Flink-based analytics application\"\"\"\n        \n        return self.kda.create_application(\n            ApplicationName=config['name'],\n            RuntimeEnvironment='FLINK-1_15',\n            ServiceExecutionRole=config['service_role'],\n            ApplicationConfiguration={\n                'FlinkApplicationConfiguration': {\n                    'CheckpointConfiguration': {\n                        'ConfigurationType': 'CUSTOM',\n                        'CheckpointingEnabled': True,\n                        'CheckpointInterval': 60000,\n                        'MinPauseBetweenCheckpoints': 5000\n                    },\n                    'MonitoringConfiguration': {\n                        'ConfigurationType': 'CUSTOM',\n                        'MetricsLevel': 'APPLICATION',\n                        'LogLevel': 'INFO'\n                    },\n                    'ParallelismConfiguration': {\n                        'ConfigurationType': 'CUSTOM',\n                        'Parallelism': config.get('parallelism', 1),\n                        'ParallelismPerKPU': 1,\n                        'AutoScalingEnabled': True\n                    }\n                },\n                'ApplicationCodeConfiguration': {\n                    'CodeContent': {\n                        'S3ContentLocation': {\n                            'BucketARN': config['code_bucket_arn'],\n                            'FileKey': config['code_file_key']\n                        }\n                    },\n                    'CodeContentType': 'ZIPFILE'\n                }\n            }\n        )\n"})}),"\n",(0,a.jsx)(n.h3,{id:"4-kinesis-video-streams",children:"4. Kinesis Video Streams"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# Video streaming service\nclass KinesisVideoStreamsManager:\n    def __init__(self):\n        self.kvs = boto3.client('kinesisvideo')\n    \n    def create_video_analytics_pipeline(self, pipeline_config):\n        \"\"\"Create complete video analytics pipeline\"\"\"\n        \n        # Create video stream\n        stream = self.kvs.create_stream(\n            StreamName=pipeline_config['stream_name'],\n            DataRetentionInHours=pipeline_config.get('retention_hours', 24),\n            MediaType='video/h264'\n        )\n        \n        # Set up video analysis\n        if pipeline_config.get('enable_rekognition'):\n            self.setup_rekognition_analysis(\n                pipeline_config['stream_name'],\n                pipeline_config['rekognition_config']\n            )\n        \n        # Set up custom processing\n        if pipeline_config.get('custom_processor_lambda'):\n            self.setup_custom_processing(\n                pipeline_config['stream_name'],\n                pipeline_config['custom_processor_lambda']\n            )\n        \n        return stream\n    \n    def setup_rekognition_analysis(self, stream_name, rekognition_config):\n        \"\"\"Set up Rekognition video analysis\"\"\"\n        \n        rekognition = boto3.client('rekognition')\n        \n        # Start face detection\n        if rekognition_config.get('face_detection'):\n            rekognition.start_face_detection(\n                Video={\n                    'KinesisVideoStream': {\n                        'Arn': f'arn:aws:kinesisvideo:region:account:stream/{stream_name}'\n                    }\n                },\n                NotificationChannel={\n                    'SNSTopicArn': rekognition_config['sns_topic_arn'],\n                    'RoleArn': rekognition_config['service_role_arn']\n                }\n            )\n        \n        # Start label detection\n        if rekognition_config.get('label_detection'):\n            rekognition.start_label_detection(\n                Video={\n                    'KinesisVideoStream': {\n                        'Arn': f'arn:aws:kinesisvideo:region:account:stream/{stream_name}'\n                    }\n                },\n                MinConfidence=rekognition_config.get('min_confidence', 80),\n                NotificationChannel={\n                    'SNSTopicArn': rekognition_config['sns_topic_arn'],\n                    'RoleArn': rekognition_config['service_role_arn']\n                }\n            )\n"})}),"\n",(0,a.jsx)(n.h2,{id:"end-to-end-pipeline-implementation",children:"End-to-End Pipeline Implementation"}),"\n",(0,a.jsx)(n.h3,{id:"1-complete-streaming-pipeline",children:"1. Complete Streaming Pipeline"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class KinesisStreamingPipeline:\n    def __init__(self, pipeline_name):\n        self.pipeline_name = pipeline_name\n        self.kinesis = boto3.client(\'kinesis\')\n        self.firehose = boto3.client(\'firehose\')\n        self.kda = boto3.client(\'kinesisanalyticsv2\')\n        \n    def deploy_complete_pipeline(self, config):\n        """Deploy complete streaming pipeline"""\n        \n        pipeline_resources = {}\n        \n        # 1. Create data streams\n        if config.get(\'data_streams\'):\n            pipeline_resources[\'data_streams\'] = self.create_data_streams(\n                config[\'data_streams\']\n            )\n        \n        # 2. Create analytics applications\n        if config.get(\'analytics_apps\'):\n            pipeline_resources[\'analytics_apps\'] = self.create_analytics_apps(\n                config[\'analytics_apps\']\n            )\n        \n        # 3. Create delivery streams\n        if config.get(\'delivery_streams\'):\n            pipeline_resources[\'delivery_streams\'] = self.create_delivery_streams(\n                config[\'delivery_streams\']\n            )\n        \n        # 4. Set up monitoring\n        pipeline_resources[\'monitoring\'] = self.setup_monitoring(config)\n        \n        # 5. Configure auto-scaling\n        if config.get(\'auto_scaling\'):\n            pipeline_resources[\'auto_scaling\'] = self.setup_auto_scaling(\n                config[\'auto_scaling\']\n            )\n        \n        return pipeline_resources\n    \n    def create_data_streams(self, streams_config):\n        """Create multiple data streams"""\n        \n        streams = {}\n        \n        for stream_config in streams_config:\n            stream = self.kinesis.create_stream(\n                StreamName=f"{self.pipeline_name}-{stream_config[\'name\']}",\n                ShardCount=stream_config[\'shard_count\']\n            )\n            \n            # Configure retention\n            if stream_config.get(\'retention_hours\'):\n                self.kinesis.increase_stream_retention_period(\n                    StreamName=f"{self.pipeline_name}-{stream_config[\'name\']}",\n                    RetentionPeriodHours=stream_config[\'retention_hours\']\n                )\n            \n            # Enable encryption\n            if stream_config.get(\'encryption_key\'):\n                self.kinesis.enable_stream_encryption(\n                    StreamName=f"{self.pipeline_name}-{stream_config[\'name\']}",\n                    EncryptionType=\'KMS\',\n                    KeyId=stream_config[\'encryption_key\']\n                )\n            \n            streams[stream_config[\'name\']] = stream\n        \n        return streams\n    \n    def setup_monitoring(self, config):\n        """Set up comprehensive monitoring"""\n        \n        cloudwatch = boto3.client(\'cloudwatch\')\n        \n        # Create custom dashboard\n        dashboard_body = {\n            "widgets": [\n                {\n                    "type": "metric",\n                    "properties": {\n                        "metrics": [\n                            ["AWS/Kinesis", "IncomingRecords", "StreamName", f"{self.pipeline_name}-main"],\n                            [".", "IncomingBytes", ".", "."],\n                            [".", "OutgoingRecords", ".", "."]\n                        ],\n                        "period": 300,\n                        "stat": "Sum",\n                        "region": "us-west-2",\n                        "title": "Stream Throughput"\n                    }\n                },\n                {\n                    "type": "metric",\n                    "properties": {\n                        "metrics": [\n                            ["AWS/KinesisFirehose", "DeliveryToS3.Success", "DeliveryStreamName", f"{self.pipeline_name}-delivery"],\n                            [".", "DeliveryToS3.DataFreshness", ".", "."]\n                        ],\n                        "period": 300,\n                        "stat": "Average",\n                        "region": "us-west-2",\n                        "title": "Delivery Performance"\n                    }\n                }\n            ]\n        }\n        \n        cloudwatch.put_dashboard(\n            DashboardName=f\'{self.pipeline_name}-monitoring\',\n            DashboardBody=json.dumps(dashboard_body)\n        )\n        \n        # Create alarms\n        alarms = [\n            {\n                \'AlarmName\': f\'{self.pipeline_name}-high-incoming-records\',\n                \'MetricName\': \'IncomingRecords\',\n                \'Threshold\': 10000,\n                \'ComparisonOperator\': \'GreaterThanThreshold\'\n            },\n            {\n                \'AlarmName\': f\'{self.pipeline_name}-delivery-failures\',\n                \'MetricName\': \'DeliveryToS3.Success\',\n                \'Threshold\': 0.95,\n                \'ComparisonOperator\': \'LessThanThreshold\'\n            }\n        ]\n        \n        for alarm in alarms:\n            cloudwatch.put_metric_alarm(\n                AlarmName=alarm[\'AlarmName\'],\n                ComparisonOperator=alarm[\'ComparisonOperator\'],\n                EvaluationPeriods=2,\n                MetricName=alarm[\'MetricName\'],\n                Namespace=\'AWS/Kinesis\',\n                Period=300,\n                Statistic=\'Average\',\n                Threshold=alarm[\'Threshold\'],\n                ActionsEnabled=True,\n                AlarmActions=[\n                    config.get(\'sns_topic_arn\', \'arn:aws:sns:region:account:kinesis-alerts\')\n                ]\n            )\n        \n        return {\'dashboard\': f\'{self.pipeline_name}-monitoring\', \'alarms\': alarms}\n'})}),"\n",(0,a.jsx)(n.h2,{id:"cost-optimization-strategies",children:"Cost Optimization Strategies"}),"\n",(0,a.jsx)(n.h3,{id:"1-multi-service-cost-analysis",children:"1. Multi-Service Cost Analysis"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"class KinesisCostOptimizer:\n    def __init__(self):\n        self.ce = boto3.client('ce')\n        self.cloudwatch = boto3.client('cloudwatch')\n    \n    def analyze_kinesis_costs(self, start_date, end_date):\n        \"\"\"Analyze costs across all Kinesis services\"\"\"\n        \n        # Get cost breakdown by service\n        response = self.ce.get_cost_and_usage(\n            TimePeriod={\n                'Start': start_date,\n                'End': end_date\n            },\n            Granularity='DAILY',\n            Metrics=['BlendedCost'],\n            GroupBy=[\n                {\n                    'Type': 'DIMENSION',\n                    'Key': 'SERVICE'\n                }\n            ],\n            Filter={\n                'Dimensions': {\n                    'Key': 'SERVICE',\n                    'Values': [\n                        'Amazon Kinesis',\n                        'Amazon Kinesis Analytics',\n                        'Amazon Kinesis Firehose',\n                        'Amazon Kinesis Video Streams'\n                    ]\n                }\n            }\n        )\n        \n        cost_breakdown = {}\n        for result in response['ResultsByTime']:\n            date = result['TimePeriod']['Start']\n            cost_breakdown[date] = {}\n            \n            for group in result['Groups']:\n                service = group['Keys'][0]\n                cost = float(group['Metrics']['BlendedCost']['Amount'])\n                cost_breakdown[date][service] = cost\n        \n        return cost_breakdown\n    \n    def optimize_shard_count(self, stream_name):\n        \"\"\"Optimize shard count based on usage patterns\"\"\"\n        \n        # Get stream metrics\n        metrics = self.get_stream_utilization_metrics(stream_name)\n        \n        # Calculate optimal shard count\n        avg_throughput_mb = metrics['avg_incoming_bytes'] / (1024 * 1024)  # Convert to MB\n        avg_records_per_sec = metrics['avg_incoming_records'] / 60  # Convert to per second\n        \n        # Each shard can handle 1MB/sec or 1000 records/sec\n        shards_needed_for_bytes = math.ceil(avg_throughput_mb)\n        shards_needed_for_records = math.ceil(avg_records_per_sec / 1000)\n        \n        optimal_shards = max(shards_needed_for_bytes, shards_needed_for_records)\n        \n        # Add 20% buffer for spikes\n        recommended_shards = math.ceil(optimal_shards * 1.2)\n        \n        return {\n            'current_metrics': metrics,\n            'optimal_shards': optimal_shards,\n            'recommended_shards': recommended_shards,\n            'potential_savings': self.calculate_shard_savings(\n                metrics['current_shards'], \n                recommended_shards\n            )\n        }\n    \n    def recommend_on_demand_vs_provisioned(self, stream_usage_pattern):\n        \"\"\"Recommend between on-demand and provisioned capacity\"\"\"\n        \n        # Calculate costs for both models\n        provisioned_cost = self.calculate_provisioned_cost(stream_usage_pattern)\n        on_demand_cost = self.calculate_on_demand_cost(stream_usage_pattern)\n        \n        recommendation = {\n            'provisioned_cost': provisioned_cost,\n            'on_demand_cost': on_demand_cost,\n            'recommended_mode': 'on-demand' if on_demand_cost < provisioned_cost else 'provisioned',\n            'potential_savings': abs(provisioned_cost - on_demand_cost),\n            'reasoning': self.generate_recommendation_reasoning(\n                stream_usage_pattern, provisioned_cost, on_demand_cost\n            )\n        }\n        \n        return recommendation\n"})}),"\n",(0,a.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Service Selection"}),": Ch\u1ecdn Kinesis service ph\xf9 h\u1ee3p cho use case"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Capacity Planning"}),": Right-size shards v\xe0 throughput"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Cost Optimization"}),": Monitor v\xe0 optimize costs regularly"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Security"}),": Implement encryption v\xe0 proper IAM roles"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Monitoring"}),": Set up comprehensive monitoring"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Error Handling"}),": Implement robust error handling"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Data Retention"}),": Set appropriate retention periods"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Integration"}),": Leverage AWS ecosystem effectively"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"t\xedch-h\u1ee3p-v\u1edbi-c\xe1c-d\u1ecbch-v\u1ee5-aws-kh\xe1c",children:"T\xedch h\u1ee3p v\u1edbi c\xe1c d\u1ecbch v\u1ee5 AWS kh\xe1c"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"AWS Lambda"}),": Serverless processing"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Amazon S3"}),": Data storage v\xe0 archival"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Amazon DynamoDB"}),": Real-time data storage"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Amazon Redshift"}),": Data warehousing"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Amazon OpenSearch"}),": Search v\xe0 analytics"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Amazon SageMaker"}),": Machine learning"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Amazon Rekognition"}),": Video analysis"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Amazon CloudWatch"}),": Monitoring v\xe0 alerting"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"AWS IAM"}),": Security v\xe0 access control"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Amazon VPC"}),": Network isolation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"AWS Glue"}),": Data catalog v\xe0 ETL"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Amazon SNS"}),": Notifications"]}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}}}]);