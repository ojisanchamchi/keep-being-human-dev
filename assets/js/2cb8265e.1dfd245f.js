"use strict";(self.webpackChunkkeep_being_human_dev=self.webpackChunkkeep_being_human_dev||[]).push([[6690],{36670:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>d,contentTitle:()=>i,default:()=>u,frontMatter:()=>o,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"ruby/json/middle/streaming_json_parsing","title":"streaming_json_parsing","description":"\ud83d\udce5 Stream Large JSON Payloads Efficiently","source":"@site/docs/ruby/json/middle/streaming_json_parsing.md","sourceDirName":"ruby/json/middle","slug":"/ruby/json/middle/streaming_json_parsing","permalink":"/keep-being-human-dev/docs/ruby/json/middle/streaming_json_parsing","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/ruby/json/middle/streaming_json_parsing.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"json_schema_validation","permalink":"/keep-being-human-dev/docs/ruby/json/middle/json_schema_validation"},"next":{"title":"custom_log_formatter_with_metadata","permalink":"/keep-being-human-dev/docs/ruby/logging/advanced/custom_log_formatter_with_metadata"}}');var s=a(23420),t=a(65404);const o={},i=void 0,d={},l=[{value:"\ud83d\udce5 Stream Large JSON Payloads Efficiently",id:"-stream-large-json-payloads-efficiently",level:2}];function c(e){const n={code:"code",h2:"h2",p:"p",pre:"pre",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h2,{id:"-stream-large-json-payloads-efficiently",children:"\ud83d\udce5 Stream Large JSON Payloads Efficiently"}),"\n",(0,s.jsxs)(n.p,{children:["When working with big JSON files, loading the entire content into memory can cause performance bottlenecks or even OOM errors. By using ",(0,s.jsx)(n.code,{children:"JSON.load"})," on an IO object or a streaming parser like ",(0,s.jsx)(n.code,{children:"Oj"}),", you can process chunks incrementally without blowing up memory usage."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-ruby",children:"# Using stdlib JSON to stream from a file\nFile.open('huge_data.json') do |file|\n  JSON.load(file) do |obj|\n    # process each top-level object or array element\n    handle_record(obj)\n  end\nend\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-ruby",children:"# Using Oj for event-based streaming\nrequire 'oj'\nhandler = Class.new do\n  def initialize; end\n  def hash_start; puts \"New object:\"; end\n  def hash_end; end\n  def array_start; end\n  def array_end; end\n  def add_value(value); handle_record(value); end\nend.new\n\nOj.sc_parse(handler, File.read('huge_data.json'))\n"})}),"\n",(0,s.jsx)(n.p,{children:"This approach lets you parse and process JSON incrementally, reducing peak memory usage."})]})}function u(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},65404:(e,n,a)=>{a.d(n,{R:()=>o,x:()=>i});var r=a(36672);const s={},t=r.createContext(s);function o(e){const n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),r.createElement(t.Provider,{value:n},e.children)}}}]);