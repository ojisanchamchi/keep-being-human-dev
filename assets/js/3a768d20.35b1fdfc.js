"use strict";(self.webpackChunkkeep_being_human_dev=self.webpackChunkkeep_being_human_dev||[]).push([[28422],{42652:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>i,contentTitle:()=>a,default:()=>p,frontMatter:()=>o,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"ruby/csv/expert/csv_parallel_processing","title":"csv_parallel_processing","description":"\u26a1\ufe0f Parallelizing CSV Parsing with Concurrent Processing","source":"@site/docs/ruby/csv/expert/csv_parallel_processing.md","sourceDirName":"ruby/csv/expert","slug":"/ruby/csv/expert/csv_parallel_processing","permalink":"/keep-being-human-dev/docs/ruby/csv/expert/csv_parallel_processing","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/ruby/csv/expert/csv_parallel_processing.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"csv_lazy_streaming","permalink":"/keep-being-human-dev/docs/ruby/csv/expert/csv_lazy_streaming"},"next":{"title":"csv_random_access_indexing","permalink":"/keep-being-human-dev/docs/ruby/csv/expert/csv_random_access_indexing"}}');var t=r(23420),c=r(65404);const o={},a=void 0,i={},l=[{value:"\u26a1\ufe0f Parallelizing CSV Parsing with Concurrent Processing",id:"\ufe0f-parallelizing-csv-parsing-with-concurrent-processing",level:2}];function u(e){const n={code:"code",h2:"h2",p:"p",pre:"pre",...(0,c.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h2,{id:"\ufe0f-parallelizing-csv-parsing-with-concurrent-processing",children:"\u26a1\ufe0f Parallelizing CSV Parsing with Concurrent Processing"}),"\n",(0,t.jsxs)(n.p,{children:["Speed up large CSV workloads by distributing row chunks across threads using the ",(0,t.jsx)(n.code,{children:"concurrent-ruby"})," gem. Batch rows with ",(0,t.jsx)(n.code,{children:"Enumerable#each_slice"})," and post tasks to a thread pool, then wait for graceful shutdown."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-ruby",children:"require 'csv'\nrequire 'concurrent'\n\nfile = 'large.csv'\nthread_count = 4\nbatch_size = 1000\n\npool = Concurrent::FixedThreadPool.new(thread_count)\nCSV.foreach(file, headers: true).each_slice(batch_size) do |rows|\n  pool.post(rows) do |chunk|\n    chunk.each { |row| heavy_compute(row) }\n  end\nend\npool.shutdown\npool.wait_for_termination\n"})})]})}function p(e={}){const{wrapper:n}={...(0,c.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(u,{...e})}):u(e)}},65404:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>a});var s=r(36672);const t={},c=s.createContext(t);function o(e){const n=s.useContext(c);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),s.createElement(c.Provider,{value:n},e.children)}}}]);