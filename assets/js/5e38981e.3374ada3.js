"use strict";(self.webpackChunkkeep_being_human_dev=self.webpackChunkkeep_being_human_dev||[]).push([[78505],{65404:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>c});var a=t(36672);const r={},s=a.createContext(r);function o(e){const n=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),a.createElement(s.Provider,{value:n},e.children)}},84303:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>i,contentTitle:()=>c,default:()=>l,frontMatter:()=>o,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"gems/ruby-openai/advanced/streaming_chat_responses","title":"streaming_chat_responses","description":"\ud83d\ude80 Streaming Chat Responses with Enumerator","source":"@site/docs/gems/ruby-openai/advanced/streaming_chat_responses.md","sourceDirName":"gems/ruby-openai/advanced","slug":"/gems/ruby-openai/advanced/streaming_chat_responses","permalink":"/keep-being-human-dev/docs/gems/ruby-openai/advanced/streaming_chat_responses","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/gems/ruby-openai/advanced/streaming_chat_responses.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"function_calling_advanced","permalink":"/keep-being-human-dev/docs/gems/ruby-openai/advanced/function_calling_advanced"},"next":{"title":"handle_errors_gracefully","permalink":"/keep-being-human-dev/docs/gems/ruby-openai/beginner/handle_errors_gracefully"}}');var r=t(23420),s=t(65404);const o={},c=void 0,i={},d=[{value:"\ud83d\ude80 Streaming Chat Responses with Enumerator",id:"-streaming-chat-responses-with-enumerator",level:2}];function u(e){const n={code:"code",h2:"h2",p:"p",pre:"pre",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h2,{id:"-streaming-chat-responses-with-enumerator",children:"\ud83d\ude80 Streaming Chat Responses with Enumerator"}),"\n",(0,r.jsxs)(n.p,{children:["Streaming responses allow you to process tokens as they arrive, reducing latency and providing real\u2011time interactivity. The ",(0,r.jsx)(n.code,{children:"ruby-openai"})," client exposes an ",(0,r.jsx)(n.code,{children:"stream"})," method that returns an Enumerator of chunks. You can ",(0,r.jsx)(n.code,{children:"each"})," over it to handle partial content, implement custom timeouts, and gracefully handle errors or cancellations."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-ruby",children:'require "ruby/openai"\n\nclient = OpenAI::Client.new(access_token: ENV.fetch("OPENAI_API_KEY"))\n\nstream = client.chat.completions(stream: true, parameters: {\n  model: "gpt-4o",\n  messages: [{ role: "user", content: "Generate a haiku about autumn." }]\n})\n\nstream.each do |chunk|\n  # Each chunk is a hash: { "choices" => [{ "delta" => { "content"=>"..." } }] }\n  text = chunk.dig("choices", 0, "delta", "content")\n  print text if text\nend\n'})}),"\n",(0,r.jsx)(n.p,{children:"By reading partial chunks, you can update UI elements in real time, abort streaming early when thresholds are met, or handle rate\u2011limit retries transparently."})]})}function l(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(u,{...e})}):u(e)}}}]);