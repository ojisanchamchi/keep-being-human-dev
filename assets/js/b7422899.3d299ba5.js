"use strict";(self.webpackChunkkeep_being_human_dev=self.webpackChunkkeep_being_human_dev||[]).push([[21332],{3076:(e,n,a)=>{a.d(n,{A:()=>t});const t=a.p+"assets/images/glue-data-catalog-b2299cf8bbe51a59994bd750b33542eb.png"},22477:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>m,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"diagrams/aws-nodes/aws-analytics/glue-data-catalog","title":"AWS Glue Data Catalog","description":"T\u1ed5ng quan","source":"@site/docs/diagrams/aws-nodes/aws-analytics/17-glue-data-catalog.md","sourceDirName":"diagrams/aws-nodes/aws-analytics","slug":"/diagrams/aws-nodes/aws-analytics/glue-data-catalog","permalink":"/keep-being-human-dev/docs/diagrams/aws-nodes/aws-analytics/glue-data-catalog","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/diagrams/aws-nodes/aws-analytics/17-glue-data-catalog.md","tags":[],"version":"current","sidebarPosition":17,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"AWS Glue Crawlers","permalink":"/keep-being-human-dev/docs/diagrams/aws-nodes/aws-analytics/glue-crawlers"},"next":{"title":"AWS Glue","permalink":"/keep-being-human-dev/docs/diagrams/aws-nodes/aws-analytics/glue"}}');var i=a(23420),s=a(65404);const r={},l="AWS Glue Data Catalog",o={},c=[{value:"T\u1ed5ng quan",id:"t\u1ed5ng-quan",level:2},{value:"Ch\u1ee9c n\u0103ng ch\xednh",id:"ch\u1ee9c-n\u0103ng-ch\xednh",level:2},{value:"1. Metadata Management",id:"1-metadata-management",level:3},{value:"2. Schema Evolution",id:"2-schema-evolution",level:3},{value:"3. Data Discovery",id:"3-data-discovery",level:3},{value:"4. Integration Hub",id:"4-integration-hub",level:3},{value:"Use Cases ph\u1ed5 bi\u1ebfn",id:"use-cases-ph\u1ed5-bi\u1ebfn",level:2},{value:"Diagram Architecture",id:"diagram-architecture",level:2},{value:"Code \u0111\u1ec3 t\u1ea1o diagram:",id:"code-\u0111\u1ec3-t\u1ea1o-diagram",level:3},{value:"Data Catalog Structure",id:"data-catalog-structure",level:2},{value:"1. Database Management",id:"1-database-management",level:3},{value:"2. Table Definition",id:"2-table-definition",level:3},{value:"3. Partition Management",id:"3-partition-management",level:3},{value:"Advanced Features",id:"advanced-features",level:2},{value:"1. Data Lineage Tracking",id:"1-data-lineage-tracking",level:3},{value:"2. Data Classification",id:"2-data-classification",level:3},{value:"3. Schema Evolution Management",id:"3-schema-evolution-management",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"1. Catalog Query Optimization",id:"1-catalog-query-optimization",level:3},{value:"Security v\xe0 Access Control",id:"security-v\xe0-access-control",level:2},{value:"1. Fine-grained Access Control",id:"1-fine-grained-access-control",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"T\xedch h\u1ee3p v\u1edbi c\xe1c d\u1ecbch v\u1ee5 AWS kh\xe1c",id:"t\xedch-h\u1ee3p-v\u1edbi-c\xe1c-d\u1ecbch-v\u1ee5-aws-kh\xe1c",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"aws-glue-data-catalog",children:"AWS Glue Data Catalog"})}),"\n",(0,i.jsx)(n.h2,{id:"t\u1ed5ng-quan",children:"T\u1ed5ng quan"}),"\n",(0,i.jsx)(n.p,{children:"AWS Glue Data Catalog l\xe0 node \u0111\u1ea1i di\u1ec7n cho metadata repository trung t\xe2m c\u1ee7a AWS Glue ecosystem. Data Catalog l\u01b0u tr\u1eef metadata v\u1ec1 data sources, transformations, v\xe0 targets, cung c\u1ea5p unified view c\u1ee7a d\u1eef li\u1ec7u across multiple data stores. \u0110\xe2y l\xe0 th\xe0nh ph\u1ea7n c\u1ed1t l\xf5i cho data discovery, schema management, v\xe0 ETL operations trong AWS analytics ecosystem."}),"\n",(0,i.jsx)(n.h2,{id:"ch\u1ee9c-n\u0103ng-ch\xednh",children:"Ch\u1ee9c n\u0103ng ch\xednh"}),"\n",(0,i.jsx)(n.h3,{id:"1-metadata-management",children:"1. Metadata Management"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Schema Storage"}),": L\u01b0u tr\u1eef table schemas v\xe0 column definitions"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Database Organization"}),": T\u1ed5 ch\u1ee9c tables th\xe0nh logical databases"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Partition Management"}),": Qu\u1ea3n l\xfd partition metadata"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Data Location"}),": Tracking physical location c\u1ee7a data"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"2-schema-evolution",children:"2. Schema Evolution"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Version Control"}),": Theo d\xf5i schema versions theo th\u1eddi gian"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Backward Compatibility"}),": Maintain compatibility v\u1edbi older schemas"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Change Detection"}),": T\u1ef1 \u0111\u1ed9ng ph\xe1t hi\u1ec7n schema changes"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Impact Analysis"}),": Ph\xe2n t\xedch impact c\u1ee7a schema changes"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"3-data-discovery",children:"3. Data Discovery"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Search Capabilities"}),": T\xecm ki\u1ebfm tables v\xe0 columns"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Data Lineage"}),": Theo d\xf5i data flow v\xe0 transformations"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Classification"}),": T\u1ef1 \u0111\u1ed9ng classify sensitive data"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Tagging"}),": Metadata tagging cho organization"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"4-integration-hub",children:"4. Integration Hub"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cross-Service"}),": T\xedch h\u1ee3p v\u1edbi Athena, EMR, Redshift, Spark"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"API Access"}),": RESTful APIs cho programmatic access"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Security Integration"}),": IAM-based access control"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Event Notifications"}),": CloudWatch events cho metadata changes"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"use-cases-ph\u1ed5-bi\u1ebfn",children:"Use Cases ph\u1ed5 bi\u1ebfn"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Centralized Metadata"}),": Single source of truth cho metadata"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Data Governance"}),": Implement data governance policies"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Self-Service Analytics"}),": Enable self-service data discovery"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ETL Automation"}),": Automate ETL job creation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Compliance Management"}),": Track sensitive data locations"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"diagram-architecture",children:"Diagram Architecture"}),"\n",(0,i.jsx)(n.p,{children:"Ki\u1ebfn tr\xfac AWS Glue Data Catalog ecosystem:"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"AWS Glue Data Catalog Architecture",src:a(3076).A+"",width:"2875",height:"1765"})}),"\n",(0,i.jsx)(n.h3,{id:"code-\u0111\u1ec3-t\u1ea1o-diagram",children:"Code \u0111\u1ec3 t\u1ea1o diagram:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.analytics import GlueDataCatalog, GlueCrawlers, Athena, EMR\nfrom diagrams.aws.storage import S3\nfrom diagrams.aws.compute import Lambda\nfrom diagrams.aws.database import RDS, Redshift, Dynamodb\nfrom diagrams.aws.integration import SQS, SNS\nfrom diagrams.aws.management import Cloudwatch\nfrom diagrams.aws.security import IAM\nfrom diagrams.onprem.client import Users\nfrom diagrams.onprem.analytics import Spark\n\nwith Diagram("AWS Glue Data Catalog Architecture", show=False, direction="TB"):\n    \n    users = Users("Data Analysts")\n    \n    with Cluster("Data Sources"):\n        s3_data_lake = S3("Data Lake")\n        rds_db = RDS("RDS Database")\n        redshift_dw = Redshift("Data Warehouse")\n        dynamodb_table = Dynamodb("DynamoDB")\n    \n    with Cluster("Metadata Discovery"):\n        crawlers = GlueCrawlers("Glue Crawlers")\n        manual_entry = Lambda("Manual Entry")\n        api_import = Lambda("API Import")\n    \n    with Cluster("Data Catalog Core"):\n        data_catalog = GlueDataCatalog("Glue Data Catalog")\n        \n        with Cluster("Catalog Structure"):\n            databases = GlueDataCatalog("Databases")\n            tables = GlueDataCatalog("Tables")\n            partitions = GlueDataCatalog("Partitions")\n            connections = GlueDataCatalog("Connections")\n    \n    with Cluster("Analytics Services"):\n        athena = Athena("Athena")\n        emr_cluster = EMR("EMR")\n        spark_jobs = Spark("Spark Jobs")\n        etl_jobs = Lambda("Glue ETL")\n    \n    with Cluster("Data Governance"):\n        data_lineage = GlueDataCatalog("Data Lineage")\n        classification = GlueDataCatalog("Data Classification")\n        security_tags = IAM("Security Tags")\n    \n    with Cluster("Monitoring & Events"):\n        cloudwatch = Cloudwatch("CloudWatch")\n        notifications = SNS("Notifications")\n        event_queue = SQS("Event Queue")\n    \n    # Metadata discovery flow\n    s3_data_lake >> crawlers\n    rds_db >> crawlers\n    redshift_dw >> crawlers\n    dynamodb_table >> crawlers\n    \n    crawlers >> Edge(label="Discover Schema") >> data_catalog\n    manual_entry >> Edge(label="Manual Input") >> data_catalog\n    api_import >> Edge(label="API Import") >> data_catalog\n    \n    # Catalog structure\n    data_catalog >> databases\n    data_catalog >> tables\n    data_catalog >> partitions\n    data_catalog >> connections\n    \n    # Analytics consumption\n    data_catalog >> Edge(label="Schema Info") >> athena\n    data_catalog >> Edge(label="Metadata") >> emr_cluster\n    data_catalog >> Edge(label="Table Definitions") >> spark_jobs\n    data_catalog >> Edge(label="ETL Metadata") >> etl_jobs\n    \n    # Data governance\n    data_catalog >> data_lineage\n    data_catalog >> classification\n    data_catalog >> security_tags\n    \n    # User interaction\n    users >> Edge(label="Search & Browse") >> data_catalog\n    users >> Edge(label="Query Data") >> athena\n    users >> Edge(label="Run Analytics") >> emr_cluster\n    \n    # Monitoring and events\n    data_catalog >> Edge(label="Metadata Events") >> cloudwatch\n    cloudwatch >> notifications\n    data_catalog >> event_queue\n'})}),"\n",(0,i.jsx)(n.h2,{id:"data-catalog-structure",children:"Data Catalog Structure"}),"\n",(0,i.jsx)(n.h3,{id:"1-database-management",children:"1. Database Management"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import boto3\n\nglue = boto3.client('glue')\n\n# Create database\ndatabase_config = {\n    'DatabaseInput': {\n        'Name': 'analytics_db',\n        'Description': 'Analytics database for data lake',\n        'LocationUri': 's3://data-lake/analytics/',\n        'Parameters': {\n            'classification': 'analytics',\n            'department': 'data-engineering',\n            'cost-center': 'analytics-team'\n        }\n    }\n}\n\nglue.create_database(**database_config)\n\n# Create multiple databases for organization\ndatabases = [\n    {\n        'Name': 'raw_data_db',\n        'Description': 'Raw data from various sources',\n        'LocationUri': 's3://data-lake/raw/'\n    },\n    {\n        'Name': 'processed_data_db', \n        'Description': 'Processed and cleaned data',\n        'LocationUri': 's3://data-lake/processed/'\n    },\n    {\n        'Name': 'analytics_db',\n        'Description': 'Analytics-ready datasets',\n        'LocationUri': 's3://data-lake/analytics/'\n    }\n]\n\nfor db in databases:\n    glue.create_database(DatabaseInput=db)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"2-table-definition",children:"2. Table Definition"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Create table with comprehensive metadata\ntable_config = {\n    'DatabaseName': 'analytics_db',\n    'TableInput': {\n        'Name': 'customer_events',\n        'Description': 'Customer interaction events',\n        'Owner': 'data-engineering-team',\n        'Parameters': {\n            'classification': 'parquet',\n            'compressionType': 'snappy',\n            'typeOfData': 'file',\n            'sizeKey': '1000000',\n            'recordCount': '500000',\n            'averageRecordSize': '200',\n            'has_encrypted_data': 'false'\n        },\n        'StorageDescriptor': {\n            'Columns': [\n                {\n                    'Name': 'event_id',\n                    'Type': 'string',\n                    'Comment': 'Unique event identifier'\n                },\n                {\n                    'Name': 'customer_id',\n                    'Type': 'string',\n                    'Comment': 'Customer identifier'\n                },\n                {\n                    'Name': 'event_type',\n                    'Type': 'string',\n                    'Comment': 'Type of event (click, view, purchase)'\n                },\n                {\n                    'Name': 'timestamp',\n                    'Type': 'timestamp',\n                    'Comment': 'Event timestamp'\n                },\n                {\n                    'Name': 'properties',\n                    'Type': 'struct<page:string,product_id:string,amount:double>',\n                    'Comment': 'Event properties'\n                }\n            ],\n            'Location': 's3://data-lake/analytics/customer_events/',\n            'InputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat',\n            'OutputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat',\n            'SerdeInfo': {\n                'SerializationLibrary': 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n            },\n            'Compressed': True,\n            'StoredAsSubDirectories': False\n        },\n        'PartitionKeys': [\n            {\n                'Name': 'year',\n                'Type': 'string',\n                'Comment': 'Year partition'\n            },\n            {\n                'Name': 'month',\n                'Type': 'string', \n                'Comment': 'Month partition'\n            },\n            {\n                'Name': 'day',\n                'Type': 'string',\n                'Comment': 'Day partition'\n            }\n        ],\n        'TableType': 'EXTERNAL_TABLE'\n    }\n}\n\nglue.create_table(**table_config)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"3-partition-management",children:"3. Partition Management"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class PartitionManager:\n    def __init__(self, glue_client):\n        self.glue = glue_client\n    \n    def add_partitions_batch(self, database_name, table_name, partitions):\n        \"\"\"Add multiple partitions efficiently\"\"\"\n        \n        partition_inputs = []\n        for partition in partitions:\n            partition_input = {\n                'Values': partition['values'],\n                'StorageDescriptor': {\n                    'Columns': partition.get('columns', []),\n                    'Location': partition['location'],\n                    'InputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat',\n                    'OutputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat',\n                    'SerdeInfo': {\n                        'SerializationLibrary': 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n                    }\n                },\n                'Parameters': partition.get('parameters', {})\n            }\n            partition_inputs.append(partition_input)\n        \n        # Batch create partitions (max 100 per batch)\n        batch_size = 100\n        for i in range(0, len(partition_inputs), batch_size):\n            batch = partition_inputs[i:i + batch_size]\n            \n            try:\n                self.glue.batch_create_partition(\n                    DatabaseName=database_name,\n                    TableName=table_name,\n                    PartitionInputList=batch\n                )\n                print(f\"Created {len(batch)} partitions\")\n            except Exception as e:\n                print(f\"Error creating partition batch: {str(e)}\")\n    \n    def auto_discover_partitions(self, database_name, table_name, s3_path):\n        \"\"\"Automatically discover and add new partitions\"\"\"\n        import boto3\n        from datetime import datetime, timedelta\n        \n        s3 = boto3.client('s3')\n        bucket = s3_path.split('/')[2]\n        prefix = '/'.join(s3_path.split('/')[3:])\n        \n        # List S3 objects to find partition structure\n        response = s3.list_objects_v2(\n            Bucket=bucket,\n            Prefix=prefix,\n            Delimiter='/'\n        )\n        \n        partitions = []\n        for obj in response.get('CommonPrefixes', []):\n            partition_path = obj['Prefix']\n            \n            # Extract partition values from path\n            # Assuming structure: year=2023/month=01/day=15/\n            path_parts = partition_path.strip('/').split('/')\n            partition_values = []\n            \n            for part in path_parts:\n                if '=' in part:\n                    partition_values.append(part.split('=')[1])\n            \n            if partition_values:\n                partitions.append({\n                    'values': partition_values,\n                    'location': f's3://{bucket}/{partition_path}'\n                })\n        \n        if partitions:\n            self.add_partitions_batch(database_name, table_name, partitions)\n"})}),"\n",(0,i.jsx)(n.h2,{id:"advanced-features",children:"Advanced Features"}),"\n",(0,i.jsx)(n.h3,{id:"1-data-lineage-tracking",children:"1. Data Lineage Tracking"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class DataLineageTracker:\n    def __init__(self, glue_client):\n        self.glue = glue_client\n    \n    def create_lineage_relationship(self, source_table, target_table, transformation_info):\n        \"\"\"Create data lineage relationship\"\"\"\n        \n        lineage_metadata = {\n            'source': {\n                'database': source_table['database'],\n                'table': source_table['table'],\n                'columns': source_table.get('columns', [])\n            },\n            'target': {\n                'database': target_table['database'],\n                'table': target_table['table'],\n                'columns': target_table.get('columns', [])\n            },\n            'transformation': {\n                'type': transformation_info['type'],\n                'job_name': transformation_info.get('job_name'),\n                'transformation_logic': transformation_info.get('logic'),\n                'created_by': transformation_info.get('created_by'),\n                'created_date': datetime.utcnow().isoformat()\n            }\n        }\n        \n        # Store lineage in table parameters\n        self.glue.update_table(\n            DatabaseName=target_table['database'],\n            TableInput={\n                'Name': target_table['table'],\n                'Parameters': {\n                    'data_lineage': json.dumps(lineage_metadata)\n                }\n            }\n        )\n    \n    def get_table_lineage(self, database_name, table_name):\n        \"\"\"Get lineage information for a table\"\"\"\n        \n        try:\n            response = self.glue.get_table(\n                DatabaseName=database_name,\n                Name=table_name\n            )\n            \n            parameters = response['Table'].get('Parameters', {})\n            lineage_data = parameters.get('data_lineage')\n            \n            if lineage_data:\n                return json.loads(lineage_data)\n            \n        except Exception as e:\n            print(f\"Error getting lineage: {str(e)}\")\n        \n        return None\n    \n    def trace_data_flow(self, database_name, table_name, direction='downstream'):\n        \"\"\"Trace data flow upstream or downstream\"\"\"\n        \n        visited = set()\n        flow_graph = {}\n        \n        def trace_recursive(db, table, current_direction):\n            key = f\"{db}.{table}\"\n            if key in visited:\n                return\n            \n            visited.add(key)\n            lineage = self.get_table_lineage(db, table)\n            \n            if lineage:\n                if current_direction == 'downstream':\n                    # Find tables that use this table as source\n                    flow_graph[key] = lineage.get('targets', [])\n                else:\n                    # Find source tables for this table\n                    flow_graph[key] = lineage.get('sources', [])\n                \n                # Recursively trace\n                for related_table in flow_graph[key]:\n                    trace_recursive(\n                        related_table['database'],\n                        related_table['table'],\n                        current_direction\n                    )\n        \n        trace_recursive(database_name, table_name, direction)\n        return flow_graph\n"})}),"\n",(0,i.jsx)(n.h3,{id:"2-data-classification",children:"2. Data Classification"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class DataClassifier:\n    def __init__(self, glue_client):\n        self.glue = glue_client\n        self.sensitive_patterns = {\n            'PII': [\n                r'\\b\\d{3}-\\d{2}-\\d{4}\\b',  # SSN\n                r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',  # Email\n                r'\\b\\d{4}[-\\s]?\\d{4}[-\\s]?\\d{4}[-\\s]?\\d{4}\\b'  # Credit Card\n            ],\n            'FINANCIAL': [\n                r'\\$\\d+\\.\\d{2}',  # Currency\n                r'\\b\\d{9}\\b',  # Routing number\n                r'\\bACCT\\d+\\b'  # Account number\n            ]\n        }\n    \n    def classify_table_columns(self, database_name, table_name):\n        \"\"\"Classify columns based on content patterns\"\"\"\n        \n        # Get table metadata\n        table = self.glue.get_table(\n            DatabaseName=database_name,\n            Name=table_name\n        )\n        \n        columns = table['Table']['StorageDescriptor']['Columns']\n        classifications = {}\n        \n        for column in columns:\n            column_name = column['Name']\n            column_type = column['Type']\n            \n            # Classify based on name patterns\n            classification = self.classify_by_name(column_name)\n            \n            # Classify based on data type\n            if not classification:\n                classification = self.classify_by_type(column_type)\n            \n            if classification:\n                classifications[column_name] = classification\n        \n        # Update table with classification metadata\n        self.update_table_classification(database_name, table_name, classifications)\n        \n        return classifications\n    \n    def classify_by_name(self, column_name):\n        \"\"\"Classify column based on name patterns\"\"\"\n        \n        name_lower = column_name.lower()\n        \n        if any(pattern in name_lower for pattern in ['ssn', 'social_security', 'social']):\n            return 'PII_SSN'\n        elif any(pattern in name_lower for pattern in ['email', 'mail']):\n            return 'PII_EMAIL'\n        elif any(pattern in name_lower for pattern in ['phone', 'mobile', 'tel']):\n            return 'PII_PHONE'\n        elif any(pattern in name_lower for pattern in ['address', 'addr', 'street']):\n            return 'PII_ADDRESS'\n        elif any(pattern in name_lower for pattern in ['salary', 'income', 'wage']):\n            return 'FINANCIAL_INCOME'\n        elif any(pattern in name_lower for pattern in ['account', 'acct']):\n            return 'FINANCIAL_ACCOUNT'\n        \n        return None\n    \n    def classify_by_type(self, column_type):\n        \"\"\"Classify column based on data type\"\"\"\n        \n        if 'decimal' in column_type or 'double' in column_type:\n            return 'NUMERIC'\n        elif 'timestamp' in column_type or 'date' in column_type:\n            return 'TEMPORAL'\n        elif 'string' in column_type:\n            return 'TEXT'\n        \n        return 'UNKNOWN'\n    \n    def update_table_classification(self, database_name, table_name, classifications):\n        \"\"\"Update table with classification metadata\"\"\"\n        \n        # Get current table\n        table = self.glue.get_table(\n            DatabaseName=database_name,\n            Name=table_name\n        )\n        \n        # Update parameters with classification\n        parameters = table['Table'].get('Parameters', {})\n        parameters['data_classification'] = json.dumps(classifications)\n        \n        # Update table\n        table_input = table['Table'].copy()\n        table_input['Parameters'] = parameters\n        \n        # Remove read-only fields\n        table_input.pop('DatabaseName', None)\n        table_input.pop('CreateTime', None)\n        table_input.pop('UpdateTime', None)\n        table_input.pop('CreatedBy', None)\n        table_input.pop('IsRegisteredWithLakeFormation', None)\n        \n        self.glue.update_table(\n            DatabaseName=database_name,\n            TableInput=table_input\n        )\n"})}),"\n",(0,i.jsx)(n.h3,{id:"3-schema-evolution-management",children:"3. Schema Evolution Management"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class SchemaEvolutionManager:\n    def __init__(self, glue_client):\n        self.glue = glue_client\n    \n    def compare_schemas(self, database_name, table_name, version1, version2):\n        \"\"\"Compare two schema versions\"\"\"\n        \n        # Get table versions\n        response = self.glue.get_table_versions(\n            DatabaseName=database_name,\n            TableName=table_name,\n            MaxResults=100\n        )\n        \n        versions = response['TableVersions']\n        \n        # Find specified versions\n        schema1 = None\n        schema2 = None\n        \n        for version in versions:\n            if version['VersionId'] == version1:\n                schema1 = version['Table']['StorageDescriptor']['Columns']\n            elif version['VersionId'] == version2:\n                schema2 = version['Table']['StorageDescriptor']['Columns']\n        \n        if not schema1 or not schema2:\n            raise ValueError(\"One or both schema versions not found\")\n        \n        # Compare schemas\n        return self.analyze_schema_differences(schema1, schema2)\n    \n    def analyze_schema_differences(self, schema1, schema2):\n        \"\"\"Analyze differences between two schemas\"\"\"\n        \n        cols1 = {col['Name']: col for col in schema1}\n        cols2 = {col['Name']: col for col in schema2}\n        \n        differences = {\n            'added_columns': [],\n            'removed_columns': [],\n            'modified_columns': [],\n            'reordered_columns': []\n        }\n        \n        # Find added columns\n        for col_name in cols2:\n            if col_name not in cols1:\n                differences['added_columns'].append(cols2[col_name])\n        \n        # Find removed columns\n        for col_name in cols1:\n            if col_name not in cols2:\n                differences['removed_columns'].append(cols1[col_name])\n        \n        # Find modified columns\n        for col_name in cols1:\n            if col_name in cols2:\n                if cols1[col_name]['Type'] != cols2[col_name]['Type']:\n                    differences['modified_columns'].append({\n                        'column_name': col_name,\n                        'old_type': cols1[col_name]['Type'],\n                        'new_type': cols2[col_name]['Type']\n                    })\n        \n        return differences\n    \n    def create_schema_migration_plan(self, differences):\n        \"\"\"Create migration plan for schema changes\"\"\"\n        \n        migration_plan = {\n            'migration_steps': [],\n            'risk_assessment': 'LOW',\n            'estimated_downtime': '0 minutes',\n            'rollback_plan': []\n        }\n        \n        # Analyze risk\n        if differences['removed_columns'] or differences['modified_columns']:\n            migration_plan['risk_assessment'] = 'HIGH'\n            migration_plan['estimated_downtime'] = '30 minutes'\n        elif differences['added_columns']:\n            migration_plan['risk_assessment'] = 'MEDIUM'\n            migration_plan['estimated_downtime'] = '5 minutes'\n        \n        # Create migration steps\n        for col in differences['added_columns']:\n            migration_plan['migration_steps'].append({\n                'action': 'ADD_COLUMN',\n                'column': col,\n                'sql': f\"ALTER TABLE ADD COLUMN {col['Name']} {col['Type']}\"\n            })\n        \n        for col in differences['removed_columns']:\n            migration_plan['migration_steps'].append({\n                'action': 'DROP_COLUMN',\n                'column': col,\n                'sql': f\"ALTER TABLE DROP COLUMN {col['Name']}\"\n            })\n        \n        return migration_plan\n"})}),"\n",(0,i.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,i.jsx)(n.h3,{id:"1-catalog-query-optimization",children:"1. Catalog Query Optimization"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class CatalogOptimizer:\n    def __init__(self, glue_client):\n        self.glue = glue_client\n    \n    def optimize_table_search(self, search_criteria):\n        \"\"\"Optimize table search with filters\"\"\"\n        \n        # Use pagination for large results\n        paginator = self.glue.get_paginator('get_tables')\n        \n        page_iterator = paginator.paginate(\n            DatabaseName=search_criteria.get('database', ''),\n            Expression=search_criteria.get('expression', ''),\n            MaxItems=search_criteria.get('max_items', 1000),\n            PaginationConfig={\n                'PageSize': 100\n            }\n        )\n        \n        tables = []\n        for page in page_iterator:\n            tables.extend(page['TableList'])\n        \n        return tables\n    \n    def cache_frequently_accessed_metadata(self, database_name):\n        \"\"\"Cache metadata for frequently accessed tables\"\"\"\n        \n        # Get all tables in database\n        tables = self.glue.get_tables(DatabaseName=database_name)\n        \n        metadata_cache = {}\n        \n        for table in tables['TableList']:\n            table_name = table['Name']\n            \n            # Cache essential metadata\n            metadata_cache[table_name] = {\n                'columns': table['StorageDescriptor']['Columns'],\n                'location': table['StorageDescriptor']['Location'],\n                'input_format': table['StorageDescriptor']['InputFormat'],\n                'partition_keys': table.get('PartitionKeys', []),\n                'parameters': table.get('Parameters', {}),\n                'last_updated': table.get('UpdateTime', '').isoformat() if table.get('UpdateTime') else None\n            }\n        \n        # Store cache (implementation depends on your caching strategy)\n        return metadata_cache\n"})}),"\n",(0,i.jsx)(n.h2,{id:"security-v\xe0-access-control",children:"Security v\xe0 Access Control"}),"\n",(0,i.jsx)(n.h3,{id:"1-fine-grained-access-control",children:"1. Fine-grained Access Control"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class CatalogSecurity:\n    def __init__(self, glue_client, iam_client):\n        self.glue = glue_client\n        self.iam = iam_client\n    \n    def create_resource_policy(self, database_name, access_rules):\n        \"\"\"Create resource-based policy for database\"\"\"\n        \n        policy_document = {\n            \"Version\": \"2012-10-17\",\n            \"Statement\": []\n        }\n        \n        for rule in access_rules:\n            statement = {\n                \"Sid\": rule['sid'],\n                \"Effect\": rule['effect'],\n                \"Principal\": rule['principal'],\n                \"Action\": rule['actions'],\n                \"Resource\": f\"arn:aws:glue:*:*:database/{database_name}\"\n            }\n            \n            if rule.get('condition'):\n                statement['Condition'] = rule['condition']\n            \n            policy_document['Statement'].append(statement)\n        \n        # Apply resource policy\n        self.glue.put_resource_policy(\n            PolicyInJson=json.dumps(policy_document)\n        )\n    \n    def setup_column_level_security(self, database_name, table_name, column_permissions):\n        \"\"\"Setup column-level security\"\"\"\n        \n        # This would integrate with AWS Lake Formation\n        # for fine-grained access control\n        \n        for permission in column_permissions:\n            # Example: Grant SELECT on specific columns\n            lake_formation_client = boto3.client('lakeformation')\n            \n            lake_formation_client.grant_permissions(\n                Principal={'DataLakePrincipalIdentifier': permission['principal']},\n                Resource={\n                    'Table': {\n                        'DatabaseName': database_name,\n                        'Name': table_name\n                    }\n                },\n                Permissions=['SELECT'],\n                PermissionsWithGrantOption=[],\n                CatalogId=permission.get('catalog_id', ''),\n                ColumnWildcard={\n                    'IncludedColumnNames': permission['columns']\n                }\n            )\n"})}),"\n",(0,i.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Organization"}),": Organize tables into logical databases"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Naming Conventions"}),": Use consistent naming conventions"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Metadata Quality"}),": Maintain high-quality metadata v\u1edbi descriptions"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Schema Evolution"}),": Plan for schema changes"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Security"}),": Implement proper access controls"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Performance"}),": Optimize for query performance"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Governance"}),": Establish data governance policies"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Monitoring"}),": Monitor catalog usage v\xe0 performance"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"t\xedch-h\u1ee3p-v\u1edbi-c\xe1c-d\u1ecbch-v\u1ee5-aws-kh\xe1c",children:"T\xedch h\u1ee3p v\u1edbi c\xe1c d\u1ecbch v\u1ee5 AWS kh\xe1c"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"AWS Glue Crawlers"}),": Automatic metadata discovery"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Amazon Athena"}),": Query cataloged data"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Amazon EMR"}),": Access metadata for big data processing"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"AWS Glue ETL"}),": Use schemas for ETL jobs"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Amazon Redshift"}),": Integrate v\u1edbi Redshift Spectrum"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"AWS Lake Formation"}),": Fine-grained access control"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Amazon SageMaker"}),": ML feature store integration"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"AWS Lambda"}),": Programmatic catalog access"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Amazon CloudWatch"}),": Monitoring v\xe0 events"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"AWS IAM"}),": Access control v\xe0 security"]}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},65404:(e,n,a)=>{a.d(n,{R:()=>r,x:()=>l});var t=a(36672);const i={},s=t.createContext(i);function r(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);