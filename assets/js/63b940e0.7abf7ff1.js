"use strict";(self.webpackChunkkeep_being_human_dev=self.webpackChunkkeep_being_human_dev||[]).push([[20490],{22512:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>i,contentTitle:()=>t,default:()=>u,frontMatter:()=>c,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"ruby/csv/advanced/parallel_csv_processing","title":"parallel_csv_processing","description":"\u2699\ufe0f Parallel CSV Processing with Thread Pool","source":"@site/docs/ruby/csv/advanced/parallel_csv_processing.md","sourceDirName":"ruby/csv/advanced","slug":"/ruby/csv/advanced/parallel_csv_processing","permalink":"/keep-being-human-dev/docs/ruby/csv/advanced/parallel_csv_processing","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/ruby/csv/advanced/parallel_csv_processing.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"lazy_loading_huge_csv","permalink":"/keep-being-human-dev/docs/ruby/csv/advanced/lazy_loading_huge_csv"},"next":{"title":"streaming_csv_generation","permalink":"/keep-being-human-dev/docs/ruby/csv/advanced/streaming_csv_generation"}}');var a=s(23420),o=s(65404);const c={},t=void 0,i={},l=[{value:"\u2699\ufe0f Parallel CSV Processing with Thread Pool",id:"\ufe0f-parallel-csv-processing-with-thread-pool",level:2}];function d(e){const n={code:"code",h2:"h2",p:"p",pre:"pre",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h2,{id:"\ufe0f-parallel-csv-processing-with-thread-pool",children:"\u2699\ufe0f Parallel CSV Processing with Thread Pool"}),"\n",(0,a.jsx)(n.p,{children:"To speed up CPU-bound transformations on CSV data, divide rows into chunks and process them concurrently with a thread pool. This pattern maximizes CPU utilization while keeping memory usage bounded by chunk size."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-ruby",children:"require 'csv'\nrequire 'concurrent-ruby'\n\npool = Concurrent::FixedThreadPool.new(4)\nCSV.open('large.csv', headers: true).each_slice(5_000) do |slice|\n  pool.post do\n    slice.each { |row| heavy_transform(row) }\n  end\nend\npool.shutdown\npool.wait_for_termination\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Here, ",(0,a.jsx)(n.code,{children:"each_slice"})," segments the file, and ",(0,a.jsx)(n.code,{children:"FixedThreadPool"})," processes each slice in parallel. Adjust pool size and slice length to match your CPU and memory constraints."]})]})}function u(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},65404:(e,n,s)=>{s.d(n,{R:()=>c,x:()=>t});var r=s(36672);const a={},o=r.createContext(a);function c(e){const n=r.useContext(o);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:c(e.components),r.createElement(o.Provider,{value:n},e.children)}}}]);