"use strict";(self.webpackChunkkeep_being_human_dev=self.webpackChunkkeep_being_human_dev||[]).push([[7653],{42825:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>t,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"ruby/compression/advanced/streaming_large_file_compression","title":"streaming_large_file_compression","description":"\ud83d\udddc\ufe0f Streaming Large File Compression with Zlib","source":"@site/docs/ruby/compression/advanced/streaming_large_file_compression.md","sourceDirName":"ruby/compression/advanced","slug":"/ruby/compression/advanced/streaming_large_file_compression","permalink":"/keep-being-human-dev/docs/ruby/compression/advanced/streaming_large_file_compression","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/ruby/compression/advanced/streaming_large_file_compression.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"creating_tar_gz_archives","permalink":"/keep-being-human-dev/docs/ruby/compression/advanced/creating_tar_gz_archives"},"next":{"title":"compress_with_gzip","permalink":"/keep-being-human-dev/docs/ruby/compression/beginner/compress_with_gzip"}}');var s=r(23420),o=r(65404);const t={},a=void 0,c={},l=[{value:"\ud83d\udddc\ufe0f Streaming Large File Compression with Zlib",id:"\ufe0f-streaming-large-file-compression-with-zlib",level:2}];function d(e){const n={code:"code",h2:"h2",p:"p",pre:"pre",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h2,{id:"\ufe0f-streaming-large-file-compression-with-zlib",children:"\ud83d\udddc\ufe0f Streaming Large File Compression with Zlib"}),"\n",(0,s.jsxs)(n.p,{children:["When working with very large files, loading the entire file into memory can lead to out\u2011of\u2011memory errors. By streaming data through ",(0,s.jsx)(n.code,{children:"Zlib::GzipWriter"}),", you can read and compress in chunks with minimal memory overhead. This approach also allows you to tune buffer sizes for optimal throughput."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-ruby",children:"require 'zlib'\n\nINPUT_PATH  = 'path/to/large_input.log'\nOUTPUT_PATH = 'path/to/compressed_output.gz'\nBUFFER_SIZE = 16 * 1024  # 16 KB chunks\n\nZlib::GzipWriter.open(OUTPUT_PATH, Zlib::BEST_SPEED) do |gz|  # or Zlib::BEST_COMPRESSION\n  File.open(INPUT_PATH, 'rb') do |file|\n    while (chunk = file.read(BUFFER_SIZE))\n      gz.write(chunk)\n    end\n  end\nend\n"})}),"\n",(0,s.jsxs)(n.p,{children:["You can decompress in a similarly streaming fashion using ",(0,s.jsx)(n.code,{children:"Zlib::GzipReader.open"})," and writing out chunks to a new file. This pattern ensures constant, predictable memory usage."]})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},65404:(e,n,r)=>{r.d(n,{R:()=>t,x:()=>a});var i=r(36672);const s={},o=i.createContext(s);function t(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);