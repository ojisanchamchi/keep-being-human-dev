"use strict";(self.webpackChunkkeep_being_human_dev=self.webpackChunkkeep_being_human_dev||[]).push([[61829],{65404:(e,n,r)=>{r.d(n,{R:()=>i,x:()=>c});var t=r(36672);const s={},a=t.createContext(s);function i(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),t.createElement(a.Provider,{value:n},e.children)}},88613:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>o,contentTitle:()=>c,default:()=>m,frontMatter:()=>i,metadata:()=>t,toc:()=>p});const t=JSON.parse('{"id":"gems/ruby-openai/expert/advanced_streaming_with_backpressure","title":"advanced_streaming_with_backpressure","description":"\ud83c\udf00 Implement Advanced Streaming with Backpressure","source":"@site/docs/gems/ruby-openai/expert/advanced_streaming_with_backpressure.md","sourceDirName":"gems/ruby-openai/expert","slug":"/gems/ruby-openai/expert/advanced_streaming_with_backpressure","permalink":"/keep-being-human-dev/docs/gems/ruby-openai/expert/advanced_streaming_with_backpressure","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/gems/ruby-openai/expert/advanced_streaming_with_backpressure.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"simple_chat_request","permalink":"/keep-being-human-dev/docs/gems/ruby-openai/beginner/simple_chat_request"},"next":{"title":"dynamic_function_call_management","permalink":"/keep-being-human-dev/docs/gems/ruby-openai/expert/dynamic_function_call_management"}}');var s=r(23420),a=r(65404);const i={},c=void 0,o={},p=[{value:"\ud83c\udf00 Implement Advanced Streaming with Backpressure",id:"-implement-advanced-streaming-with-backpressure",level:2}];function d(e){const n={code:"code",h2:"h2",p:"p",pre:"pre",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h2,{id:"-implement-advanced-streaming-with-backpressure",children:"\ud83c\udf00 Implement Advanced Streaming with Backpressure"}),"\n",(0,s.jsxs)(n.p,{children:["When dealing with large streaming responses, controlling flow is critical to prevent memory bloat and ensure responsive processing. Use Ruby ",(0,s.jsx)(n.code,{children:"Enumerator"})," combined with Fibers to pull tokens only when your application is ready, applying backpressure to the OpenAI stream. This pattern lets you integrate streaming into GUI callbacks or I/O-bound pipelines seamlessly."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-ruby",children:"require 'openai'\n\nclient = OpenAI::Client.new\n\ndef stream_with_backpressure(client, params)\n  Enumerator.new do |yielder|\n    fiber = Fiber.current\n    client.chat.completions(parameters: params, stream: proc { |chunk|\n      Fiber.yield chunk.dig('choices', 0, 'delta', 'content')\n    })\n    yielder << nil\n  end\nend\n\nstream = stream_with_backpressure(client, {\n  model: \"gpt-4o\",\n  messages: [{ role: \"user\", content: \"Generate a poem.\" }]\n})\n\nstream.each do |token|\n  break if token.nil?\n  process_token(token)  # process or render per token\n  sleep 0.05           # apply backpressure\nend\n"})})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);