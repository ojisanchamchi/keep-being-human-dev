"use strict";(self.webpackChunkkeep_being_human_dev=self.webpackChunkkeep_being_human_dev||[]).push([[47468],{34277:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>s,contentTitle:()=>d,default:()=>l,frontMatter:()=>t,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"ruby/file_io/advanced/chunked_buffer_reading","title":"chunked_buffer_reading","description":"\ud83d\udce6 Buffered Chunk Reading","source":"@site/docs/ruby/file_io/advanced/chunked_buffer_reading.md","sourceDirName":"ruby/file_io/advanced","slug":"/ruby/file_io/advanced/chunked_buffer_reading","permalink":"/keep-being-human-dev/docs/ruby/file_io/advanced/chunked_buffer_reading","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/ruby/file_io/advanced/chunked_buffer_reading.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"atomic_file_write","permalink":"/keep-being-human-dev/docs/ruby/file_io/advanced/atomic_file_write"},"next":{"title":"encoding_transcoding","permalink":"/keep-being-human-dev/docs/ruby/file_io/advanced/encoding_transcoding"}}');var a=r(23420),o=r(65404);const t={},d=void 0,s={},c=[{value:"\ud83d\udce6 Buffered Chunk Reading",id:"-buffered-chunk-reading",level:2}];function u(e){const n={code:"code",h2:"h2",p:"p",pre:"pre",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h2,{id:"-buffered-chunk-reading",children:"\ud83d\udce6 Buffered Chunk Reading"}),"\n",(0,a.jsxs)(n.p,{children:["When processing very large files, loading the entire content into memory can lead to excessive memory pressure. Using ",(0,a.jsx)(n.code,{children:"IO#readpartial"})," allows you to read a file in manageable chunks, handling slow or blocking sources gracefully. This approach is ideal for streaming parsers or when you must throttle processing rate."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-ruby",children:"File.open('huge.log', 'r') do |file|\n  buffer_size = 1024 * 1024 # 1 MB\n  until file.eof?\n    chunk = file.readpartial(buffer_size)\n    process(chunk)  # your custom processing logic\n  end\nend\n"})}),"\n",(0,a.jsxs)(n.p,{children:["You can adjust ",(0,a.jsx)(n.code,{children:"buffer_size"})," and even wrap this in an ",(0,a.jsx)(n.code,{children:"Enumerator"})," for lazy pipelines."]})]})}function l(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(u,{...e})}):u(e)}},65404:(e,n,r)=>{r.d(n,{R:()=>t,x:()=>d});var i=r(36672);const a={},o=i.createContext(a);function t(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:t(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);