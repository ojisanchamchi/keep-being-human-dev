"use strict";(self.webpackChunkkeep_being_human_dev=self.webpackChunkkeep_being_human_dev||[]).push([[1994],{19189:(e,a,n)=>{n.r(a),n.d(a,{assets:()=>c,contentTitle:()=>i,default:()=>l,frontMatter:()=>o,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"ruby/marshal/expert/marshal_streaming_chunks","title":"marshal_streaming_chunks","description":"\ud83d\ude80 Chunked Streaming with Marshal for Large Payloads","source":"@site/docs/ruby/marshal/expert/marshal_streaming_chunks.md","sourceDirName":"ruby/marshal/expert","slug":"/ruby/marshal/expert/marshal_streaming_chunks","permalink":"/keep-being-human-dev/docs/ruby/marshal/expert/marshal_streaming_chunks","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/ruby/marshal/expert/marshal_streaming_chunks.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"marshal_secure_compression","permalink":"/keep-being-human-dev/docs/ruby/marshal/expert/marshal_secure_compression"},"next":{"title":"compress_marshal_data","permalink":"/keep-being-human-dev/docs/ruby/marshal/middle/compress_marshal_data"}}');var s=n(23420),t=n(65404);const o={},i=void 0,c={},d=[{value:"\ud83d\ude80 Chunked Streaming with Marshal for Large Payloads",id:"-chunked-streaming-with-marshal-for-large-payloads",level:2}];function u(e){const a={code:"code",h2:"h2",p:"p",pre:"pre",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(a.h2,{id:"-chunked-streaming-with-marshal-for-large-payloads",children:"\ud83d\ude80 Chunked Streaming with Marshal for Large Payloads"}),"\n",(0,s.jsx)(a.p,{children:"Dumping or loading huge objects in one go can exhaust memory. Instead, stream chunks through IO to maintain backpressure and control memory footprint."}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-ruby",children:'# Writer: dumps objects into a file in a streaming fashion\nFile.open("data.marshal", "wb") do |f|\n  objects = large_enumerable_of_records\n  objects.each do |obj|\n    chunk = Marshal.dump(obj)\n    size  = [chunk.bytesize].pack("L>")\n    f.write(size)\n    f.write(chunk)\n  end\nend\n\n# Reader: reads the length-prefixed chunks\nFile.open("data.marshal", "rb") do |f|\n  until f.eof?\n    size_data = f.read(4)\n    break unless size_data\n    size = size_data.unpack1("L>")\n    chunk = f.read(size)\n    record = Marshal.load(chunk)\n    process(record)\n  end\nend\n'})}),"\n",(0,s.jsx)(a.p,{children:"This pattern avoids building one giant blob and lets you process each object sequentially, keeping memory usage bounded."})]})}function l(e={}){const{wrapper:a}={...(0,t.R)(),...e.components};return a?(0,s.jsx)(a,{...e,children:(0,s.jsx)(u,{...e})}):u(e)}},65404:(e,a,n)=>{n.d(a,{R:()=>o,x:()=>i});var r=n(36672);const s={},t=r.createContext(s);function o(e){const a=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(a):{...a,...e}},[a,e])}function i(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),r.createElement(t.Provider,{value:a},e.children)}}}]);