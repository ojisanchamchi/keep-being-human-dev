"use strict";(self.webpackChunkkeep_being_human_dev=self.webpackChunkkeep_being_human_dev||[]).push([[22318],{56036:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>u,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"gems/ruby-openai/expert/high_throughput_rate_limited_requests","title":"high_throughput_rate_limited_requests","description":"\ud83d\ude80 Orchestrate High\u2011Throughput, Rate\u2011Limited Requests with Retries","source":"@site/docs/gems/ruby-openai/expert/high_throughput_rate_limited_requests.md","sourceDirName":"gems/ruby-openai/expert","slug":"/gems/ruby-openai/expert/high_throughput_rate_limited_requests","permalink":"/keep-being-human-dev/docs/gems/ruby-openai/expert/high_throughput_rate_limited_requests","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/gems/ruby-openai/expert/high_throughput_rate_limited_requests.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"dynamic_function_call_management","permalink":"/keep-being-human-dev/docs/gems/ruby-openai/expert/dynamic_function_call_management"},"next":{"title":"handle_rate_limit_retries","permalink":"/keep-being-human-dev/docs/gems/ruby-openai/middle/handle_rate_limit_retries"}}');var i=n(23420),s=n(65404);const a={},o=void 0,u={},c=[{value:"\ud83d\ude80 Orchestrate High\u2011Throughput, Rate\u2011Limited Requests with Retries",id:"-orchestrate-highthroughput-ratelimited-requests-with-retries",level:2}];function l(e){const t={code:"code",h2:"h2",p:"p",pre:"pre",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.h2,{id:"-orchestrate-highthroughput-ratelimited-requests-with-retries",children:"\ud83d\ude80 Orchestrate High\u2011Throughput, Rate\u2011Limited Requests with Retries"}),"\n",(0,i.jsxs)(t.p,{children:["In large-scale systems, you\u2019ll need to batch and throttle your requests to avoid 429 errors. Combine ",(0,i.jsx)(t.code,{children:"concurrent-ruby"})," with a leaky\u2011bucket rate limiter and exponential backoff on retries. This pattern ensures your service remains performant under heavy load while gracefully handling OpenAI rate limits."]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-ruby",children:"require 'openai'\nrequire 'concurrent'\n\nclass RateLimiter\n  def initialize(rate_per_sec)\n    @interval = 1.0 / rate_per_sec\n    @last = Time.now - @interval\n    @mutex = Mutex.new\n  end\n\n  def acquire\n    @mutex.synchronize do\n      wait = @interval - (Time.now - @last)\n      sleep(wait) if wait.positive?\n      @last = Time.now\n    end\n  end\nend\n\nclient = OpenAI::Client.new\nlimiter = RateLimiter.new(5)\n\ntasks = (1..100).map do |i|\n  Concurrent::Promise.execute do\n    limiter.acquire\n    begin\n      client.chat.completions(model: 'gpt-4o', messages: [{ role: 'user', content: \"Q#{i}\" }])\n    rescue OpenAI::Error => e\n      sleep(2 ** retry_count)\n      retry if (retry_count += 1) < 5\n      raise\n    end\n  end\nend\n\nresults = tasks.map(&:value)\n"})})]})}function h(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(l,{...e})}):l(e)}},65404:(e,t,n)=>{n.d(t,{R:()=>a,x:()=>o});var r=n(36672);const i={},s=r.createContext(i);function a(e){const t=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),r.createElement(s.Provider,{value:t},e.children)}}}]);