"use strict";(self.webpackChunkkeep_being_human_dev=self.webpackChunkkeep_being_human_dev||[]).push([[86321],{28749:(e,n,a)=>{a.d(n,{A:()=>r});const r=a.p+"assets/images/glue-crawlers-a75482a0df7e986a116d46c85dd743d5.png"},32498:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>c,contentTitle:()=>i,default:()=>u,frontMatter:()=>t,metadata:()=>r,toc:()=>o});const r=JSON.parse('{"id":"aws-nodes/aws-analytics/glue-crawlers","title":"AWS Glue Crawlers","description":"T\u1ed5ng quan","source":"@site/docs/aws-nodes/aws-analytics/16-glue-crawlers.md","sourceDirName":"aws-nodes/aws-analytics","slug":"/aws-nodes/aws-analytics/glue-crawlers","permalink":"/keep-being-human-dev/docs/aws-nodes/aws-analytics/glue-crawlers","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/aws-nodes/aws-analytics/16-glue-crawlers.md","tags":[],"version":"current","sidebarPosition":16,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Amazon EMR","permalink":"/keep-being-human-dev/docs/aws-nodes/aws-analytics/emr"},"next":{"title":"AWS Glue Data Catalog","permalink":"/keep-being-human-dev/docs/aws-nodes/aws-analytics/glue-data-catalog"}}');var s=a(23420),l=a(65404);const t={},i="AWS Glue Crawlers",c={},o=[{value:"T\u1ed5ng quan",id:"t\u1ed5ng-quan",level:2},{value:"Ch\u1ee9c n\u0103ng ch\xednh",id:"ch\u1ee9c-n\u0103ng-ch\xednh",level:2},{value:"1. Automatic Schema Discovery",id:"1-automatic-schema-discovery",level:3},{value:"2. Data Store Integration",id:"2-data-store-integration",level:3},{value:"3. Metadata Management",id:"3-metadata-management",level:3},{value:"4. Scheduling v\xe0 Automation",id:"4-scheduling-v\xe0-automation",level:3},{value:"Use Cases ph\u1ed5 bi\u1ebfn",id:"use-cases-ph\u1ed5-bi\u1ebfn",level:2},{value:"Diagram Architecture",id:"diagram-architecture",level:2},{value:"Code \u0111\u1ec3 t\u1ea1o diagram:",id:"code-\u0111\u1ec3-t\u1ea1o-diagram",level:3},{value:"Crawler Configuration",id:"crawler-configuration",level:2},{value:"1. S3 Crawler Setup",id:"1-s3-crawler-setup",level:3},{value:"2. JDBC Crawler Configuration",id:"2-jdbc-crawler-configuration",level:3},{value:"3. Custom Classifier",id:"3-custom-classifier",level:3},{value:"Advanced Crawler Features",id:"advanced-crawler-features",level:2},{value:"1. Schema Evolution Handling",id:"1-schema-evolution-handling",level:3},{value:"2. Incremental Crawling",id:"2-incremental-crawling",level:3},{value:"3. Data Quality Validation",id:"3-data-quality-validation",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"1. Crawler Performance Tuning",id:"1-crawler-performance-tuning",level:3},{value:"2. Parallel Crawling Strategy",id:"2-parallel-crawling-strategy",level:3},{value:"Monitoring v\xe0 Troubleshooting",id:"monitoring-v\xe0-troubleshooting",level:2},{value:"1. Crawler Metrics",id:"1-crawler-metrics",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"T\xedch h\u1ee3p v\u1edbi c\xe1c d\u1ecbch v\u1ee5 AWS kh\xe1c",id:"t\xedch-h\u1ee3p-v\u1edbi-c\xe1c-d\u1ecbch-v\u1ee5-aws-kh\xe1c",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,l.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"aws-glue-crawlers",children:"AWS Glue Crawlers"})}),"\n",(0,s.jsx)(n.h2,{id:"t\u1ed5ng-quan",children:"T\u1ed5ng quan"}),"\n",(0,s.jsx)(n.p,{children:"AWS Glue Crawlers l\xe0 node \u0111\u1ea1i di\u1ec7n cho th\xe0nh ph\u1ea7n t\u1ef1 \u0111\u1ed9ng kh\xe1m ph\xe1 v\xe0 ph\xe2n lo\u1ea1i d\u1eef li\u1ec7u trong AWS Glue ecosystem. Crawlers t\u1ef1 \u0111\u1ed9ng qu\xe9t c\xe1c data stores, x\xe1c \u0111\u1ecbnh \u0111\u1ecbnh d\u1ea1ng d\u1eef li\u1ec7u, schema, v\xe0 t\u1ea1o metadata tables trong AWS Glue Data Catalog. \u0110\xe2y l\xe0 c\xf4ng c\u1ee5 quan tr\u1ecdng \u0111\u1ec3 t\u1ef1 \u0111\u1ed9ng h\xf3a vi\u1ec7c qu\u1ea3n l\xfd metadata v\xe0 chu\u1ea9n b\u1ecb d\u1eef li\u1ec7u cho analytics."}),"\n",(0,s.jsx)(n.h2,{id:"ch\u1ee9c-n\u0103ng-ch\xednh",children:"Ch\u1ee9c n\u0103ng ch\xednh"}),"\n",(0,s.jsx)(n.h3,{id:"1-automatic-schema-discovery",children:"1. Automatic Schema Discovery"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Schema Inference"}),": T\u1ef1 \u0111\u1ed9ng ph\xe1t hi\u1ec7n schema t\u1eeb d\u1eef li\u1ec7u"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Data Type Detection"}),": X\xe1c \u0111\u1ecbnh data types ch\xednh x\xe1c"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Partition Discovery"}),": Ph\xe1t hi\u1ec7n partition structure"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Format Recognition"}),": Nh\u1eadn di\u1ec7n file formats (JSON, Parquet, CSV, etc.)"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"2-data-store-integration",children:"2. Data Store Integration"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"S3 Support"}),": Crawl d\u1eef li\u1ec7u trong Amazon S3"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Database Connectivity"}),": JDBC connections \u0111\u1ebfn databases"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"DynamoDB Integration"}),": Crawl DynamoDB tables"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Custom Classifiers"}),": T\u1ea1o custom classifiers cho formats \u0111\u1eb7c bi\u1ec7t"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"3-metadata-management",children:"3. Metadata Management"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Table Creation"}),": T\u1ef1 \u0111\u1ed9ng t\u1ea1o tables trong Data Catalog"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Schema Evolution"}),": Theo d\xf5i v\xe0 c\u1eadp nh\u1eadt schema changes"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Partition Management"}),": Qu\u1ea3n l\xfd partitions t\u1ef1 \u0111\u1ed9ng"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Data Classification"}),": Ph\xe2n lo\u1ea1i d\u1eef li\u1ec7u theo sensitivity"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"4-scheduling-v\xe0-automation",children:"4. Scheduling v\xe0 Automation"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Scheduled Runs"}),": Ch\u1ea1y crawlers theo l\u1ecbch \u0111\u1ecbnh s\u1eb5n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Event-driven"}),": Trigger b\u1edfi S3 events ho\u1eb7c CloudWatch"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Incremental Crawling"}),": Ch\u1ec9 crawl d\u1eef li\u1ec7u m\u1edbi ho\u1eb7c thay \u0111\u1ed5i"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Parallel Processing"}),": Crawl multiple data stores \u0111\u1ed3ng th\u1eddi"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"use-cases-ph\u1ed5-bi\u1ebfn",children:"Use Cases ph\u1ed5 bi\u1ebfn"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Data Lake Discovery"}),": T\u1ef1 \u0111\u1ed9ng kh\xe1m ph\xe1 d\u1eef li\u1ec7u trong data lake"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Schema Management"}),": Qu\u1ea3n l\xfd schema evolution"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Data Cataloging"}),": T\u1ea1o comprehensive data catalog"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ETL Preparation"}),": Chu\u1ea9n b\u1ecb metadata cho ETL jobs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Compliance Scanning"}),": Ph\xe1t hi\u1ec7n sensitive data cho compliance"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"diagram-architecture",children:"Diagram Architecture"}),"\n",(0,s.jsx)(n.p,{children:"Ki\u1ebfn tr\xfac AWS Glue Crawlers v\u1edbi data discovery workflow:"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"AWS Glue Crawlers Architecture",src:a(28749).A+"",width:"2724",height:"2061"})}),"\n",(0,s.jsx)(n.h3,{id:"code-\u0111\u1ec3-t\u1ea1o-diagram",children:"Code \u0111\u1ec3 t\u1ea1o diagram:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.analytics import GlueCrawlers, GlueDataCatalog, Athena\nfrom diagrams.aws.storage import S3\nfrom diagrams.aws.compute import Lambda\nfrom diagrams.aws.database import RDS, Dynamodb\nfrom diagrams.aws.integration import SQS, SNS\nfrom diagrams.aws.management import Cloudwatch, CloudwatchEventTimeBased\nfrom diagrams.aws.security import IAM\nfrom diagrams.onprem.client import Users\nfrom diagrams.onprem.database import MySQL\n\nwith Diagram("AWS Glue Crawlers Architecture", show=False, direction="TB"):\n    \n    users = Users("Data Engineers")\n    \n    with Cluster("Data Sources"):\n        s3_raw = S3("Raw Data Lake")\n        s3_processed = S3("Processed Data")\n        rds_db = RDS("RDS Database")\n        mysql_db = MySQL("On-premises DB")\n        dynamodb_table = Dynamodb("DynamoDB Tables")\n    \n    with Cluster("Glue Crawlers"):\n        crawlers = GlueCrawlers("Glue Crawlers")\n        \n        with Cluster("Crawler Types"):\n            s3_crawler = GlueCrawlers("S3 Crawler")\n            jdbc_crawler = GlueCrawlers("JDBC Crawler")\n            dynamodb_crawler = GlueCrawlers("DynamoDB Crawler")\n    \n    with Cluster("Data Catalog"):\n        data_catalog = GlueDataCatalog("Glue Data Catalog")\n        \n        with Cluster("Catalog Components"):\n            databases = GlueDataCatalog("Databases")\n            tables = GlueDataCatalog("Tables")\n            partitions = GlueDataCatalog("Partitions")\n    \n    with Cluster("Scheduling & Triggers"):\n        cloudwatch_events = CloudwatchEventTimeBased("CloudWatch Events")\n        s3_events = S3("S3 Events")\n        lambda_trigger = Lambda("Trigger Function")\n        scheduler = SQS("Scheduler Queue")\n    \n    with Cluster("Analytics Integration"):\n        athena = Athena("Athena Queries")\n        etl_jobs = Lambda("ETL Jobs")\n        ml_pipeline = Lambda("ML Pipeline")\n    \n    with Cluster("Monitoring & Security"):\n        monitoring = Cloudwatch("CloudWatch")\n        notifications = SNS("Notifications")\n        security = IAM("IAM Roles")\n    \n    # Data source connections\n    s3_raw >> Edge(label="Crawl") >> s3_crawler\n    s3_processed >> Edge(label="Crawl") >> s3_crawler\n    rds_db >> Edge(label="JDBC") >> jdbc_crawler\n    mysql_db >> Edge(label="JDBC") >> jdbc_crawler\n    dynamodb_table >> Edge(label="Scan") >> dynamodb_crawler\n    \n    # Crawler orchestration\n    crawlers >> s3_crawler\n    crawlers >> jdbc_crawler\n    crawlers >> dynamodb_crawler\n    \n    # Metadata creation\n    s3_crawler >> Edge(label="Create Tables") >> data_catalog\n    jdbc_crawler >> Edge(label="Create Tables") >> data_catalog\n    dynamodb_crawler >> Edge(label="Create Tables") >> data_catalog\n    \n    # Catalog structure\n    data_catalog >> databases\n    data_catalog >> tables\n    data_catalog >> partitions\n    \n    # Scheduling and triggers\n    cloudwatch_events >> Edge(label="Schedule") >> crawlers\n    s3_events >> lambda_trigger >> crawlers\n    scheduler >> Edge(label="Queue Jobs") >> crawlers\n    \n    # Analytics consumption\n    data_catalog >> athena\n    data_catalog >> etl_jobs\n    data_catalog >> ml_pipeline\n    \n    # User interaction\n    users >> Edge(label="Configure") >> crawlers\n    users >> Edge(label="Query Catalog") >> data_catalog\n    users >> Edge(label="Run Analytics") >> athena\n    \n    # Monitoring and notifications\n    crawlers >> Edge(label="Metrics") >> monitoring\n    crawlers >> Edge(label="Alerts") >> notifications\n    security >> Edge(label="Access Control") >> crawlers\n'})}),"\n",(0,s.jsx)(n.h2,{id:"crawler-configuration",children:"Crawler Configuration"}),"\n",(0,s.jsx)(n.h3,{id:"1-s3-crawler-setup",children:"1. S3 Crawler Setup"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import boto3\n\nglue = boto3.client('glue')\n\n# Create S3 crawler\ns3_crawler_config = {\n    'Name': 'data-lake-crawler',\n    'Role': 'arn:aws:iam::account:role/GlueCrawlerRole',\n    'DatabaseName': 'data_lake_db',\n    'Description': 'Crawls data lake for schema discovery',\n    'Targets': {\n        'S3Targets': [\n            {\n                'Path': 's3://data-lake/raw/',\n                'Exclusions': ['*.tmp', '*.log']\n            },\n            {\n                'Path': 's3://data-lake/processed/',\n                'SampleSize': 100\n            }\n        ]\n    },\n    'Schedule': 'cron(0 2 * * ? *)',  # Daily at 2 AM\n    'SchemaChangePolicy': {\n        'UpdateBehavior': 'UPDATE_IN_DATABASE',\n        'DeleteBehavior': 'LOG'\n    },\n    'RecrawlPolicy': {\n        'RecrawlBehavior': 'CRAWL_EVERYTHING'\n    },\n    'LineageConfiguration': {\n        'CrawlerLineageSettings': 'ENABLE'\n    },\n    'Configuration': json.dumps({\n        'Version': 1.0,\n        'CrawlerOutput': {\n            'Partitions': {'AddOrUpdateBehavior': 'InheritFromTable'},\n            'Tables': {'AddOrUpdateBehavior': 'MergeNewColumns'}\n        }\n    })\n}\n\nresponse = glue.create_crawler(**s3_crawler_config)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-jdbc-crawler-configuration",children:"2. JDBC Crawler Configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# JDBC crawler for relational databases\njdbc_crawler_config = {\n    'Name': 'rds-crawler',\n    'Role': 'arn:aws:iam::account:role/GlueCrawlerRole',\n    'DatabaseName': 'operational_db',\n    'Targets': {\n        'JdbcTargets': [\n            {\n                'ConnectionName': 'rds-connection',\n                'Path': 'production/public/%',\n                'Exclusions': ['temp_%', 'log_%']\n            }\n        ]\n    },\n    'Schedule': 'cron(0 6 ? * SUN *)',  # Weekly on Sunday\n    'Classifiers': ['custom-csv-classifier'],\n    'TablePrefix': 'prod_',\n    'SchemaChangePolicy': {\n        'UpdateBehavior': 'UPDATE_IN_DATABASE',\n        'DeleteBehavior': 'DEPRECATE_IN_DATABASE'\n    }\n}\n\n# Create JDBC connection first\nconnection_config = {\n    'ConnectionInput': {\n        'Name': 'rds-connection',\n        'ConnectionType': 'JDBC',\n        'ConnectionProperties': {\n            'JDBC_CONNECTION_URL': 'jdbc:postgresql://rds-endpoint:5432/production',\n            'USERNAME': 'crawler_user',\n            'PASSWORD': 'secure_password'\n        },\n        'PhysicalConnectionRequirements': {\n            'SubnetId': 'subnet-12345',\n            'SecurityGroupIdList': ['sg-67890'],\n            'AvailabilityZone': 'us-west-2a'\n        }\n    }\n}\n\nglue.create_connection(**connection_config)\nglue.create_crawler(**jdbc_crawler_config)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"3-custom-classifier",children:"3. Custom Classifier"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Create custom classifier for special formats\nclassifier_config = {\n    'CsvClassifier': {\n        'Name': 'custom-csv-classifier',\n        'Delimiter': '|',\n        'QuoteSymbol': '\"',\n        'ContainsHeader': 'PRESENT',\n        'Header': ['id', 'name', 'email', 'created_date'],\n        'DisableValueTrimming': False,\n        'AllowSingleColumn': False\n    }\n}\n\nglue.create_classifier(**classifier_config)\n\n# JSON classifier\njson_classifier_config = {\n    'JsonClassifier': {\n        'Name': 'nested-json-classifier',\n        'JsonPath': '$.data[*]'\n    }\n}\n\nglue.create_classifier(**json_classifier_config)\n"})}),"\n",(0,s.jsx)(n.h2,{id:"advanced-crawler-features",children:"Advanced Crawler Features"}),"\n",(0,s.jsx)(n.h3,{id:"1-schema-evolution-handling",children:"1. Schema Evolution Handling"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class SchemaEvolutionHandler:\n    def __init__(self, glue_client):\n        self.glue = glue_client\n    \n    def handle_schema_changes(self, crawler_name, table_name):\n        \"\"\"Handle schema evolution events\"\"\"\n        try:\n            # Get table versions\n            response = self.glue.get_table_versions(\n                DatabaseName='data_lake_db',\n                TableName=table_name,\n                MaxResults=10\n            )\n            \n            versions = response['TableVersions']\n            if len(versions) > 1:\n                current_schema = versions[0]['Table']['StorageDescriptor']['Columns']\n                previous_schema = versions[1]['Table']['StorageDescriptor']['Columns']\n                \n                # Detect changes\n                changes = self.detect_schema_changes(current_schema, previous_schema)\n                \n                if changes:\n                    self.notify_schema_changes(table_name, changes)\n                    self.update_downstream_jobs(table_name, changes)\n            \n        except Exception as e:\n            print(f\"Error handling schema evolution: {str(e)}\")\n    \n    def detect_schema_changes(self, current, previous):\n        \"\"\"Detect schema changes between versions\"\"\"\n        changes = {\n            'added_columns': [],\n            'removed_columns': [],\n            'type_changes': []\n        }\n        \n        current_cols = {col['Name']: col['Type'] for col in current}\n        previous_cols = {col['Name']: col['Type'] for col in previous}\n        \n        # Added columns\n        for col_name in current_cols:\n            if col_name not in previous_cols:\n                changes['added_columns'].append(col_name)\n        \n        # Removed columns\n        for col_name in previous_cols:\n            if col_name not in current_cols:\n                changes['removed_columns'].append(col_name)\n        \n        # Type changes\n        for col_name in current_cols:\n            if col_name in previous_cols and current_cols[col_name] != previous_cols[col_name]:\n                changes['type_changes'].append({\n                    'column': col_name,\n                    'old_type': previous_cols[col_name],\n                    'new_type': current_cols[col_name]\n                })\n        \n        return changes\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-incremental-crawling",children:"2. Incremental Crawling"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class IncrementalCrawler:\n    def __init__(self, glue_client, s3_client):\n        self.glue = glue_client\n        self.s3 = s3_client\n    \n    def setup_incremental_crawling(self, crawler_name, s3_path):\n        \"\"\"Setup incremental crawling based on S3 events\"\"\"\n        \n        # Create Lambda function for S3 event processing\n        lambda_function = self.create_crawler_trigger_function(crawler_name)\n        \n        # Setup S3 event notification\n        bucket_name = s3_path.split('/')[2]\n        prefix = '/'.join(s3_path.split('/')[3:])\n        \n        notification_config = {\n            'LambdaConfigurations': [\n                {\n                    'Id': f'{crawler_name}-trigger',\n                    'LambdaFunctionArn': lambda_function['FunctionArn'],\n                    'Events': ['s3:ObjectCreated:*'],\n                    'Filter': {\n                        'Key': {\n                            'FilterRules': [\n                                {\n                                    'Name': 'prefix',\n                                    'Value': prefix\n                                }\n                            ]\n                        }\n                    }\n                }\n            ]\n        }\n        \n        self.s3.put_bucket_notification_configuration(\n            Bucket=bucket_name,\n            NotificationConfiguration=notification_config\n        )\n    \n    def create_crawler_trigger_function(self, crawler_name):\n        \"\"\"Create Lambda function to trigger crawler\"\"\"\n        lambda_code = f'''\nimport boto3\nimport json\n\ndef lambda_handler(event, context):\n    glue = boto3.client('glue')\n    \n    # Check if crawler is already running\n    try:\n        response = glue.get_crawler(Name='{crawler_name}')\n        if response['Crawler']['State'] == 'RUNNING':\n            return {{'statusCode': 200, 'body': 'Crawler already running'}}\n    except:\n        pass\n    \n    # Start crawler\n    try:\n        glue.start_crawler(Name='{crawler_name}')\n        return {{'statusCode': 200, 'body': 'Crawler started successfully'}}\n    except Exception as e:\n        return {{'statusCode': 500, 'body': f'Error: {{str(e)}}'}}\n'''\n        \n        # This would create the actual Lambda function\n        # Implementation depends on your deployment method\n        return {'FunctionArn': f'arn:aws:lambda:region:account:function:{crawler_name}-trigger'}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"3-data-quality-validation",children:"3. Data Quality Validation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class CrawlerDataQuality:\n    def __init__(self, glue_client):\n        self.glue = glue_client\n    \n    def validate_crawled_data(self, database_name, table_name):\n        \"\"\"Validate data quality after crawling\"\"\"\n        \n        # Get table metadata\n        table = self.glue.get_table(\n            DatabaseName=database_name,\n            Name=table_name\n        )\n        \n        validation_results = {\n            'table_name': table_name,\n            'validation_time': datetime.utcnow().isoformat(),\n            'checks': []\n        }\n        \n        # Check 1: Schema consistency\n        schema_check = self.validate_schema_consistency(table)\n        validation_results['checks'].append(schema_check)\n        \n        # Check 2: Data freshness\n        freshness_check = self.validate_data_freshness(table)\n        validation_results['checks'].append(freshness_check)\n        \n        # Check 3: Partition completeness\n        partition_check = self.validate_partition_completeness(table)\n        validation_results['checks'].append(partition_check)\n        \n        return validation_results\n    \n    def validate_schema_consistency(self, table):\n        \"\"\"Validate schema consistency\"\"\"\n        columns = table['Table']['StorageDescriptor']['Columns']\n        \n        # Check for required columns\n        required_columns = ['id', 'created_date']  # Example\n        missing_columns = [col for col in required_columns \n                          if col not in [c['Name'] for c in columns]]\n        \n        return {\n            'check_name': 'schema_consistency',\n            'status': 'PASS' if not missing_columns else 'FAIL',\n            'details': {\n                'missing_columns': missing_columns,\n                'total_columns': len(columns)\n            }\n        }\n    \n    def validate_data_freshness(self, table):\n        \"\"\"Validate data freshness\"\"\"\n        # Get last update time from table metadata\n        last_updated = table['Table'].get('UpdateTime')\n        \n        if last_updated:\n            age_hours = (datetime.utcnow() - last_updated).total_seconds() / 3600\n            is_fresh = age_hours <= 24  # Data should be less than 24 hours old\n            \n            return {\n                'check_name': 'data_freshness',\n                'status': 'PASS' if is_fresh else 'FAIL',\n                'details': {\n                    'last_updated': last_updated.isoformat(),\n                    'age_hours': age_hours\n                }\n            }\n        \n        return {\n            'check_name': 'data_freshness',\n            'status': 'UNKNOWN',\n            'details': {'message': 'No update time available'}\n        }\n"})}),"\n",(0,s.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,s.jsx)(n.h3,{id:"1-crawler-performance-tuning",children:"1. Crawler Performance Tuning"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Optimize crawler performance\noptimized_crawler_config = {\n    'Name': 'optimized-crawler',\n    'Role': 'arn:aws:iam::account:role/GlueCrawlerRole',\n    'DatabaseName': 'optimized_db',\n    'Targets': {\n        'S3Targets': [\n            {\n                'Path': 's3://large-dataset/',\n                'SampleSize': 1000,  # Sample size for large datasets\n                'ConnectionName': 's3-vpc-endpoint'  # Use VPC endpoint\n            }\n        ]\n    },\n    'Configuration': json.dumps({\n        'Version': 1.0,\n        'Grouping': {\n            'TableGroupingPolicy': 'CombineCompatibleSchemas'\n        },\n        'CrawlerOutput': {\n            'Partitions': {'AddOrUpdateBehavior': 'InheritFromTable'},\n            'Tables': {'AddOrUpdateBehavior': 'MergeNewColumns'}\n        }\n    }),\n    'RecrawlPolicy': {\n        'RecrawlBehavior': 'CRAWL_NEW_FOLDERS_ONLY'\n    }\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-parallel-crawling-strategy",children:"2. Parallel Crawling Strategy"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class ParallelCrawlerManager:\n    def __init__(self, glue_client):\n        self.glue = glue_client\n    \n    def create_parallel_crawlers(self, s3_paths, base_crawler_config):\n        """Create multiple crawlers for parallel processing"""\n        crawlers = []\n        \n        for i, path in enumerate(s3_paths):\n            crawler_config = base_crawler_config.copy()\n            crawler_config[\'Name\'] = f"{base_crawler_config[\'Name\']}-{i}"\n            crawler_config[\'Targets\'] = {\n                \'S3Targets\': [{\'Path\': path}]\n            }\n            \n            self.glue.create_crawler(**crawler_config)\n            crawlers.append(crawler_config[\'Name\'])\n        \n        return crawlers\n    \n    def run_crawlers_parallel(self, crawler_names):\n        """Run multiple crawlers in parallel"""\n        for crawler_name in crawler_names:\n            try:\n                self.glue.start_crawler(Name=crawler_name)\n                print(f"Started crawler: {crawler_name}")\n            except Exception as e:\n                print(f"Failed to start crawler {crawler_name}: {str(e)}")\n    \n    def monitor_crawler_progress(self, crawler_names):\n        """Monitor progress of parallel crawlers"""\n        while True:\n            running_crawlers = []\n            \n            for crawler_name in crawler_names:\n                response = self.glue.get_crawler(Name=crawler_name)\n                state = response[\'Crawler\'][\'State\']\n                \n                if state == \'RUNNING\':\n                    running_crawlers.append(crawler_name)\n                elif state == \'STOPPING\':\n                    running_crawlers.append(crawler_name)\n            \n            if not running_crawlers:\n                break\n            \n            print(f"Still running: {running_crawlers}")\n            time.sleep(30)\n        \n        print("All crawlers completed")\n'})}),"\n",(0,s.jsx)(n.h2,{id:"monitoring-v\xe0-troubleshooting",children:"Monitoring v\xe0 Troubleshooting"}),"\n",(0,s.jsx)(n.h3,{id:"1-crawler-metrics",children:"1. Crawler Metrics"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class CrawlerMonitoring:\n    def __init__(self, cloudwatch_client):\n        self.cloudwatch = cloudwatch_client\n    \n    def get_crawler_metrics(self, crawler_name, start_time, end_time):\n        \"\"\"Get CloudWatch metrics for crawler\"\"\"\n        \n        metrics = [\n            'glue.driver.aggregate.numCompletedTasks',\n            'glue.driver.aggregate.numFailedTasks',\n            'glue.driver.BlockManager.disk.diskSpaceUsed_MB',\n            'glue.driver.jvm.heap.usage'\n        ]\n        \n        results = {}\n        \n        for metric in metrics:\n            response = self.cloudwatch.get_metric_statistics(\n                Namespace='AWS/Glue',\n                MetricName=metric,\n                Dimensions=[\n                    {\n                        'Name': 'JobName',\n                        'Value': crawler_name\n                    }\n                ],\n                StartTime=start_time,\n                EndTime=end_time,\n                Period=300,\n                Statistics=['Average', 'Maximum']\n            )\n            \n            results[metric] = response['Datapoints']\n        \n        return results\n    \n    def create_crawler_alarms(self, crawler_name):\n        \"\"\"Create CloudWatch alarms for crawler\"\"\"\n        \n        # Alarm for failed crawls\n        self.cloudwatch.put_metric_alarm(\n            AlarmName=f'{crawler_name}-failures',\n            ComparisonOperator='GreaterThanThreshold',\n            EvaluationPeriods=1,\n            MetricName='glue.driver.aggregate.numFailedTasks',\n            Namespace='AWS/Glue',\n            Period=300,\n            Statistic='Sum',\n            Threshold=0,\n            ActionsEnabled=True,\n            AlarmActions=[\n                'arn:aws:sns:region:account:crawler-alerts'\n            ],\n            AlarmDescription=f'Alarm for {crawler_name} failures',\n            Dimensions=[\n                {\n                    'Name': 'JobName',\n                    'Value': crawler_name\n                }\n            ]\n        )\n"})}),"\n",(0,s.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Crawler Design"}),": T\u1ea1o separate crawlers cho different data sources"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Scheduling"}),": S\u1eed d\u1ee5ng appropriate schedules \u0111\u1ec3 avoid conflicts"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Schema Evolution"}),": Configure proper schema change policies"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Performance"}),": Optimize sample sizes v\xe0 exclusion patterns"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Security"}),": Use least privilege IAM roles"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Monitoring"}),": Set up comprehensive monitoring v\xe0 alerting"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cost Management"}),": Use incremental crawling \u0111\u1ec3 reduce costs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Data Quality"}),": Implement validation checks post-crawling"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"t\xedch-h\u1ee3p-v\u1edbi-c\xe1c-d\u1ecbch-v\u1ee5-aws-kh\xe1c",children:"T\xedch h\u1ee3p v\u1edbi c\xe1c d\u1ecbch v\u1ee5 AWS kh\xe1c"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Amazon S3"}),": Primary data source cho crawling"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"AWS Glue Data Catalog"}),": Metadata repository"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Amazon Athena"}),": Query cataloged data"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"AWS Glue ETL"}),": Use discovered schemas"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Amazon EMR"}),": Access cataloged metadata"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"AWS Lambda"}),": Event-driven crawler triggers"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Amazon CloudWatch"}),": Monitoring v\xe0 alerting"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"AWS IAM"}),": Access control v\xe0 security"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Amazon SNS"}),": Notifications v\xe0 alerts"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"AWS Step Functions"}),": Orchestrate crawler workflows"]}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},65404:(e,n,a)=>{a.d(n,{R:()=>t,x:()=>i});var r=a(36672);const s={},l=r.createContext(s);function t(e){const n=r.useContext(l);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),r.createElement(l.Provider,{value:n},e.children)}}}]);