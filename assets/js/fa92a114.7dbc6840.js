"use strict";(self.webpackChunkkeep_being_human_dev=self.webpackChunkkeep_being_human_dev||[]).push([[13672],{22583:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>i,contentTitle:()=>o,default:()=>u,frontMatter:()=>t,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"ruby/csv/advanced/lazy_loading_huge_csv","title":"lazy_loading_huge_csv","description":"\ud83d\ude80 Lazy Loading Huge CSV Files","source":"@site/docs/ruby/csv/advanced/lazy_loading_huge_csv.md","sourceDirName":"ruby/csv/advanced","slug":"/ruby/csv/advanced/lazy_loading_huge_csv","permalink":"/keep-being-human-dev/docs/ruby/csv/advanced/lazy_loading_huge_csv","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/ruby/csv/advanced/lazy_loading_huge_csv.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"custom_converters_casting","permalink":"/keep-being-human-dev/docs/ruby/csv/advanced/custom_converters_casting"},"next":{"title":"parallel_csv_processing","permalink":"/keep-being-human-dev/docs/ruby/csv/advanced/parallel_csv_processing"}}');var c=a(23420),r=a(65404);const t={},o=void 0,i={},d=[{value:"\ud83d\ude80 Lazy Loading Huge CSV Files",id:"-lazy-loading-huge-csv-files",level:2}];function l(e){const n={code:"code",h2:"h2",p:"p",pre:"pre",...(0,r.R)(),...e.components};return(0,c.jsxs)(c.Fragment,{children:[(0,c.jsx)(n.h2,{id:"-lazy-loading-huge-csv-files",children:"\ud83d\ude80 Lazy Loading Huge CSV Files"}),"\n",(0,c.jsx)(n.p,{children:"When dealing with multi-GB CSVs, loading everything into memory can crash your process. You can leverage Ruby's lazy enumerators combined with CSV to process data in manageable chunks. This approach reads rows on demand and batches them for processing without loading the entire file at once."}),"\n",(0,c.jsx)(n.pre,{children:(0,c.jsx)(n.code,{className:"language-ruby",children:"require 'csv'\n\nfile = File.open('huge_dataset.csv', 'r:bom|utf-8')\nenumerator = CSV.new(file, headers: true).lazy\n\nenumerator.each_slice(10_000) do |batch|\n  # Process 10k rows at a time\n  batch.each { |row| process_row(row) }\nend\n"})}),"\n",(0,c.jsxs)(n.p,{children:["The ",(0,c.jsx)(n.code,{children:"lazy"})," call ensures that rows are parsed only when accessed, and ",(0,c.jsx)(n.code,{children:"each_slice"})," helps you work in fixed-size batches."]})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,c.jsx)(n,{...e,children:(0,c.jsx)(l,{...e})}):l(e)}},65404:(e,n,a)=>{a.d(n,{R:()=>t,x:()=>o});var s=a(36672);const c={},r=s.createContext(c);function t(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(c):e.components||c:t(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);